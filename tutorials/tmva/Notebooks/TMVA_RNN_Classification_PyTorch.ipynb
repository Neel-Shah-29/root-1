{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a54454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "from ROOT import TMVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed3512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ninput = 30\n",
    "# ninput=10\n",
    "ntime = 10\n",
    "# batchSize = 100\n",
    "batchSize = 100\n",
    "maxepochs = 20\n",
    "\n",
    "use_type = 1\n",
    "\n",
    "nTotEvts = 10000 # total events to be generated for signal or background\n",
    "\n",
    "useKeras = True\n",
    "\n",
    "\n",
    "useTMVA_RNN = True\n",
    "useTMVA_DNN = True\n",
    "useTMVA_BDT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367c0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn_types = [\"RNN\", \"LSTM\", \"GRU\"]\n",
    "use_rnn_type = [1, 1, 1]\n",
    "if (use_type >=0 & use_type < 3) :\n",
    "      use_rnn_type = [0,0,0]\n",
    "      use_rnn_type[use_type] = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd80dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "archString = \"CPU\"\n",
    "writeOutputFile = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e13c932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_type = \"RNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ae63a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT.TMVA.Tools.Instance()\n",
    "ROOT.TMVA.PyMethodBase.PyInitialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2d9a537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with nthreads  = 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_threads = 0   # use by default all threads\n",
    "#    do enable MT running\n",
    "if (num_threads >= 0):\n",
    "    ROOT.EnableImplicitMT(num_threads)\n",
    "    if (num_threads > 0):\n",
    "        ROOT.gSystem.Setenv(\"OMP_NUM_THREADS\", num_threads)\n",
    "    else:\n",
    "      ROOT.gSystem.Setenv(\"OMP_NUM_THREADS\", \"1\")\n",
    "\n",
    "\n",
    "print(\"Running with nthreads  = \" + str(ROOT.GetThreadPoolSize()) + \"\\n\" )\n",
    "\n",
    "inputFileName = \"time_data_t10_d30.root\"\n",
    "\n",
    "fileExist = ROOT.gSystem.AccessPathName(inputFileName)\n",
    "\n",
    "#if file does not exists create it\n",
    "if (fileExist==None):\n",
    "    MakeTimeData(nTotEvts,ntime, ninput)\n",
    "\n",
    "inputFile = ROOT.TFile.Open(inputFileName)\n",
    "if (inputFile==None):\n",
    "    Error(\"TMVA_RNN_Classification\", \"Error opening input file %s - exit\", inputFileName.Data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97be065d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RNNClassification  : Using input file: time_data_t10_d30.root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- RNNClassification  : Using input file: \" + inputFile.GetName()+\"\\n\")\n",
    "\n",
    "#   Create a ROOT output file where TMVA will store ntuples, histograms, etc.\n",
    "outfileName = \"data_RNN_\"+ archString +\".root\"\n",
    "\n",
    "if (writeOutputFile):\n",
    "    outputFile = ROOT.TFile.Open(outfileName, \"RECREATE\")\n",
    "\n",
    "#  Creating the factory object\n",
    "factory = ROOT.TMVA.Factory(\"TMVAClassification\", outputFile,\"!V:!Silent:Color:DrawProgressBar:Transformations=None:!Correlations:\"+\"AnalysisType=Classification:ModelPersistence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2ffc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of variables is 300\n",
      "\n",
      "vars_time0[0]\n",
      "\n",
      "vars_time0[1]\n",
      "\n",
      "vars_time0[2]\n",
      "\n",
      "vars_time0[3]\n",
      "\n",
      "vars_time0[4]\n",
      "\n",
      "vars_time0[5]\n",
      "\n",
      "vars_time0[6]\n",
      "\n",
      "vars_time0[7]\n",
      "\n",
      "vars_time0[8]\n",
      "\n",
      "vars_time0[9]\n",
      "\n",
      "vars_time0[10]\n",
      "\n",
      "vars_time0[11]\n",
      "\n",
      "vars_time0[12]\n",
      "\n",
      "vars_time0[13]\n",
      "\n",
      "vars_time0[14]\n",
      "\n",
      "vars_time0[15]\n",
      "\n",
      "vars_time0[16]\n",
      "\n",
      "vars_time0[17]\n",
      "\n",
      "vars_time0[18]\n",
      "\n",
      "vars_time0[19]\n",
      "\n",
      "vars_time0[20]\n",
      "\n",
      "vars_time0[21]\n",
      "\n",
      "vars_time0[22]\n",
      "\n",
      "vars_time0[23]\n",
      "\n",
      "vars_time0[24]\n",
      "\n",
      "vars_time0[25]\n",
      "\n",
      "vars_time0[26]\n",
      "\n",
      "vars_time0[27]\n",
      "\n",
      "vars_time0[28]\n",
      "\n",
      "vars_time0[29]\n",
      "\n",
      "vars_time1[0]\n",
      "\n",
      "vars_time1[1]\n",
      "\n",
      "vars_time1[2]\n",
      "\n",
      "vars_time1[3]\n",
      "\n",
      "vars_time1[4]\n",
      "\n",
      "vars_time1[5]\n",
      "\n",
      "vars_time1[6]\n",
      "\n",
      "vars_time1[7]\n",
      "\n",
      "vars_time1[8]\n",
      "\n",
      "vars_time1[9]\n",
      "\n",
      "vars_time1[10]\n",
      "\n",
      "vars_time1[11]\n",
      "\n",
      "vars_time1[12]\n",
      "\n",
      "vars_time1[13]\n",
      "\n",
      "vars_time1[14]\n",
      "\n",
      "vars_time1[15]\n",
      "\n",
      "vars_time1[16]\n",
      "\n",
      "vars_time1[17]\n",
      "\n",
      "vars_time1[18]\n",
      "\n",
      "vars_time1[19]\n",
      "\n",
      "vars_time1[20]\n",
      "\n",
      "vars_time1[21]\n",
      "\n",
      "vars_time1[22]\n",
      "\n",
      "vars_time1[23]\n",
      "\n",
      "vars_time1[24]\n",
      "\n",
      "vars_time1[25]\n",
      "\n",
      "vars_time1[26]\n",
      "\n",
      "vars_time1[27]\n",
      "\n",
      "vars_time1[28]\n",
      "\n",
      "vars_time1[29]\n",
      "\n",
      "vars_time2[0]\n",
      "\n",
      "vars_time2[1]\n",
      "\n",
      "vars_time2[2]\n",
      "\n",
      "vars_time2[3]\n",
      "\n",
      "vars_time2[4]\n",
      "\n",
      "vars_time2[5]\n",
      "\n",
      "vars_time2[6]\n",
      "\n",
      "vars_time2[7]\n",
      "\n",
      "vars_time2[8]\n",
      "\n",
      "vars_time2[9]\n",
      "\n",
      "vars_time2[10]\n",
      "\n",
      "vars_time2[11]\n",
      "\n",
      "vars_time2[12]\n",
      "\n",
      "vars_time2[13]\n",
      "\n",
      "vars_time2[14]\n",
      "\n",
      "vars_time2[15]\n",
      "\n",
      "vars_time2[16]\n",
      "\n",
      "vars_time2[17]\n",
      "\n",
      "vars_time2[18]\n",
      "\n",
      "vars_time2[19]\n",
      "\n",
      "vars_time2[20]\n",
      "\n",
      "vars_time2[21]\n",
      "\n",
      "vars_time2[22]\n",
      "\n",
      "vars_time2[23]\n",
      "\n",
      "vars_time2[24]\n",
      "\n",
      "vars_time2[25]\n",
      "\n",
      "vars_time2[26]\n",
      "\n",
      "vars_time2[27]\n",
      "\n",
      "vars_time2[28]\n",
      "\n",
      "vars_time2[29]\n",
      "\n",
      "vars_time3[0]\n",
      "\n",
      "vars_time3[1]\n",
      "\n",
      "vars_time3[2]\n",
      "\n",
      "vars_time3[3]\n",
      "\n",
      "vars_time3[4]\n",
      "\n",
      "vars_time3[5]\n",
      "\n",
      "vars_time3[6]\n",
      "\n",
      "vars_time3[7]\n",
      "\n",
      "vars_time3[8]\n",
      "\n",
      "vars_time3[9]\n",
      "\n",
      "vars_time3[10]\n",
      "\n",
      "vars_time3[11]\n",
      "\n",
      "vars_time3[12]\n",
      "\n",
      "vars_time3[13]\n",
      "\n",
      "vars_time3[14]\n",
      "\n",
      "vars_time3[15]\n",
      "\n",
      "vars_time3[16]\n",
      "\n",
      "vars_time3[17]\n",
      "\n",
      "vars_time3[18]\n",
      "\n",
      "vars_time3[19]\n",
      "\n",
      "vars_time3[20]\n",
      "\n",
      "vars_time3[21]\n",
      "\n",
      "vars_time3[22]\n",
      "\n",
      "vars_time3[23]\n",
      "\n",
      "vars_time3[24]\n",
      "\n",
      "vars_time3[25]\n",
      "\n",
      "vars_time3[26]\n",
      "\n",
      "vars_time3[27]\n",
      "\n",
      "vars_time3[28]\n",
      "\n",
      "vars_time3[29]\n",
      "\n",
      "vars_time4[0]\n",
      "\n",
      "vars_time4[1]\n",
      "\n",
      "vars_time4[2]\n",
      "\n",
      "vars_time4[3]\n",
      "\n",
      "vars_time4[4]\n",
      "\n",
      "vars_time4[5]\n",
      "\n",
      "vars_time4[6]\n",
      "\n",
      "vars_time4[7]\n",
      "\n",
      "vars_time4[8]\n",
      "\n",
      "vars_time4[9]\n",
      "\n",
      "vars_time4[10]\n",
      "\n",
      "vars_time4[11]\n",
      "\n",
      "vars_time4[12]\n",
      "\n",
      "vars_time4[13]\n",
      "\n",
      "vars_time4[14]\n",
      "\n",
      "vars_time4[15]\n",
      "\n",
      "vars_time4[16]\n",
      "\n",
      "vars_time4[17]\n",
      "\n",
      "vars_time4[18]\n",
      "\n",
      "vars_time4[19]\n",
      "\n",
      "vars_time4[20]\n",
      "\n",
      "vars_time4[21]\n",
      "\n",
      "vars_time4[22]\n",
      "\n",
      "vars_time4[23]\n",
      "\n",
      "vars_time4[24]\n",
      "\n",
      "vars_time4[25]\n",
      "\n",
      "vars_time4[26]\n",
      "\n",
      "vars_time4[27]\n",
      "\n",
      "vars_time4[28]\n",
      "\n",
      "vars_time4[29]\n",
      "\n",
      "vars_time5[0]\n",
      "\n",
      "vars_time5[1]\n",
      "\n",
      "vars_time5[2]\n",
      "\n",
      "vars_time5[3]\n",
      "\n",
      "vars_time5[4]\n",
      "\n",
      "vars_time5[5]\n",
      "\n",
      "vars_time5[6]\n",
      "\n",
      "vars_time5[7]\n",
      "\n",
      "vars_time5[8]\n",
      "\n",
      "vars_time5[9]\n",
      "\n",
      "vars_time5[10]\n",
      "\n",
      "vars_time5[11]\n",
      "\n",
      "vars_time5[12]\n",
      "\n",
      "vars_time5[13]\n",
      "\n",
      "vars_time5[14]\n",
      "\n",
      "vars_time5[15]\n",
      "\n",
      "vars_time5[16]\n",
      "\n",
      "vars_time5[17]\n",
      "\n",
      "vars_time5[18]\n",
      "\n",
      "vars_time5[19]\n",
      "\n",
      "vars_time5[20]\n",
      "\n",
      "vars_time5[21]\n",
      "\n",
      "vars_time5[22]\n",
      "\n",
      "vars_time5[23]\n",
      "\n",
      "vars_time5[24]\n",
      "\n",
      "vars_time5[25]\n",
      "\n",
      "vars_time5[26]\n",
      "\n",
      "vars_time5[27]\n",
      "\n",
      "vars_time5[28]\n",
      "\n",
      "vars_time5[29]\n",
      "\n",
      "vars_time6[0]\n",
      "\n",
      "vars_time6[1]\n",
      "\n",
      "vars_time6[2]\n",
      "\n",
      "vars_time6[3]\n",
      "\n",
      "vars_time6[4]\n",
      "\n",
      "vars_time6[5]\n",
      "\n",
      "vars_time6[6]\n",
      "\n",
      "vars_time6[7]\n",
      "\n",
      "vars_time6[8]\n",
      "\n",
      "vars_time6[9]\n",
      "\n",
      "vars_time6[10]\n",
      "\n",
      "vars_time6[11]\n",
      "\n",
      "vars_time6[12]\n",
      "\n",
      "vars_time6[13]\n",
      "\n",
      "vars_time6[14]\n",
      "\n",
      "vars_time6[15]\n",
      "\n",
      "vars_time6[16]\n",
      "\n",
      "vars_time6[17]\n",
      "\n",
      "vars_time6[18]\n",
      "\n",
      "vars_time6[19]\n",
      "\n",
      "vars_time6[20]\n",
      "\n",
      "vars_time6[21]\n",
      "\n",
      "vars_time6[22]\n",
      "\n",
      "vars_time6[23]\n",
      "\n",
      "vars_time6[24]\n",
      "\n",
      "vars_time6[25]\n",
      "\n",
      "vars_time6[26]\n",
      "\n",
      "vars_time6[27]\n",
      "\n",
      "vars_time6[28]\n",
      "\n",
      "vars_time6[29]\n",
      "\n",
      "vars_time7[0]\n",
      "\n",
      "vars_time7[1]\n",
      "\n",
      "vars_time7[2]\n",
      "\n",
      "vars_time7[3]\n",
      "\n",
      "vars_time7[4]\n",
      "\n",
      "vars_time7[5]\n",
      "\n",
      "vars_time7[6]\n",
      "\n",
      "vars_time7[7]\n",
      "\n",
      "vars_time7[8]\n",
      "\n",
      "vars_time7[9]\n",
      "\n",
      "vars_time7[10]\n",
      "\n",
      "vars_time7[11]\n",
      "\n",
      "vars_time7[12]\n",
      "\n",
      "vars_time7[13]\n",
      "\n",
      "vars_time7[14]\n",
      "\n",
      "vars_time7[15]\n",
      "\n",
      "vars_time7[16]\n",
      "\n",
      "vars_time7[17]\n",
      "\n",
      "vars_time7[18]\n",
      "\n",
      "vars_time7[19]\n",
      "\n",
      "vars_time7[20]\n",
      "\n",
      "vars_time7[21]\n",
      "\n",
      "vars_time7[22]\n",
      "\n",
      "vars_time7[23]\n",
      "\n",
      "vars_time7[24]\n",
      "\n",
      "vars_time7[25]\n",
      "\n",
      "vars_time7[26]\n",
      "\n",
      "vars_time7[27]\n",
      "\n",
      "vars_time7[28]\n",
      "\n",
      "vars_time7[29]\n",
      "\n",
      "vars_time8[0]\n",
      "\n",
      "vars_time8[1]\n",
      "\n",
      "vars_time8[2]\n",
      "\n",
      "vars_time8[3]\n",
      "\n",
      "vars_time8[4]\n",
      "\n",
      "vars_time8[5]\n",
      "\n",
      "vars_time8[6]\n",
      "\n",
      "vars_time8[7]\n",
      "\n",
      "vars_time8[8]\n",
      "\n",
      "vars_time8[9]\n",
      "\n",
      "vars_time8[10]\n",
      "\n",
      "vars_time8[11]\n",
      "\n",
      "vars_time8[12]\n",
      "\n",
      "vars_time8[13]\n",
      "\n",
      "vars_time8[14]\n",
      "\n",
      "vars_time8[15]\n",
      "\n",
      "vars_time8[16]\n",
      "\n",
      "vars_time8[17]\n",
      "\n",
      "vars_time8[18]\n",
      "\n",
      "vars_time8[19]\n",
      "\n",
      "vars_time8[20]\n",
      "\n",
      "vars_time8[21]\n",
      "\n",
      "vars_time8[22]\n",
      "\n",
      "vars_time8[23]\n",
      "\n",
      "vars_time8[24]\n",
      "\n",
      "vars_time8[25]\n",
      "\n",
      "vars_time8[26]\n",
      "\n",
      "vars_time8[27]\n",
      "\n",
      "vars_time8[28]\n",
      "\n",
      "vars_time8[29]\n",
      "\n",
      "vars_time9[0]\n",
      "\n",
      "vars_time9[1]\n",
      "\n",
      "vars_time9[2]\n",
      "\n",
      "vars_time9[3]\n",
      "\n",
      "vars_time9[4]\n",
      "\n",
      "vars_time9[5]\n",
      "\n",
      "vars_time9[6]\n",
      "\n",
      "vars_time9[7]\n",
      "\n",
      "vars_time9[8]\n",
      "\n",
      "vars_time9[9]\n",
      "\n",
      "vars_time9[10]\n",
      "\n",
      "vars_time9[11]\n",
      "\n",
      "vars_time9[12]\n",
      "\n",
      "vars_time9[13]\n",
      "\n",
      "vars_time9[14]\n",
      "\n",
      "vars_time9[15]\n",
      "\n",
      "vars_time9[16]\n",
      "\n",
      "vars_time9[17]\n",
      "\n",
      "vars_time9[18]\n",
      "\n",
      "vars_time9[19]\n",
      "\n",
      "vars_time9[20]\n",
      "\n",
      "vars_time9[21]\n",
      "\n",
      "vars_time9[22]\n",
      "\n",
      "vars_time9[23]\n",
      "\n",
      "vars_time9[24]\n",
      "\n",
      "vars_time9[25]\n",
      "\n",
      "vars_time9[26]\n",
      "\n",
      "vars_time9[27]\n",
      "\n",
      "vars_time9[28]\n",
      "\n",
      "vars_time9[29]\n",
      "\n",
      "******************************************************************************\n",
      "*Tree    :sgn       : sgn                                                    *\n",
      "*Entries :    10000 : Total =        13449901 bytes  File  Size =   11258248 *\n",
      "*        :          : Tree compression factor =   1.19                       *\n",
      "******************************************************************************\n",
      "*Br    0 :vars_time0 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1124563 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    1 :vars_time1 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1124654 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    2 :vars_time2 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1124757 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    3 :vars_time3 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1124807 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    4 :vars_time4 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1125129 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    5 :vars_time5 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1125625 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    6 :vars_time6 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1125720 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    7 :vars_time7 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1126131 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    8 :vars_time8 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1126511 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    9 :vars_time9 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1126524 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "DataSetInfo              : [dataset] : Added class \"Signal\"\n",
      "                         : Add Tree sgn of type Signal with 10000 events\n",
      "DataSetInfo              : [dataset] : Added class \"Background\"\n",
      "                         : Add Tree bkg of type Background with 10000 events\n"
     ]
    }
   ],
   "source": [
    "dataloader =ROOT.TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "signalTree = inputFile.Get(\"sgn\")\n",
    "background = inputFile.Get(\"bkg\")\n",
    "\n",
    "signalTree.Print()\n",
    "nvar = ninput * ntime\n",
    "\n",
    "# add variables - use new AddVariablesArray function\n",
    "for i in range(ntime):\n",
    "    varName = \"vars_time\"+str(i)\n",
    "    dataloader.AddVariablesArray(varName,ninput,'F')\n",
    "\n",
    "dataloader.AddSignalTree(signalTree, 1.0)\n",
    "dataloader.AddBackgroundTree(background, 1.0)\n",
    "\n",
    "# check given input\n",
    "datainfo = dataloader.GetDataSetInfo()\n",
    "vars = datainfo.GetListOfVariables()\n",
    "print(\"number of variables is \" + str(vars.size())+ \"\\n\")\n",
    "for v in vars:\n",
    "    print(str(v)+\"\\n\")\n",
    "\n",
    "nTrainSig = 0.8 * nTotEvts\n",
    "nTrainBkg = 0.8 *  nTotEvts\n",
    "\n",
    "#build the string options for DataLoader::PrepareTrainingAndTestTree\n",
    "prepareOptions = \"nTrain_Signal=\"+str(nTrainSig)+\":nTrain_Background=\"+str(nTrainBkg)+\":SplitMode=Random:SplitSeed=100:NormMode=NumEvents:!V:!CalcCorrelations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e2f198d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared DATA LOADER \n"
     ]
    }
   ],
   "source": [
    "# Apply additional cuts on the signal and background samples (can be different)\n",
    "mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "dataloader.PrepareTrainingAndTestTree(mycuts, mycutb, prepareOptions)\n",
    "\n",
    "print(\"prepared DATA LOADER \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89b5ec65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mTMVA_RNN\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=RNN|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=RNN|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"10|30\" [The Layout of the input]\n",
      "                         :     Layout: \"RNN|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     RandomSeed: \"1234\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     ValidationSize: \"0.2\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n",
      "Factory                  : Booking method: \u001b[1mTMVA_LSTM\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"10|30\" [The Layout of the input]\n",
      "                         :     Layout: \"LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     RandomSeed: \"1234\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     ValidationSize: \"0.2\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n",
      "Factory                  : Booking method: \u001b[1mTMVA_GRU\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=GRU|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=GRU|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"10|30\" [The Layout of the input]\n",
      "                         :     Layout: \"GRU|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     RandomSeed: \"1234\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     ValidationSize: \"0.2\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if (useTMVA_RNN):\n",
    "    for i in range(3):\n",
    "        if (use_rnn_type[i]==None):\n",
    "            continue\n",
    "        rnn_type = str(rnn_types[i])\n",
    "\n",
    "#          define the inputlayout string for RNN\n",
    "#          the input data should be organize as   following:\n",
    "#          input layout for RNN:    time x ndim\n",
    "\n",
    "        inputLayoutString = \"InputLayout=\"+str(ntime)+\"|\"+str(ninput)\n",
    "\n",
    "        # Define RNN layer layout\n",
    "        # it should be   LayerType (RNN or LSTM or GRU) |  number of units | number of inputs | time steps | remember output (typically no=0 | return full sequence\n",
    "        rnnLayout = str(rnn_type) + \"|10|\"+ str(ninput) + \"|\" + str(ntime) + \"|0|1\"\n",
    "\n",
    "        #        add after RNN a reshape layer (needed top flatten the output) and a dense layer with 64 units and a last one\n",
    "        #        Note the last layer is linear because  when using Crossentropy a Sigmoid is applied already\n",
    "        layoutString =\"Layout=\" + rnnLayout + \",RESHAPE|FLAT,DENSE|64|TANH,LINEAR\"\n",
    "\n",
    "        #Defining Training strategies. Different training strings can be concatenate. Use however only one\n",
    "        trainingString1 = \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,\"+\"ConvergenceSteps=5,BatchSize=\"+str(batchSize)+\",TestRepetitions=1,\"+\"WeightDecay=1e-2,Regularization=None,MaxEpochs=\"+str(maxepochs\n",
    "        )+\",\"+\"Optimizer=ADAM,DropConfig=0.0+0.+0.+0.\"\n",
    "\n",
    "        trainingStrategyString=\"TrainingStrategy=\"\n",
    "        trainingStrategyString += trainingString1; # + \"|\" + trainingString2\n",
    "\n",
    "        # Define the full RNN Noption string adding the final options for all network\n",
    "        rnnOptions = \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"+\"WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234\"\n",
    "        rnnOptions +=  \":\" + inputLayoutString\n",
    "        rnnOptions +=  \":\" + layoutString\n",
    "        rnnOptions +=  \":\" + trainingStrategyString\n",
    "        rnnOptions +=  \":\" + \"Architecture=\" + str(archString)\n",
    "\n",
    "        rnnName = \"TMVA_\" + rnn_type\n",
    "        factory.BookMethod(dataloader, TMVA.Types.kDL, rnnName, rnnOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e3b5474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mTMVA_DNN\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"1|1|300\" [The Layout of the input]\n",
      "                         :     Layout: \"DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIER\" [Weight initialization strategy]\n",
      "                         :     RandomSeed: \"0\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         :     ValidationSize: \"20%\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n"
     ]
    }
   ],
   "source": [
    "if (useTMVA_DNN):\n",
    "#    Method DL with Dense Layer\n",
    "    inputLayoutString = \"InputLayout=1|1|\" + str(ntime * ninput)\n",
    "\n",
    "    layoutString = \"Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR\"\n",
    "#   Training strategies.\n",
    "    trainingString1 = \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,\"+\"ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,\"+\"WeightDecay=1e-4,Regularization=None,MaxEpochs=20\"+\"DropConfig=0.0+0.+0.+0.,Optimizer=ADAM\"\n",
    "    trainingStrategyString = \"TrainingStrategy=\"\n",
    "    trainingStrategyString += trainingString1 # + \"|\" + trainingString2\n",
    "\n",
    "      # General Options.\n",
    "    dnnOptions = \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"+\"WeightInitialization=XAVIER:RandomSeed=0\" \n",
    "\n",
    "    dnnOptions +=  \":\" + inputLayoutString\n",
    "    dnnOptions +=  \":\" + layoutString\n",
    "    dnnOptions +=  \":\" + trainingStrategyString\n",
    "    dnnOptions +=  \":\" + \"Architecture=\" + str(archString)\n",
    "\n",
    "\n",
    "    dnnName = \"TMVA_DNN\"\n",
    "    factory.BookMethod(dataloader, TMVA.Types.kDL, dnnName, dnnOptions)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "903e155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Define model\n",
    "\n",
    "# Custom Reshape Layer\n",
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1,1,10,30)\n",
    "\n",
    "# CNN Model Definition\n",
    "net = torch.nn.Sequential(\n",
    "    Reshape(),\n",
    "    nn.Conv2d(1, 10, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(10),\n",
    "    nn.Conv2d(10, 10, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(10*5*15, 300),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(300, 2),\n",
    "    nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "# Construct loss function and Optimizer.\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam\n",
    "\n",
    "\n",
    "def fit(model, train_loader, val_loader, num_epochs, batch_size, optimizer, criterion, save_best, scheduler):\n",
    "    trainer = optimizer(model.parameters(), lr=0.01)\n",
    "    schedule, schedulerSteps = scheduler\n",
    "    best_val = None\n",
    "\n",
    "    # Setup GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Loop\n",
    "        # Set to train mode\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        running_val_loss = 0.0\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            output = model(X)\n",
    "            target = y\n",
    "            train_loss = criterion(output, target)\n",
    "            train_loss.backward()\n",
    "            trainer.step()\n",
    "\n",
    "            # print train statistics\n",
    "            running_train_loss += train_loss.item()\n",
    "            if i % 4 == 3:    # print every 4 mini-batches\n",
    "                print(f\"[{epoch+1}, {i+1}] train loss: {running_train_loss / 4 :.3f}\")\n",
    "                running_train_loss = 0.0\n",
    "\n",
    "        if schedule:\n",
    "            schedule(optimizer, epoch, schedulerSteps)\n",
    "\n",
    "        # Validation Loop\n",
    "        # Set to eval mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (X, y) in enumerate(val_loader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                output = model(X)\n",
    "                target = y\n",
    "                val_loss = criterion(output, target)\n",
    "                running_val_loss += val_loss.item()\n",
    "\n",
    "            curr_val = running_val_loss / len(val_loader)\n",
    "            if save_best:\n",
    "               if best_val==None:\n",
    "                   best_val = curr_val\n",
    "               best_val = save_best(model, curr_val, best_val)\n",
    "\n",
    "            # print val statistics per epoch\n",
    "            print(f\"[{epoch+1}] val loss: {curr_val :.3f}\")\n",
    "            running_val_loss = 0.0\n",
    "\n",
    "    print(f\"Finished Training on {epoch+1} Epochs!\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, test_X, batch_size=100):\n",
    "    # Set to eval mode\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(torch.Tensor(test_X))\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            X = data[0].to(device)\n",
    "            outputs = model(X)\n",
    "            predictions.append(outputs)\n",
    "        preds = torch.cat(predictions)\n",
    "\n",
    "    return preds.cpu().numpy()\n",
    "\n",
    "\n",
    "load_model_custom_objects = {\"optimizer\": optimizer, \"criterion\": criterion, \"train_func\": fit, \"predict_func\": predict}\n",
    "\n",
    "# Store model to file\n",
    "m = torch.jit.script(net)\n",
    "torch.jit.save(m,\"PyTorchModelRNN.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5165e03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom objects for loading model :  {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7ff54b9ff280>, 'predict_func': <function predict at 0x7ff54b9ff310>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cppyy.gbl.TMVA.MethodPyTorch object at 0xbc0c350>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mPyTorch\u001b[0m\n",
      "                         : \n",
      "                         : Using PyTorch - setting special configuration options \n",
      "                         : Using PyTorch version 1\n",
      "                         :  Setup PyTorch Model \n",
      "                         : Loaded pytorch train function: \n",
      "                         : Loaded pytorch optimizer: \n",
      "                         : Loaded pytorch loss function: \n",
      "                         : Loaded pytorch predict function: \n",
      "                         : Load model from file: PyTorchModelRNN.pt\n"
     ]
    }
   ],
   "source": [
    "factory.BookMethod(dataloader, ROOT.TMVA.Types.kPyTorch, \"PyTorch\",\"H:!V:VarTransform=None:FilenameModel=PyTorchModelRNN.pt:\" + \"FilenameTrainedModel=PyTorchTrainedModelRNN.pt:NumEpochs=20:BatchSize=100\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd4f627a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=Sequential\n",
      "  (0): RecursiveScriptModule(original_name=Reshape)\n",
      "  (1): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (2): RecursiveScriptModule(original_name=ReLU)\n",
      "  (3): RecursiveScriptModule(original_name=BatchNorm2d)\n",
      "  (4): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (5): RecursiveScriptModule(original_name=ReLU)\n",
      "  (6): RecursiveScriptModule(original_name=MaxPool2d)\n",
      "  (7): RecursiveScriptModule(original_name=Flatten)\n",
      "  (8): RecursiveScriptModule(original_name=Linear)\n",
      "  (9): RecursiveScriptModule(original_name=ReLU)\n",
      "  (10): RecursiveScriptModule(original_name=Linear)\n",
      "  (11): RecursiveScriptModule(original_name=Sigmoid)\n",
      ")\n",
      "[1, 4] train loss: 1.246\n",
      "[1, 8] train loss: 0.732\n",
      "[1, 12] train loss: 0.675\n",
      "[1, 16] train loss: 0.660\n",
      "[1, 20] train loss: 0.680\n",
      "[1, 24] train loss: 0.584\n",
      "[1, 28] train loss: 0.639\n",
      "[1, 32] train loss: 0.517\n",
      "[1, 36] train loss: 0.514\n",
      "[1, 40] train loss: 0.494\n",
      "[1, 44] train loss: 0.478\n",
      "[1, 48] train loss: 0.428\n",
      "[1, 52] train loss: 0.428\n",
      "[1, 56] train loss: 0.418\n",
      "[1, 60] train loss: 0.469\n",
      "[1, 64] train loss: 0.437\n",
      "[1, 68] train loss: 0.463\n",
      "[1, 72] train loss: 0.435\n",
      "[1, 76] train loss: 0.386\n",
      "[1, 80] train loss: 0.373\n",
      "[1, 84] train loss: 0.441\n",
      "[1, 88] train loss: 0.469\n",
      "[1, 92] train loss: 0.424\n",
      "[1, 96] train loss: 0.416\n",
      "[1, 100] train loss: 0.373\n",
      "[1, 104] train loss: 0.409\n",
      "[1, 108] train loss: 0.416\n",
      "[1, 112] train loss: 0.409\n",
      "[1, 116] train loss: 0.427\n",
      "[1, 120] train loss: 0.403\n",
      "[1, 124] train loss: 0.473\n",
      "[1, 128] train loss: 0.508\n",
      "[1] val loss: 0.518\n",
      "[2, 4] train loss: 0.459\n",
      "[2, 8] train loss: 0.399\n",
      "[2, 12] train loss: 0.368\n",
      "[2, 16] train loss: 0.348\n",
      "[2, 20] train loss: 0.377\n",
      "[2, 24] train loss: 0.379\n",
      "[2, 28] train loss: 0.437\n",
      "[2, 32] train loss: 0.439\n",
      "[2, 36] train loss: 0.420\n",
      "[2, 40] train loss: 0.427\n",
      "[2, 44] train loss: 0.419\n",
      "[2, 48] train loss: 0.380\n",
      "[2, 52] train loss: 0.366\n",
      "[2, 56] train loss: 0.349\n",
      "[2, 60] train loss: 0.412\n",
      "[2, 64] train loss: 0.428\n",
      "[2, 68] train loss: 0.426\n",
      "[2, 72] train loss: 0.402\n",
      "[2, 76] train loss: 0.362\n",
      "[2, 80] train loss: 0.347\n",
      "[2, 84] train loss: 0.405\n",
      "[2, 88] train loss: 0.400\n",
      "[2, 92] train loss: 0.400\n",
      "[2, 96] train loss: 0.385\n",
      "[2, 100] train loss: 0.348\n",
      "[2, 104] train loss: 0.372\n",
      "[2, 108] train loss: 0.383\n",
      "[2, 112] train loss: 0.362\n",
      "[2, 116] train loss: 0.394\n",
      "[2, 120] train loss: 0.382\n",
      "[2, 124] train loss: 0.441\n",
      "[2, 128] train loss: 0.479\n",
      "[2] val loss: 0.437\n",
      "[3, 4] train loss: 0.421\n",
      "[3, 8] train loss: 0.375\n",
      "[3, 12] train loss: 0.376\n",
      "[3, 16] train loss: 0.348\n",
      "[3, 20] train loss: 0.383\n",
      "[3, 24] train loss: 0.387\n",
      "[3, 28] train loss: 0.492\n",
      "[3, 32] train loss: 0.405\n",
      "[3, 36] train loss: 0.429\n",
      "[3, 40] train loss: 0.444\n",
      "[3, 44] train loss: 0.396\n",
      "[3, 48] train loss: 0.361\n",
      "[3, 52] train loss: 0.347\n",
      "[3, 56] train loss: 0.334\n",
      "[3, 60] train loss: 0.390\n",
      "[3, 64] train loss: 0.404\n",
      "[3, 68] train loss: 0.396\n",
      "[3, 72] train loss: 0.391\n",
      "[3, 76] train loss: 0.353\n",
      "[3, 80] train loss: 0.339\n",
      "[3, 84] train loss: 0.369\n",
      "[3, 88] train loss: 0.368\n",
      "[3, 92] train loss: 0.399\n",
      "[3, 96] train loss: 0.365\n",
      "[3, 100] train loss: 0.360\n",
      "[3, 104] train loss: 0.372\n",
      "[3, 108] train loss: 0.385\n",
      "[3, 112] train loss: 0.354\n",
      "[3, 116] train loss: 0.376\n",
      "[3, 120] train loss: 0.359\n",
      "[3, 124] train loss: 0.448\n",
      "[3, 128] train loss: 0.486\n",
      "[3] val loss: 0.483\n",
      "[4, 4] train loss: 0.416\n",
      "[4, 8] train loss: 0.375\n",
      "[4, 12] train loss: 0.344\n",
      "[4, 16] train loss: 0.311\n",
      "[4, 20] train loss: 0.342\n",
      "[4, 24] train loss: 0.355\n",
      "[4, 28] train loss: 0.427\n",
      "[4, 32] train loss: 0.400\n",
      "[4, 36] train loss: 0.385\n",
      "[4, 40] train loss: 0.412\n",
      "[4, 44] train loss: 0.415\n",
      "[4, 48] train loss: 0.354\n",
      "[4, 52] train loss: 0.329\n",
      "[4, 56] train loss: 0.317\n",
      "[4, 60] train loss: 0.391\n",
      "[4, 64] train loss: 0.394\n",
      "[4, 68] train loss: 0.386\n",
      "[4, 72] train loss: 0.387\n",
      "[4, 76] train loss: 0.358\n",
      "[4, 80] train loss: 0.330\n",
      "[4, 84] train loss: 0.364\n",
      "[4, 88] train loss: 0.363\n",
      "[4, 92] train loss: 0.392\n",
      "[4, 96] train loss: 0.351\n",
      "[4, 100] train loss: 0.342\n",
      "[4, 104] train loss: 0.355\n",
      "[4, 108] train loss: 0.369\n",
      "[4, 112] train loss: 0.339\n",
      "[4, 116] train loss: 0.353\n",
      "[4, 120] train loss: 0.346\n",
      "[4, 124] train loss: 0.432\n",
      "[4, 128] train loss: 0.466\n",
      "[4] val loss: 0.455\n",
      "[5, 4] train loss: 0.395\n",
      "[5, 8] train loss: 0.359\n",
      "[5, 12] train loss: 0.344\n",
      "[5, 16] train loss: 0.316\n",
      "[5, 20] train loss: 0.346\n",
      "[5, 24] train loss: 0.359\n",
      "[5, 28] train loss: 0.456\n",
      "[5, 32] train loss: 0.397\n",
      "[5, 36] train loss: 0.369\n",
      "[5, 40] train loss: 0.429\n",
      "[5, 44] train loss: 0.411\n",
      "[5, 48] train loss: 0.359\n",
      "[5, 52] train loss: 0.341\n",
      "[5, 56] train loss: 0.327\n",
      "[5, 60] train loss: 0.396\n",
      "[5, 64] train loss: 0.404\n",
      "[5, 68] train loss: 0.384\n",
      "[5, 72] train loss: 0.392\n",
      "[5, 76] train loss: 0.362\n",
      "[5, 80] train loss: 0.328\n",
      "[5, 84] train loss: 0.367\n",
      "[5, 88] train loss: 0.342\n",
      "[5, 92] train loss: 0.386\n",
      "[5, 96] train loss: 0.354\n",
      "[5, 100] train loss: 0.368\n",
      "[5, 104] train loss: 0.373\n",
      "[5, 108] train loss: 0.377\n",
      "[5, 112] train loss: 0.339\n",
      "[5, 116] train loss: 0.350\n",
      "[5, 120] train loss: 0.341\n",
      "[5, 124] train loss: 0.414\n",
      "[5, 128] train loss: 0.437\n",
      "[5] val loss: 0.415\n",
      "[6, 4] train loss: 0.377\n",
      "[6, 8] train loss: 0.347\n",
      "[6, 12] train loss: 0.342\n",
      "[6, 16] train loss: 0.320\n",
      "[6, 20] train loss: 0.355\n",
      "[6, 24] train loss: 0.362\n",
      "[6, 28] train loss: 0.438\n",
      "[6, 32] train loss: 0.378\n",
      "[6, 36] train loss: 0.378\n",
      "[6, 40] train loss: 0.435\n",
      "[6, 44] train loss: 0.397\n",
      "[6, 48] train loss: 0.356\n",
      "[6, 52] train loss: 0.345\n",
      "[6, 56] train loss: 0.336\n",
      "[6, 60] train loss: 0.421\n",
      "[6, 64] train loss: 0.394\n",
      "[6, 68] train loss: 0.400\n",
      "[6, 72] train loss: 0.386\n",
      "[6, 76] train loss: 0.339\n",
      "[6, 80] train loss: 0.322\n",
      "[6, 84] train loss: 0.353\n",
      "[6, 88] train loss: 0.341\n",
      "[6, 92] train loss: 0.390\n",
      "[6, 96] train loss: 0.344\n",
      "[6, 100] train loss: 0.358\n",
      "[6, 104] train loss: 0.366\n",
      "[6, 108] train loss: 0.374\n",
      "[6, 112] train loss: 0.336\n",
      "[6, 116] train loss: 0.347\n",
      "[6, 120] train loss: 0.341\n",
      "[6, 124] train loss: 0.409\n",
      "[6, 128] train loss: 0.424\n",
      "[6] val loss: 0.404\n",
      "[7, 4] train loss: 0.372\n",
      "[7, 8] train loss: 0.345\n",
      "[7, 12] train loss: 0.335\n",
      "[7, 16] train loss: 0.306\n",
      "[7, 20] train loss: 0.348\n",
      "[7, 24] train loss: 0.366\n",
      "[7, 28] train loss: 0.444\n",
      "[7, 32] train loss: 0.370\n",
      "[7, 36] train loss: 0.371\n",
      "[7, 40] train loss: 0.425\n",
      "[7, 44] train loss: 0.389\n",
      "[7, 48] train loss: 0.355\n",
      "[7, 52] train loss: 0.349\n",
      "[7, 56] train loss: 0.323\n",
      "[7, 60] train loss: 0.414\n",
      "[7, 64] train loss: 0.382\n",
      "[7, 68] train loss: 0.399\n",
      "[7, 72] train loss: 0.375\n",
      "[7, 76] train loss: 0.336\n",
      "[7, 80] train loss: 0.318\n",
      "[7, 84] train loss: 0.350\n",
      "[7, 88] train loss: 0.340\n",
      "[7, 92] train loss: 0.379\n",
      "[7, 96] train loss: 0.342\n",
      "[7, 100] train loss: 0.349\n",
      "[7, 104] train loss: 0.353\n",
      "[7, 108] train loss: 0.359\n",
      "[7, 112] train loss: 0.329\n",
      "[7, 116] train loss: 0.339\n",
      "[7, 120] train loss: 0.340\n",
      "[7, 124] train loss: 0.404\n",
      "[7, 128] train loss: 0.423\n",
      "[7] val loss: 0.402\n",
      "[8, 4] train loss: 0.367\n",
      "[8, 8] train loss: 0.338\n",
      "[8, 12] train loss: 0.331\n",
      "[8, 16] train loss: 0.307\n",
      "[8, 20] train loss: 0.346\n",
      "[8, 24] train loss: 0.357\n",
      "[8, 28] train loss: 0.436\n",
      "[8, 32] train loss: 0.365\n",
      "[8, 36] train loss: 0.366\n",
      "[8, 40] train loss: 0.418\n",
      "[8, 44] train loss: 0.385\n",
      "[8, 48] train loss: 0.348\n",
      "[8, 52] train loss: 0.335\n",
      "[8, 56] train loss: 0.325\n",
      "[8, 60] train loss: 0.406\n",
      "[8, 64] train loss: 0.387\n",
      "[8, 68] train loss: 0.394\n",
      "[8, 72] train loss: 0.375\n",
      "[8, 76] train loss: 0.334\n",
      "[8, 80] train loss: 0.315\n",
      "[8, 84] train loss: 0.344\n",
      "[8, 88] train loss: 0.335\n",
      "[8, 92] train loss: 0.380\n",
      "[8, 96] train loss: 0.341\n",
      "[8, 100] train loss: 0.351\n",
      "[8, 104] train loss: 0.350\n",
      "[8, 108] train loss: 0.359\n",
      "[8, 112] train loss: 0.329\n",
      "[8, 116] train loss: 0.337\n",
      "[8, 120] train loss: 0.343\n",
      "[8, 124] train loss: 0.402\n",
      "[8, 128] train loss: 0.426\n",
      "[8] val loss: 0.416\n",
      "[9, 4] train loss: 0.366\n",
      "[9, 8] train loss: 0.339\n",
      "[9, 12] train loss: 0.323\n",
      "[9, 16] train loss: 0.295\n",
      "[9, 20] train loss: 0.335\n",
      "[9, 24] train loss: 0.345\n",
      "[9, 28] train loss: 0.428\n",
      "[9, 32] train loss: 0.362\n",
      "[9, 36] train loss: 0.359\n",
      "[9, 40] train loss: 0.410\n",
      "[9, 44] train loss: 0.381\n",
      "[9, 48] train loss: 0.339\n",
      "[9, 52] train loss: 0.325\n",
      "[9, 56] train loss: 0.316\n",
      "[9, 60] train loss: 0.400\n",
      "[9, 64] train loss: 0.385\n",
      "[9, 68] train loss: 0.393\n",
      "[9, 72] train loss: 0.374\n",
      "[9, 76] train loss: 0.329\n",
      "[9, 80] train loss: 0.309\n",
      "[9, 84] train loss: 0.329\n",
      "[9, 88] train loss: 0.318\n",
      "[9, 92] train loss: 0.382\n",
      "[9, 96] train loss: 0.340\n",
      "[9, 100] train loss: 0.355\n",
      "[9, 104] train loss: 0.348\n",
      "[9, 108] train loss: 0.359\n",
      "[9, 112] train loss: 0.324\n",
      "[9, 116] train loss: 0.337\n",
      "[9, 120] train loss: 0.341\n",
      "[9, 124] train loss: 0.409\n",
      "[9, 128] train loss: 0.435\n",
      "[9] val loss: 0.427\n",
      "[10, 4] train loss: 0.373\n",
      "[10, 8] train loss: 0.344\n",
      "[10, 12] train loss: 0.321\n",
      "[10, 16] train loss: 0.286\n",
      "[10, 20] train loss: 0.335\n",
      "[10, 24] train loss: 0.341\n",
      "[10, 28] train loss: 0.443\n",
      "[10, 32] train loss: 0.364\n",
      "[10, 36] train loss: 0.354\n",
      "[10, 40] train loss: 0.414\n",
      "[10, 44] train loss: 0.381\n",
      "[10, 48] train loss: 0.346\n",
      "[10, 52] train loss: 0.326\n",
      "[10, 56] train loss: 0.307\n",
      "[10, 60] train loss: 0.398\n",
      "[10, 64] train loss: 0.378\n",
      "[10, 68] train loss: 0.397\n",
      "[10, 72] train loss: 0.370\n",
      "[10, 76] train loss: 0.326\n",
      "[10, 80] train loss: 0.308\n",
      "[10, 84] train loss: 0.325\n",
      "[10, 88] train loss: 0.315\n",
      "[10, 92] train loss: 0.370\n",
      "[10, 96] train loss: 0.330\n",
      "[10, 100] train loss: 0.344\n",
      "[10, 104] train loss: 0.346\n",
      "[10, 108] train loss: 0.348\n",
      "[10, 112] train loss: 0.330\n",
      "[10, 116] train loss: 0.336\n",
      "[10, 120] train loss: 0.334\n",
      "[10, 124] train loss: 0.406\n",
      "[10, 128] train loss: 0.429\n",
      "[10] val loss: 0.429\n",
      "[11, 4] train loss: 0.371\n",
      "[11, 8] train loss: 0.351\n",
      "[11, 12] train loss: 0.329\n",
      "[11, 16] train loss: 0.296\n",
      "[11, 20] train loss: 0.337\n",
      "[11, 24] train loss: 0.338\n",
      "[11, 28] train loss: 0.425\n",
      "[11, 32] train loss: 0.352\n",
      "[11, 36] train loss: 0.370\n",
      "[11, 40] train loss: 0.405\n",
      "[11, 44] train loss: 0.380\n",
      "[11, 48] train loss: 0.362\n",
      "[11, 52] train loss: 0.337\n",
      "[11, 56] train loss: 0.305\n",
      "[11, 60] train loss: 0.386\n",
      "[11, 64] train loss: 0.369\n",
      "[11, 68] train loss: 0.377\n",
      "[11, 72] train loss: 0.361\n",
      "[11, 76] train loss: 0.321\n",
      "[11, 80] train loss: 0.303\n",
      "[11, 84] train loss: 0.322\n",
      "[11, 88] train loss: 0.314\n",
      "[11, 92] train loss: 0.363\n",
      "[11, 96] train loss: 0.322\n",
      "[11, 100] train loss: 0.338\n",
      "[11, 104] train loss: 0.344\n",
      "[11, 108] train loss: 0.346\n",
      "[11, 112] train loss: 0.322\n",
      "[11, 116] train loss: 0.326\n",
      "[11, 120] train loss: 0.328\n",
      "[11, 124] train loss: 0.399\n",
      "[11, 128] train loss: 0.412\n",
      "[11] val loss: 0.430\n",
      "[12, 4] train loss: 0.361\n",
      "[12, 8] train loss: 0.343\n",
      "[12, 12] train loss: 0.322\n",
      "[12, 16] train loss: 0.291\n",
      "[12, 20] train loss: 0.339\n",
      "[12, 24] train loss: 0.317\n",
      "[12, 28] train loss: 0.400\n",
      "[12, 32] train loss: 0.337\n",
      "[12, 36] train loss: 0.332\n",
      "[12, 40] train loss: 0.395\n",
      "[12, 44] train loss: 0.376\n",
      "[12, 48] train loss: 0.340\n",
      "[12, 52] train loss: 0.323\n",
      "[12, 56] train loss: 0.290\n",
      "[12, 60] train loss: 0.379\n",
      "[12, 64] train loss: 0.371\n",
      "[12, 68] train loss: 0.378\n",
      "[12, 72] train loss: 0.360\n",
      "[12, 76] train loss: 0.310\n",
      "[12, 80] train loss: 0.306\n",
      "[12, 84] train loss: 0.323\n",
      "[12, 88] train loss: 0.309\n",
      "[12, 92] train loss: 0.368\n",
      "[12, 96] train loss: 0.314\n",
      "[12, 100] train loss: 0.323\n",
      "[12, 104] train loss: 0.337\n",
      "[12, 108] train loss: 0.340\n",
      "[12, 112] train loss: 0.329\n",
      "[12, 116] train loss: 0.329\n",
      "[12, 120] train loss: 0.338\n",
      "[12, 124] train loss: 0.395\n",
      "[12, 128] train loss: 0.390\n",
      "[12] val loss: 0.410\n",
      "[13, 4] train loss: 0.355\n",
      "[13, 8] train loss: 0.333\n",
      "[13, 12] train loss: 0.321\n",
      "[13, 16] train loss: 0.294\n",
      "[13, 20] train loss: 0.336\n",
      "[13, 24] train loss: 0.316\n",
      "[13, 28] train loss: 0.402\n",
      "[13, 32] train loss: 0.341\n",
      "[13, 36] train loss: 0.351\n",
      "[13, 40] train loss: 0.397\n",
      "[13, 44] train loss: 0.363\n",
      "[13, 48] train loss: 0.352\n",
      "[13, 52] train loss: 0.323\n",
      "[13, 56] train loss: 0.293\n",
      "[13, 60] train loss: 0.381\n",
      "[13, 64] train loss: 0.360\n",
      "[13, 68] train loss: 0.388\n",
      "[13, 72] train loss: 0.364\n",
      "[13, 76] train loss: 0.323\n",
      "[13, 80] train loss: 0.299\n",
      "[13, 84] train loss: 0.317\n",
      "[13, 88] train loss: 0.301\n",
      "[13, 92] train loss: 0.352\n",
      "[13, 96] train loss: 0.305\n",
      "[13, 100] train loss: 0.313\n",
      "[13, 104] train loss: 0.323\n",
      "[13, 108] train loss: 0.332\n",
      "[13, 112] train loss: 0.312\n",
      "[13, 116] train loss: 0.322\n",
      "[13, 120] train loss: 0.320\n",
      "[13, 124] train loss: 0.388\n",
      "[13, 128] train loss: 0.394\n",
      "[13] val loss: 0.450\n",
      "[14, 4] train loss: 0.360\n",
      "[14, 8] train loss: 0.344\n",
      "[14, 12] train loss: 0.318\n",
      "[14, 16] train loss: 0.281\n",
      "[14, 20] train loss: 0.325\n",
      "[14, 24] train loss: 0.314\n",
      "[14, 28] train loss: 0.387\n",
      "[14, 32] train loss: 0.340\n",
      "[14, 36] train loss: 0.335\n",
      "[14, 40] train loss: 0.388\n",
      "[14, 44] train loss: 0.358\n",
      "[14, 48] train loss: 0.344\n",
      "[14, 52] train loss: 0.320\n",
      "[14, 56] train loss: 0.294\n",
      "[14, 60] train loss: 0.381\n",
      "[14, 64] train loss: 0.365\n",
      "[14, 68] train loss: 0.382\n",
      "[14, 72] train loss: 0.358\n",
      "[14, 76] train loss: 0.318\n",
      "[14, 80] train loss: 0.303\n",
      "[14, 84] train loss: 0.318\n",
      "[14, 88] train loss: 0.301\n",
      "[14, 92] train loss: 0.367\n",
      "[14, 96] train loss: 0.301\n",
      "[14, 100] train loss: 0.325\n",
      "[14, 104] train loss: 0.326\n",
      "[14, 108] train loss: 0.328\n",
      "[14, 112] train loss: 0.308\n",
      "[14, 116] train loss: 0.313\n",
      "[14, 120] train loss: 0.321\n",
      "[14, 124] train loss: 0.378\n",
      "[14, 128] train loss: 0.386\n",
      "[14] val loss: 0.452\n",
      "[15, 4] train loss: 0.357\n",
      "[15, 8] train loss: 0.340\n",
      "[15, 12] train loss: 0.316\n",
      "[15, 16] train loss: 0.283\n",
      "[15, 20] train loss: 0.331\n",
      "[15, 24] train loss: 0.318\n",
      "[15, 28] train loss: 0.411\n",
      "[15, 32] train loss: 0.349\n",
      "[15, 36] train loss: 0.353\n",
      "[15, 40] train loss: 0.398\n",
      "[15, 44] train loss: 0.361\n",
      "[15, 48] train loss: 0.348\n",
      "[15, 52] train loss: 0.327\n",
      "[15, 56] train loss: 0.295\n",
      "[15, 60] train loss: 0.367\n",
      "[15, 64] train loss: 0.351\n",
      "[15, 68] train loss: 0.393\n",
      "[15, 72] train loss: 0.362\n",
      "[15, 76] train loss: 0.314\n",
      "[15, 80] train loss: 0.296\n",
      "[15, 84] train loss: 0.324\n",
      "[15, 88] train loss: 0.301\n",
      "[15, 92] train loss: 0.342\n",
      "[15, 96] train loss: 0.293\n",
      "[15, 100] train loss: 0.301\n",
      "[15, 104] train loss: 0.314\n",
      "[15, 108] train loss: 0.312\n",
      "[15, 112] train loss: 0.305\n",
      "[15, 116] train loss: 0.305\n",
      "[15, 120] train loss: 0.323\n",
      "[15, 124] train loss: 0.369\n",
      "[15, 128] train loss: 0.391\n",
      "[15] val loss: 0.454\n",
      "[16, 4] train loss: 0.351\n",
      "[16, 8] train loss: 0.347\n",
      "[16, 12] train loss: 0.316\n",
      "[16, 16] train loss: 0.292\n",
      "[16, 20] train loss: 0.327\n",
      "[16, 24] train loss: 0.317\n",
      "[16, 28] train loss: 0.392\n",
      "[16, 32] train loss: 0.343\n",
      "[16, 36] train loss: 0.337\n",
      "[16, 40] train loss: 0.385\n",
      "[16, 44] train loss: 0.365\n",
      "[16, 48] train loss: 0.356\n",
      "[16, 52] train loss: 0.318\n",
      "[16, 56] train loss: 0.285\n",
      "[16, 60] train loss: 0.378\n",
      "[16, 64] train loss: 0.351\n",
      "[16, 68] train loss: 0.382\n",
      "[16, 72] train loss: 0.353\n",
      "[16, 76] train loss: 0.311\n",
      "[16, 80] train loss: 0.294\n",
      "[16, 84] train loss: 0.320\n",
      "[16, 88] train loss: 0.295\n",
      "[16, 92] train loss: 0.343\n",
      "[16, 96] train loss: 0.309\n",
      "[16, 100] train loss: 0.318\n",
      "[16, 104] train loss: 0.318\n",
      "[16, 108] train loss: 0.325\n",
      "[16, 112] train loss: 0.304\n",
      "[16, 116] train loss: 0.308\n",
      "[16, 120] train loss: 0.302\n",
      "[16, 124] train loss: 0.373\n",
      "[16, 128] train loss: 0.418\n",
      "[16] val loss: 0.505\n",
      "[17, 4] train loss: 0.376\n",
      "[17, 8] train loss: 0.363\n",
      "[17, 12] train loss: 0.350\n",
      "[17, 16] train loss: 0.291\n",
      "[17, 20] train loss: 0.325\n",
      "[17, 24] train loss: 0.318\n",
      "[17, 28] train loss: 0.417\n",
      "[17, 32] train loss: 0.346\n",
      "[17, 36] train loss: 0.340\n",
      "[17, 40] train loss: 0.396\n",
      "[17, 44] train loss: 0.365\n",
      "[17, 48] train loss: 0.343\n",
      "[17, 52] train loss: 0.327\n",
      "[17, 56] train loss: 0.292\n",
      "[17, 60] train loss: 0.367\n",
      "[17, 64] train loss: 0.343\n",
      "[17, 68] train loss: 0.390\n",
      "[17, 72] train loss: 0.352\n",
      "[17, 76] train loss: 0.308\n",
      "[17, 80] train loss: 0.294\n",
      "[17, 84] train loss: 0.331\n",
      "[17, 88] train loss: 0.293\n",
      "[17, 92] train loss: 0.342\n",
      "[17, 96] train loss: 0.304\n",
      "[17, 100] train loss: 0.285\n",
      "[17, 104] train loss: 0.286\n",
      "[17, 108] train loss: 0.319\n",
      "[17, 112] train loss: 0.299\n",
      "[17, 116] train loss: 0.295\n",
      "[17, 120] train loss: 0.310\n",
      "[17, 124] train loss: 0.352\n",
      "[17, 128] train loss: 0.406\n",
      "[17] val loss: 0.520\n",
      "[18, 4] train loss: 0.358\n",
      "[18, 8] train loss: 0.352\n",
      "[18, 12] train loss: 0.337\n",
      "[18, 16] train loss: 0.274\n",
      "[18, 20] train loss: 0.383\n",
      "[18, 24] train loss: 0.332\n",
      "[18, 28] train loss: 0.381\n",
      "[18, 32] train loss: 0.363\n",
      "[18, 36] train loss: 0.336\n",
      "[18, 40] train loss: 0.393\n",
      "[18, 44] train loss: 0.362\n",
      "[18, 48] train loss: 0.335\n",
      "[18, 52] train loss: 0.308\n",
      "[18, 56] train loss: 0.277\n",
      "[18, 60] train loss: 0.350\n",
      "[18, 64] train loss: 0.345\n",
      "[18, 68] train loss: 0.370\n",
      "[18, 72] train loss: 0.355\n",
      "[18, 76] train loss: 0.311\n",
      "[18, 80] train loss: 0.293\n",
      "[18, 84] train loss: 0.327\n",
      "[18, 88] train loss: 0.306\n",
      "[18, 92] train loss: 0.332\n",
      "[18, 96] train loss: 0.292\n",
      "[18, 100] train loss: 0.294\n",
      "[18, 104] train loss: 0.287\n",
      "[18, 108] train loss: 0.333\n",
      "[18, 112] train loss: 0.297\n",
      "[18, 116] train loss: 0.301\n",
      "[18, 120] train loss: 0.294\n",
      "[18, 124] train loss: 0.346\n",
      "[18, 128] train loss: 0.400\n",
      "[18] val loss: 0.565\n",
      "[19, 4] train loss: 0.346\n",
      "[19, 8] train loss: 0.353\n",
      "[19, 12] train loss: 0.338\n",
      "[19, 16] train loss: 0.293\n",
      "[19, 20] train loss: 0.319\n",
      "[19, 24] train loss: 0.317\n",
      "[19, 28] train loss: 0.416\n",
      "[19, 32] train loss: 0.343\n",
      "[19, 36] train loss: 0.327\n",
      "[19, 40] train loss: 0.398\n",
      "[19, 44] train loss: 0.356\n",
      "[19, 48] train loss: 0.335\n",
      "[19, 52] train loss: 0.324\n",
      "[19, 56] train loss: 0.287\n",
      "[19, 60] train loss: 0.377\n",
      "[19, 64] train loss: 0.348\n",
      "[19, 68] train loss: 0.377\n",
      "[19, 72] train loss: 0.352\n",
      "[19, 76] train loss: 0.316\n",
      "[19, 80] train loss: 0.296\n",
      "[19, 84] train loss: 0.311\n",
      "[19, 88] train loss: 0.284\n",
      "[19, 92] train loss: 0.333\n",
      "[19, 96] train loss: 0.286\n",
      "[19, 100] train loss: 0.276\n",
      "[19, 104] train loss: 0.279\n",
      "[19, 108] train loss: 0.307\n",
      "[19, 112] train loss: 0.302\n",
      "[19, 116] train loss: 0.299\n",
      "[19, 120] train loss: 0.291\n",
      "[19, 124] train loss: 0.340\n",
      "[19, 128] train loss: 0.383\n",
      "[19] val loss: 0.575\n",
      "[20, 4] train loss: 0.336\n",
      "[20, 8] train loss: 0.347\n",
      "[20, 12] train loss: 0.328\n",
      "[20, 16] train loss: 0.281\n",
      "[20, 20] train loss: 0.316\n",
      "[20, 24] train loss: 0.299\n",
      "[20, 28] train loss: 0.383\n",
      "[20, 32] train loss: 0.343\n",
      "[20, 36] train loss: 0.319\n",
      "[20, 40] train loss: 0.394\n",
      "[20, 44] train loss: 0.353\n",
      "[20, 48] train loss: 0.332\n",
      "[20, 52] train loss: 0.306\n",
      "[20, 56] train loss: 0.282\n",
      "[20, 60] train loss: 0.384\n",
      "[20, 64] train loss: 0.340\n",
      "[20, 68] train loss: 0.380\n",
      "[20, 72] train loss: 0.351\n",
      "[20, 76] train loss: 0.322\n",
      "[20, 80] train loss: 0.284\n",
      "[20, 84] train loss: 0.315\n",
      "[20, 88] train loss: 0.286\n",
      "[20, 92] train loss: 0.324\n",
      "[20, 96] train loss: 0.280\n",
      "[20, 100] train loss: 0.257\n",
      "[20, 104] train loss: 0.266\n",
      "[20, 108] train loss: 0.305\n",
      "[20, 112] train loss: 0.306\n",
      "[20, 116] train loss: 0.309\n",
      "[20, 120] train loss: 0.290\n",
      "[20, 124] train loss: 0.362\n",
      "[20, 128] train loss: 0.368\n",
      "[20] val loss: 0.548\n",
      "Finished Training on 20 Epochs!\n",
      "Factory                  : \u001b[1mTrain all methods\u001b[0m\n",
      "                         : Rebuilding Dataset dataset\n",
      "                         : Building event vectors for type 2 Signal\n",
      "                         : Dataset[dataset] :  create input formulas for tree sgn\n",
      "                         : Using variable vars_time0[0] from array expression vars_time0 of size 30\n",
      "                         : Using variable vars_time1[0] from array expression vars_time1 of size 30\n",
      "                         : Using variable vars_time2[0] from array expression vars_time2 of size 30\n",
      "                         : Using variable vars_time3[0] from array expression vars_time3 of size 30\n",
      "                         : Using variable vars_time4[0] from array expression vars_time4 of size 30\n",
      "                         : Using variable vars_time5[0] from array expression vars_time5 of size 30\n",
      "                         : Using variable vars_time6[0] from array expression vars_time6 of size 30\n",
      "                         : Using variable vars_time7[0] from array expression vars_time7 of size 30\n",
      "                         : Using variable vars_time8[0] from array expression vars_time8 of size 30\n",
      "                         : Using variable vars_time9[0] from array expression vars_time9 of size 30\n",
      "                         : Building event vectors for type 2 Background\n",
      "                         : Dataset[dataset] :  create input formulas for tree bkg\n",
      "                         : Using variable vars_time0[0] from array expression vars_time0 of size 30\n",
      "                         : Using variable vars_time1[0] from array expression vars_time1 of size 30\n",
      "                         : Using variable vars_time2[0] from array expression vars_time2 of size 30\n",
      "                         : Using variable vars_time3[0] from array expression vars_time3 of size 30\n",
      "                         : Using variable vars_time4[0] from array expression vars_time4 of size 30\n",
      "                         : Using variable vars_time5[0] from array expression vars_time5 of size 30\n",
      "                         : Using variable vars_time6[0] from array expression vars_time6 of size 30\n",
      "                         : Using variable vars_time7[0] from array expression vars_time7 of size 30\n",
      "                         : Using variable vars_time8[0] from array expression vars_time8 of size 30\n",
      "                         : Using variable vars_time9[0] from array expression vars_time9 of size 30\n",
      "DataSetFactory           : [dataset] : Number of events in input trees\n",
      "                         : \n",
      "                         : \n",
      "                         : Number of training and testing events\n",
      "                         : ---------------------------------------------------------------------------\n",
      "                         : Signal     -- training events            : 8000\n",
      "                         : Signal     -- testing events             : 2000\n",
      "                         : Signal     -- training and testing events: 10000\n",
      "                         : Background -- training events            : 8000\n",
      "                         : Background -- testing events             : 2000\n",
      "                         : Background -- training and testing events: 10000\n",
      "                         : \n",
      "Factory                  : Train method: TMVA_RNN for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU using MT,  nthreads = 4\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 4  Input = ( 10, 1, 30 )  Batch size = 100  Loss function = C\n",
      "\tLayer 0\t RECURRENT Layer: \t  (NInput = 30, NState = 10, NTime  = 10 )\tOutput = ( 100 , 10 , 10 )\n",
      "\tLayer 1\t RESHAPE Layer \t Input = ( 1 , 10 , 10 ) \tOutput = ( 1 , 100 , 100 ) \n",
      "\tLayer 2\t DENSE Layer: \t ( Input =   100 , Width =    64 ) \tOutput = (  1 ,   100 ,    64 ) \t Activation Function = Tanh\n",
      "\tLayer 3\t DENSE Layer: \t ( Input =    64 , Width =     1 ) \tOutput = (  1 ,   100 ,     1 ) \t Activation Function = Identity\n",
      "                         : Using 12800 events for training and 3200 for testing\n",
      "                         : Compute initial loss  on the validation data \n",
      "                         : Training phase 1 of 1:  Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.929443\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Val. Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :    Start epoch iteration ...\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |     0.681476    0.658081    0.499452   0.0555195     28833.2           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.630023    0.617435    0.484303   0.0530413     29680.4           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.592102    0.574997    0.494594   0.0530789     28991.1           0\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |     0.550848    0.549153    0.500264   0.0536503     28660.1           0\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |     0.520528    0.518078    0.494185   0.0540083     29079.2           0\n",
      "                         :          6 Minimum Test error found - save the configuration \n",
      "                         :          6 |      0.49195     0.49751    0.501272   0.0544197     28644.8           0\n",
      "                         :          7 Minimum Test error found - save the configuration \n",
      "                         :          7 |     0.470456    0.491288    0.499243   0.0543494     28770.9           0\n",
      "                         :          8 Minimum Test error found - save the configuration \n",
      "                         :          8 |     0.457386    0.469806    0.497302   0.0543744     28898.6           0\n",
      "                         :          9 Minimum Test error found - save the configuration \n",
      "                         :          9 |     0.445034     0.45544    0.501181   0.0543325     28645.1           0\n",
      "                         :         10 |     0.435398    0.456206    0.497401   0.0539893     28867.1           1\n",
      "                         :         11 Minimum Test error found - save the configuration \n",
      "                         :         11 |     0.429634    0.450135    0.495869    0.054241     28983.7           0\n",
      "                         :         12 Minimum Test error found - save the configuration \n",
      "                         :         12 |     0.426046    0.435608    0.496435   0.0543837     28955.9           0\n",
      "                         :         13 |     0.421704    0.448529    0.501958   0.0567616     28751.4           1\n",
      "                         :         14 |     0.416236    0.438522    0.499819   0.0542698     28728.6           2\n",
      "                         :         15 Minimum Test error found - save the configuration \n",
      "                         :         15 |     0.414906    0.431005    0.498827   0.0543884     28800.4           0\n",
      "                         :         16 Minimum Test error found - save the configuration \n",
      "                         :         16 |     0.408304     0.43009    0.499753   0.0539603     28712.9           0\n",
      "                         :         17 Minimum Test error found - save the configuration \n",
      "                         :         17 |     0.410414    0.428649    0.495249   0.0542802       29027           0\n",
      "                         :         18 |     0.403984    0.429797    0.499319   0.0549651     28805.9           1\n",
      "                         :         19 |      0.40319    0.438378    0.502879   0.0546778     28558.6           2\n",
      "                         :         20 Minimum Test error found - save the configuration \n",
      "                         :         20 |     0.400041    0.424189    0.509387   0.0553231     28189.9           0\n",
      "                         : \n",
      "                         : Elapsed time for training with 16000 events: 10 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 100\n",
      "                         : \n",
      "TMVA_RNN                 : [dataset] : Evaluation of TMVA_RNN on training sample (16000 events)\n",
      "                         : Elapsed time for evaluation of 16000 events: 0.288 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_RNN.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_RNN.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: TMVA_LSTM for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU using MT,  nthreads = 4\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 4  Input = ( 10, 1, 30 )  Batch size = 100  Loss function = C\n",
      "\tLayer 0\t LSTM Layer: \t  (NInput = 30, NState = 10, NTime  = 10 )\tOutput = ( 100 , 10 , 10 )\n",
      "\tLayer 1\t RESHAPE Layer \t Input = ( 1 , 10 , 10 ) \tOutput = ( 1 , 100 , 100 ) \n",
      "\tLayer 2\t DENSE Layer: \t ( Input =   100 , Width =    64 ) \tOutput = (  1 ,   100 ,    64 ) \t Activation Function = Tanh\n",
      "\tLayer 3\t DENSE Layer: \t ( Input =    64 , Width =     1 ) \tOutput = (  1 ,   100 ,     1 ) \t Activation Function = Identity\n",
      "                         : Using 12800 events for training and 3200 for testing\n",
      "                         : Compute initial loss  on the validation data \n",
      "                         : Training phase 1 of 1:  Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.709671\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Val. Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :    Start epoch iteration ...\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |     0.686673    0.668444     2.16077    0.154892     6381.24           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.630225    0.593114       2.477    0.162683     5530.78           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.560831    0.539399     2.29474    0.147983     5962.48           0\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |     0.505407     0.49572     1.99071     0.14994     6953.61           0\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |     0.466613    0.468333      2.0872    0.152121     6614.71           0\n",
      "                         :          6 Minimum Test error found - save the configuration \n",
      "                         :          6 |     0.440583    0.449364     2.12177    0.151868     6497.78           0\n",
      "                         :          7 |     0.430692    0.465379     2.05116    0.150455     6734.35           1\n",
      "                         :          8 Minimum Test error found - save the configuration \n",
      "                         :          8 |     0.420473    0.435105     2.07782    0.148755     6635.33           0\n",
      "                         :          9 Minimum Test error found - save the configuration \n",
      "                         :          9 |     0.410565    0.425871     2.07912      0.1578      6662.1           0\n",
      "                         :         10 Minimum Test error found - save the configuration \n",
      "                         :         10 |     0.406882    0.419272     2.01799    0.146924     6841.01           0\n",
      "                         :         11 |     0.403718    0.422049     1.99864    0.154038     6939.15           1\n",
      "                         :         12 Minimum Test error found - save the configuration \n",
      "                         :         12 |     0.403093     0.41696     2.07215    0.146791      6648.1           0\n",
      "                         :         13 Minimum Test error found - save the configuration \n",
      "                         :         13 |     0.398086    0.412449      2.1009    0.149322     6558.79           0\n",
      "                         :         14 Minimum Test error found - save the configuration \n",
      "                         :         14 |     0.394568    0.407053     2.10985    0.149488     6529.39           0\n",
      "                         :         15 |     0.393418    0.407509      2.1362    0.152427     6452.34           1\n",
      "                         :         16 |     0.389071    0.418958      2.0924    0.148708     6585.42           2\n",
      "                         :         17 Minimum Test error found - save the configuration \n",
      "                         :         17 |     0.392416     0.40564     2.02879    0.145729     6797.45           0\n",
      "                         :         18 Minimum Test error found - save the configuration \n",
      "                         :         18 |     0.390365    0.403835     2.01304    0.143012     6844.84           0\n",
      "                         :         19 |      0.39019    0.408447      2.0699    0.143698     6645.21           1\n",
      "                         :         20 Minimum Test error found - save the configuration \n",
      "                         :         20 |     0.387259    0.401651     2.01461    0.142242     6836.26           0\n",
      "                         : \n",
      "                         : Elapsed time for training with 16000 events: 42.1 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 100\n",
      "                         : \n",
      "TMVA_LSTM                : [dataset] : Evaluation of TMVA_LSTM on training sample (16000 events)\n",
      "                         : Elapsed time for evaluation of 16000 events: 0.69 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: TMVA_GRU for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU using MT,  nthreads = 4\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 4  Input = ( 10, 1, 30 )  Batch size = 100  Loss function = C\n",
      "\tLayer 0\t GRU Layer: \t  (NInput = 30, NState = 10, NTime  = 10 )\tOutput = ( 100 , 10 , 10 )\n",
      "\tLayer 1\t RESHAPE Layer \t Input = ( 1 , 10 , 10 ) \tOutput = ( 1 , 100 , 100 ) \n",
      "\tLayer 2\t DENSE Layer: \t ( Input =   100 , Width =    64 ) \tOutput = (  1 ,   100 ,    64 ) \t Activation Function = Tanh\n",
      "\tLayer 3\t DENSE Layer: \t ( Input =    64 , Width =     1 ) \tOutput = (  1 ,   100 ,     1 ) \t Activation Function = Identity\n",
      "                         : Using 12800 events for training and 3200 for testing\n",
      "                         : Compute initial loss  on the validation data \n",
      "                         : Training phase 1 of 1:  Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.709404\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Val. Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :    Start epoch iteration ...\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |     0.683836    0.675681     1.05885   0.0847124     13139.9           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.661734    0.660563    0.953787   0.0850604     14734.2           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.633841    0.619115    0.968541   0.0817042     14433.3           0\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |       0.6107    0.589355    0.900377   0.0811236       15624           0\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |     0.590247    0.577662    0.898764   0.0798777       15631           0\n",
      "                         :          6 Minimum Test error found - save the configuration \n",
      "                         :          6 |     0.563742    0.550545    0.906355   0.0814193     15516.4           0\n",
      "                         :          7 Minimum Test error found - save the configuration \n",
      "                         :          7 |     0.545018    0.544448    0.922739   0.0838364       15258           0\n",
      "                         :          8 Minimum Test error found - save the configuration \n",
      "                         :          8 |     0.529968    0.521708    0.975524   0.0842501     14361.5           0\n",
      "                         :          9 Minimum Test error found - save the configuration \n",
      "                         :          9 |     0.515032    0.514816     0.99514   0.0811509     14004.5           0\n",
      "                         :         10 Minimum Test error found - save the configuration \n",
      "                         :         10 |      0.49921    0.500196     1.00443   0.0801328     13848.4           0\n",
      "                         :         11 Minimum Test error found - save the configuration \n",
      "                         :         11 |     0.491648    0.492723     1.00581    0.081809     13852.7           0\n",
      "                         :         12 Minimum Test error found - save the configuration \n",
      "                         :         12 |     0.480009    0.486195     1.05187   0.0806843     13179.7           0\n",
      "                         :         13 Minimum Test error found - save the configuration \n",
      "                         :         13 |     0.472961    0.483426     1.01889   0.0800066     13633.2           0\n",
      "                         :         14 Minimum Test error found - save the configuration \n",
      "                         :         14 |     0.470983    0.473198     1.01503   0.0801644     13691.8           0\n",
      "                         :         15 |      0.46673    0.478858     1.01213   0.0804578     13738.8           1\n",
      "                         :         16 Minimum Test error found - save the configuration \n",
      "                         :         16 |     0.458625    0.469393     1.02593   0.0815268     13553.5           0\n",
      "                         :         17 Minimum Test error found - save the configuration \n",
      "                         :         17 |     0.450367    0.458926     1.04984   0.0805532     13205.6           0\n",
      "                         :         18 |     0.447508    0.462523     1.04337   0.0800056     13286.8           1\n",
      "                         :         19 Minimum Test error found - save the configuration \n",
      "                         :         19 |     0.445449    0.453093     1.03852   0.0827838     13392.8           0\n",
      "                         :         20 Minimum Test error found - save the configuration \n",
      "                         :         20 |      0.43637    0.448977     1.03317   0.0801358     13430.9           0\n",
      "                         : \n",
      "                         : Elapsed time for training with 16000 events: 20 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 100\n",
      "                         : \n",
      "TMVA_GRU                 : [dataset] : Evaluation of TMVA_GRU on training sample (16000 events)\n",
      "                         : Elapsed time for evaluation of 16000 events: 0.399 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_GRU.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_GRU.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: TMVA_DNN for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU using MT,  nthreads = 4\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 4  Input = ( 1, 1, 300 )  Batch size = 256  Loss function = C\n",
      "\tLayer 0\t DENSE Layer: \t ( Input =   300 , Width =    64 ) \tOutput = (  1 ,   256 ,    64 ) \t Activation Function = Tanh\n",
      "\tLayer 1\t DENSE Layer: \t ( Input =    64 , Width =    64 ) \tOutput = (  1 ,   256 ,    64 ) \t Activation Function = Tanh\n",
      "\tLayer 2\t DENSE Layer: \t ( Input =    64 , Width =    64 ) \tOutput = (  1 ,   256 ,    64 ) \t Activation Function = Tanh\n",
      "\tLayer 3\t DENSE Layer: \t ( Input =    64 , Width =     1 ) \tOutput = (  1 ,   256 ,     1 ) \t Activation Function = Identity\n",
      "                         : Using 12800 events for training and 3200 for testing\n",
      "                         : Compute initial loss  on the validation data \n",
      "                         : Training phase 1 of 1:  Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.28272\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Val. Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :    Start epoch iteration ...\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |     0.713352    0.671483     0.25511   0.0308988     57089.1           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.663142    0.660707    0.261008   0.0309038       55627           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.646135    0.655052    0.258814   0.0309441     56172.5           0\n",
      "                         :          4 |     0.643933    0.664197    0.255887   0.0305117     56794.2           1\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |      0.65587    0.628699    0.257398   0.0309361     56521.6           0\n",
      "                         :          6 |     0.625901    0.632739    0.258973   0.0308331     56105.9           1\n",
      "                         :          7 Minimum Test error found - save the configuration \n",
      "                         :          7 |     0.623231    0.611692    0.258977   0.0304101       56001           0\n",
      "                         :          8 |     0.613314    0.650434    0.255761   0.0322192     57259.9           1\n",
      "                         :          9 |     0.641769    0.621051    0.275888   0.0334787     52803.3           2\n",
      "                         :         10 Minimum Test error found - save the configuration \n",
      "                         :         10 |     0.596461    0.587063    0.285467    0.037613     51643.4           0\n",
      "                         :         11 Minimum Test error found - save the configuration \n",
      "                         :         11 |     0.574691    0.586463    0.257311   0.0303021     56385.6           0\n",
      "                         :         12 Minimum Test error found - save the configuration \n",
      "                         :         12 |     0.573551    0.576863     0.22966   0.0254685     62686.4           0\n",
      "                         :         13 Minimum Test error found - save the configuration \n",
      "                         :         13 |     0.556467    0.575816     0.20255   0.0254551     72277.7           0\n",
      "                         :         14 Minimum Test error found - save the configuration \n",
      "                         :         14 |     0.540521     0.55158    0.217455   0.0253206     66620.1           0\n",
      "                         :         15 Minimum Test error found - save the configuration \n",
      "                         :         15 |     0.539261    0.550775    0.243251    0.025173     58694.6           0\n",
      "                         :         16 Minimum Test error found - save the configuration \n",
      "                         :         16 |     0.554442    0.545737    0.261902   0.0256214     54172.8           0\n",
      "                         :         17 Minimum Test error found - save the configuration \n",
      "                         :         17 |     0.526003    0.521332    0.266585     0.02579     53157.2           0\n",
      "                         :         18 |     0.518063    0.532543    0.257352    0.024848     55052.8           1\n",
      "                         :         19 |     0.523388    0.525893    0.255385   0.0246766     55481.4           2\n",
      "                         :         20 Minimum Test error found - save the configuration \n",
      "                         :         20 |     0.488509    0.493649    0.259355   0.0252529       54677           0\n",
      "                         : \n",
      "                         : Elapsed time for training with 16000 events: 5.1 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 256\n",
      "                         : \n",
      "TMVA_DNN                 : [dataset] : Evaluation of TMVA_DNN on training sample (16000 events)\n",
      "                         : Elapsed time for evaluation of 16000 events: 0.128 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: PyTorch for Classification\n",
      "                         : \n",
      "                         : \n",
      "                         : \u001b[1m================================================================\u001b[0m\n",
      "                         : \u001b[1mH e l p   f o r   M V A   m e t h o d   [ PyTorch ] :\u001b[0m\n",
      "                         : \n",
      "                         : PyTorch is a scientific computing package supporting\n",
      "                         : automatic differentiation. This method wraps the training\n",
      "                         : and predictions steps of the PyTorch Python package for\n",
      "                         : TMVA, so that dataloading, preprocessing and evaluation\n",
      "                         : can be done within the TMVA system. To use this PyTorch\n",
      "                         : interface, you need to generatea model with PyTorch first.\n",
      "                         : Then, this model can be loaded and trained in TMVA.\n",
      "                         : \n",
      "                         : \n",
      "                         : <Suppress this message by specifying \"!H\" in the booking option>\n",
      "                         : \u001b[1m================================================================\u001b[0m\n",
      "                         : \n",
      "                         : Split TMVA training data in 12800 training events and 3200 validation events\n",
      "                         : Print Training Model Architecture\n",
      "                         : Option SaveBestOnly: Only model weights with smallest validation loss will be stored\n",
      "                         : Elapsed time for training with 16000 events: 84.1 sec         \n",
      "PyTorch                  : [dataset] : Evaluation of PyTorch on training sample (16000 events)\n",
      "                         : Elapsed time for evaluation of 16000 events: 1.6 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVAClassification_PyTorch.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVAClassification_PyTorch.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "                         : Ranking input variables (method specific)...\n",
      "                         : No variable ranking supplied by classifier: TMVA_RNN\n",
      "                         : No variable ranking supplied by classifier: TMVA_LSTM\n",
      "                         : No variable ranking supplied by classifier: TMVA_GRU\n",
      "                         : No variable ranking supplied by classifier: TMVA_DNN\n",
      "                         : No variable ranking supplied by classifier: PyTorch\n",
      "TH1.Print Name  = TrainingHistory_TMVA_RNN_trainingError, Entries= 0, Total sum= 9.40966\n",
      "TH1.Print Name  = TrainingHistory_TMVA_RNN_valError, Entries= 0, Total sum= 9.6429\n",
      "TH1.Print Name  = TrainingHistory_TMVA_LSTM_trainingError, Entries= 0, Total sum= 8.90113\n",
      "TH1.Print Name  = TrainingHistory_TMVA_LSTM_valError, Entries= 0, Total sum= 9.06455\n",
      "TH1.Print Name  = TrainingHistory_TMVA_GRU_trainingError, Entries= 0, Total sum= 10.454\n",
      "TH1.Print Name  = TrainingHistory_TMVA_GRU_valError, Entries= 0, Total sum= 10.4614\n",
      "TH1.Print Name  = TrainingHistory_TMVA_DNN_trainingError, Entries= 0, Total sum= 11.818\n",
      "TH1.Print Name  = TrainingHistory_TMVA_DNN_valError, Entries= 0, Total sum= 11.8438\n",
      "Factory                  : === Destroy and recreate all methods via weight files for testing ===\n",
      "                         : \n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_RNN.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_GRU.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVAClassification_PyTorch.weights.xml\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train all methods\n",
    "factory.TrainAllMethods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cb0fb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nthreads  = 4\n",
      "\n",
      "custom objects for loading model :  {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7ff54b9ff280>, 'predict_func': <function predict at 0x7ff54b9ff310>}\n",
      "Factory                  : \u001b[1mTest all methods\u001b[0m\n",
      "Factory                  : Test method: TMVA_RNN for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TMVA_RNN                 : [dataset] : Evaluation of TMVA_RNN on testing sample (4000 events)\n",
      "                         : Elapsed time for evaluation of 4000 events: 0.0704 sec       \n",
      "Factory                  : Test method: TMVA_LSTM for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TMVA_LSTM                : [dataset] : Evaluation of TMVA_LSTM on testing sample (4000 events)\n",
      "                         : Elapsed time for evaluation of 4000 events: 0.165 sec       \n",
      "Factory                  : Test method: TMVA_GRU for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TMVA_GRU                 : [dataset] : Evaluation of TMVA_GRU on testing sample (4000 events)\n",
      "                         : Elapsed time for evaluation of 4000 events: 0.094 sec       \n",
      "Factory                  : Test method: TMVA_DNN for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TMVA_DNN                 : [dataset] : Evaluation of TMVA_DNN on testing sample (4000 events)\n",
      "                         : Elapsed time for evaluation of 4000 events: 0.0238 sec       \n",
      "Factory                  : Test method: PyTorch for Classification performance\n",
      "                         : \n",
      "                         :  Setup PyTorch Model \n",
      "                         : Loaded pytorch train function: \n",
      "                         : Loaded pytorch optimizer: \n",
      "                         : Loaded pytorch loss function: \n",
      "                         : Loaded pytorch predict function: \n",
      "                         : Load model from file: PyTorchTrainedModelRNN.pt\n",
      "PyTorch                  : [dataset] : Evaluation of PyTorch on testing sample (4000 events)\n",
      "                         : Elapsed time for evaluation of 4000 events: 0.397 sec       \n"
     ]
    }
   ],
   "source": [
    "print(\"nthreads  = \"+ str(ROOT.GetThreadPoolSize()) + \"\\n\")\n",
    "\n",
    "# Evaluate all MVAs using the set of test events\n",
    "factory.TestAllMethods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6822177d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mEvaluate all methods\u001b[0m\n",
      "Factory                  : Evaluate classifier: TMVA_RNN\n",
      "                         : \n",
      "TMVA_RNN                 : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 300 , it is larger than 200\n",
      "Factory                  : Evaluate classifier: TMVA_LSTM\n",
      "                         : \n",
      "TMVA_LSTM                : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 300 , it is larger than 200\n",
      "Factory                  : Evaluate classifier: TMVA_GRU\n",
      "                         : \n",
      "TMVA_GRU                 : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 300 , it is larger than 200\n",
      "Factory                  : Evaluate classifier: TMVA_DNN\n",
      "                         : \n",
      "TMVA_DNN                 : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 300 , it is larger than 200\n",
      "Factory                  : Evaluate classifier: PyTorch\n",
      "                         : \n",
      "PyTorch                  : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 300 , it is larger than 200\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       PyTorch        : 0.921\n",
      "                         : dataset       TMVA_LSTM      : 0.910\n",
      "                         : dataset       TMVA_RNN       : 0.898\n",
      "                         : dataset       TMVA_GRU       : 0.878\n",
      "                         : dataset       TMVA_DNN       : 0.861\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              PyTorch        : 0.396 (0.402)       0.763 (0.795)      0.928 (0.932)\n",
      "                         : dataset              TMVA_LSTM      : 0.295 (0.322)       0.742 (0.732)      0.921 (0.919)\n",
      "                         : dataset              TMVA_RNN       : 0.325 (0.284)       0.686 (0.711)      0.908 (0.902)\n",
      "                         : dataset              TMVA_GRU       : 0.208 (0.223)       0.655 (0.667)      0.870 (0.876)\n",
      "                         : dataset              TMVA_DNN       : 0.000 (0.000)       0.621 (0.632)      0.835 (0.856)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "Dataset:dataset          : Created tree 'TestTree' with 4000 events\n",
      "                         : \n",
      "Dataset:dataset          : Created tree 'TrainTree' with 16000 events\n",
      "                         : \n",
      "Factory                  : \u001b[1mThank you for using TMVA!\u001b[0m\n",
      "                         : \u001b[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and compare performance of all configured MVAs\n",
    "factory.EvaluateAllMethods()\n",
    "#  check method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1df339ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  plot ROC curve\n",
    "c1 = factory.GetROCCurve(dataloader)\n",
    "c1.Draw() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56c2402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (outputFile):\n",
    "    outputFile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e83771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
