{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b750ba9",
   "metadata": {},
   "source": [
    "#   TMVA Classification Example Using a Recurrent Neural Network\n",
    " \n",
    "This is an example of using a RNN in TMVA. We do classification using a toy time dependent data set\n",
    "that is generated when running this example macro.\n",
    "\n",
    "This is an example of using a RNN in TMVA. We do the classification using a toy data set containing a time series of data sample ntimes and with dimension ndim that is generated when running the provided function `MakeTimeData (nevents, ntime, ndim)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4093cd4",
   "metadata": {},
   "source": [
    "### Import the necessary modules\n",
    "\n",
    "We start with importing the necessary modules required for the tutorial. Here we imported ROOT and TMVA(Toolkit for Multivariate Data Analysis). If you want to know more about TMVA, you can refer the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af11d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in <TCling::LoadPCM>: ROOT PCM /home/neel/Root/install/lib/libNet_rdict.pcm file does not exist\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libASImageGui_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libASImage_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libEG_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libEve_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libFFTW_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libFITSIO_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libFitPanel_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libFoam_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libFumili_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGX11TTF_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGX11_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGdml_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGed_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGenVector_G__GenVector32_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGenVector_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGenetic_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGeomBuilder_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGeomPainter_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGeom_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGpad_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGraf3d_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGraf_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGuiBld_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGuiHtml_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGui_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libGviz3d_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libHistFactory_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libHistPainter_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libHist_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libHtml_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libMLP_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libMathCore_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libMathMore_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libMatrix_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libMinuit2_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libMinuit_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libNetxNG_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libPgSQL_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libPhysics_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libPostscript_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libProofBench_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libProofDraw_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libProofPlayer_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libProof_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libPyMVA_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libQuadp_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRCsg_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRGL_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRHTTPSniff_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRHTTP_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRMySQL_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libROOTDataFrame_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libROOTTMVASofieParser_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libROOTTMVASofie_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libROOTTPython_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libROOTVecOps_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRSQLite_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRecorder_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRooFitCore_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRooFitHS3_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRooFitMore_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRooFitRDataFrameHelpers_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRooFit_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRooStats_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libRootAuth_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libSPlot_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libSQLIO_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libSessionViewer_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libSmatrix_G__Smatrix32_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libSmatrix_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libSpectrumPainter_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libSpectrum_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libTMVAGui_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libTMVA_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libTreePlayer_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libTreeViewer_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libTree_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libUnfold_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libX3d_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libXMLIO_rdict.pcm\n",
      "\n",
      "Info in <TCling::LoadPCM>: In-memory ROOT PCM candidate /home/neel/Root/install/lib/libXMLParser_rdict.pcm\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.27/01\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "from ROOT import TMVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8276e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ninput = 30\n",
    "# ninput=10\n",
    "ntime = 10\n",
    "# batchSize = 100\n",
    "batchSize = 100\n",
    "maxepochs = 20\n",
    "\n",
    "use_type = 1\n",
    "\n",
    "nTotEvts = 10000 # total events to be generated for signal or background\n",
    "\n",
    "useKeras = True\n",
    "\n",
    "\n",
    "useTMVA_RNN = True\n",
    "useTMVA_DNN = True\n",
    "useTMVA_BDT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc7e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn_types = [\"RNN\", \"LSTM\", \"GRU\"]\n",
    "use_rnn_type = [1, 1, 1]\n",
    "if (use_type >=0 & use_type < 3) :\n",
    "      use_rnn_type = [0,0,0]\n",
    "      use_rnn_type[use_type] = 1;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "936061e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "archString = \"CPU\"\n",
    "writeOutputFile = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b1d1590",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_type = \"RNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae7a37",
   "metadata": {},
   "source": [
    "### Setting up TMVA\n",
    "\n",
    "TMVA requires initialization the PyMVA to utilize PyTorch. PyMVA is the interface for third-party MVA tools based on Python. It is created to make powerful external libraries easily accessible with a direct integration into the TMVA workflow. All PyMVA methods provide the same plug-and-play mechanisms as the TMVA methods. Because the base method of PyMVA is inherited from the TMVA base method, all options of internal TMVA methods apply for PyMVA methods as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06612027",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT.TMVA.Tools.Instance()\n",
    "ROOT.TMVA.PyMethodBase.PyInitialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b40031",
   "metadata": {},
   "source": [
    "### Define the input files and the number of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f588b1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with nthreads  = 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_threads = 0   # use by default all threads\n",
    "#    do enable MT running\n",
    "if (num_threads >= 0):\n",
    "    ROOT.EnableImplicitMT(num_threads)\n",
    "    if (num_threads > 0):\n",
    "        ROOT.gSystem.Setenv(\"OMP_NUM_THREADS\", num_threads)\n",
    "    else:\n",
    "      ROOT.gSystem.Setenv(\"OMP_NUM_THREADS\", \"1\")\n",
    "\n",
    "\n",
    "print(\"Running with nthreads  = \" + str(ROOT.GetThreadPoolSize()) + \"\\n\" )\n",
    "\n",
    "inputFileName = \"time_data_t10_d30.root\"\n",
    "\n",
    "fileExist = ROOT.gSystem.AccessPathName(inputFileName)\n",
    "\n",
    "#if file does not exists create it\n",
    "if (fileExist==None):\n",
    "    MakeTimeData(nTotEvts,ntime, ninput)\n",
    "\n",
    "inputFile = ROOT.TFile.Open(inputFileName)\n",
    "if (inputFile==None):\n",
    "    Error(\"TMVA_RNN_Classification\", \"Error opening input file %s - exit\", inputFileName.Data())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204016f",
   "metadata": {},
   "source": [
    "### Create an Output File and Declare Factory\n",
    "\n",
    "Create the Factory class. Later you can choose the methods whose performance you'd like to investigate.\n",
    "\n",
    "The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass\n",
    "\n",
    "- The first argument is the base of the name of all the output weightfiles in the directory weight/ that will be created with the method parameters\n",
    "\n",
    "- The second argument is the output file for the training results\n",
    "\n",
    "- The third argument is a string option defining some general configuration for the TMVA session. \n",
    "\n",
    "For example all TMVA output can be suppressed by removing the \"!\" (not) in front of the \"Silent\" argument in the option string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d49cde7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RNNClassification  : Using input file: time_data_t10_d30.root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- RNNClassification  : Using input file: \" + inputFile.GetName()+\"\\n\")\n",
    "\n",
    "#   Create a ROOT output file where TMVA will store ntuples, histograms, etc.\n",
    "outfileName = \"data_RNN_\"+ archString +\".root\"\n",
    "\n",
    "if (writeOutputFile):\n",
    "    outputFile = ROOT.TFile.Open(outfileName, \"RECREATE\")\n",
    "\n",
    "#  Creating the factory object\n",
    "factory = ROOT.TMVA.Factory(\"TMVAClassification\", outputFile,\"!V:!Silent:Color:DrawProgressBar:Transformations=None:!Correlations:\"+\"AnalysisType=Classification:ModelPersistence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c07fc9b",
   "metadata": {},
   "source": [
    "### Declare DataLoader(s)\n",
    "\n",
    "The next step is to declare the DataLoader class that deals with input variables\n",
    "\n",
    "Define the input variables that shall be used for the MVA training\n",
    "note that you may also use variable expressions, which can be parsed by TTree::Draw( \"expression\" )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11738b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of variables is 300\n",
      "\n",
      "vars_time0[0]\n",
      "\n",
      "vars_time0[1]\n",
      "\n",
      "vars_time0[2]\n",
      "\n",
      "vars_time0[3]\n",
      "\n",
      "vars_time0[4]\n",
      "\n",
      "vars_time0[5]\n",
      "\n",
      "vars_time0[6]\n",
      "\n",
      "vars_time0[7]\n",
      "\n",
      "vars_time0[8]\n",
      "\n",
      "vars_time0[9]\n",
      "\n",
      "vars_time0[10]\n",
      "\n",
      "vars_time0[11]\n",
      "\n",
      "vars_time0[12]\n",
      "\n",
      "vars_time0[13]\n",
      "\n",
      "vars_time0[14]\n",
      "\n",
      "vars_time0[15]\n",
      "\n",
      "vars_time0[16]\n",
      "\n",
      "vars_time0[17]\n",
      "\n",
      "vars_time0[18]\n",
      "\n",
      "vars_time0[19]\n",
      "\n",
      "vars_time0[20]\n",
      "\n",
      "vars_time0[21]\n",
      "\n",
      "vars_time0[22]\n",
      "\n",
      "vars_time0[23]\n",
      "\n",
      "vars_time0[24]\n",
      "\n",
      "vars_time0[25]\n",
      "\n",
      "vars_time0[26]\n",
      "\n",
      "vars_time0[27]\n",
      "\n",
      "vars_time0[28]\n",
      "\n",
      "vars_time0[29]\n",
      "\n",
      "vars_time1[0]\n",
      "\n",
      "vars_time1[1]\n",
      "\n",
      "vars_time1[2]\n",
      "\n",
      "vars_time1[3]\n",
      "\n",
      "vars_time1[4]\n",
      "\n",
      "vars_time1[5]\n",
      "\n",
      "vars_time1[6]\n",
      "\n",
      "vars_time1[7]\n",
      "\n",
      "vars_time1[8]\n",
      "\n",
      "vars_time1[9]\n",
      "\n",
      "vars_time1[10]\n",
      "\n",
      "vars_time1[11]\n",
      "\n",
      "vars_time1[12]\n",
      "\n",
      "vars_time1[13]\n",
      "\n",
      "vars_time1[14]\n",
      "\n",
      "vars_time1[15]\n",
      "\n",
      "vars_time1[16]\n",
      "\n",
      "vars_time1[17]\n",
      "\n",
      "vars_time1[18]\n",
      "\n",
      "vars_time1[19]\n",
      "\n",
      "vars_time1[20]\n",
      "\n",
      "vars_time1[21]\n",
      "\n",
      "vars_time1[22]\n",
      "\n",
      "vars_time1[23]\n",
      "\n",
      "vars_time1[24]\n",
      "\n",
      "vars_time1[25]\n",
      "\n",
      "vars_time1[26]\n",
      "\n",
      "vars_time1[27]\n",
      "\n",
      "vars_time1[28]\n",
      "\n",
      "vars_time1[29]\n",
      "\n",
      "vars_time2[0]\n",
      "\n",
      "vars_time2[1]\n",
      "\n",
      "vars_time2[2]\n",
      "\n",
      "vars_time2[3]\n",
      "\n",
      "vars_time2[4]\n",
      "\n",
      "vars_time2[5]\n",
      "\n",
      "vars_time2[6]\n",
      "\n",
      "vars_time2[7]\n",
      "\n",
      "vars_time2[8]\n",
      "\n",
      "vars_time2[9]\n",
      "\n",
      "vars_time2[10]\n",
      "\n",
      "vars_time2[11]\n",
      "\n",
      "vars_time2[12]\n",
      "\n",
      "vars_time2[13]\n",
      "\n",
      "vars_time2[14]\n",
      "\n",
      "vars_time2[15]\n",
      "\n",
      "vars_time2[16]\n",
      "\n",
      "vars_time2[17]\n",
      "\n",
      "vars_time2[18]\n",
      "\n",
      "vars_time2[19]\n",
      "\n",
      "vars_time2[20]\n",
      "\n",
      "vars_time2[21]\n",
      "\n",
      "vars_time2[22]\n",
      "\n",
      "vars_time2[23]\n",
      "\n",
      "vars_time2[24]\n",
      "\n",
      "vars_time2[25]\n",
      "\n",
      "vars_time2[26]\n",
      "\n",
      "vars_time2[27]\n",
      "\n",
      "vars_time2[28]\n",
      "\n",
      "vars_time2[29]\n",
      "\n",
      "vars_time3[0]\n",
      "\n",
      "vars_time3[1]\n",
      "\n",
      "vars_time3[2]\n",
      "\n",
      "vars_time3[3]\n",
      "\n",
      "vars_time3[4]\n",
      "\n",
      "vars_time3[5]\n",
      "\n",
      "vars_time3[6]\n",
      "\n",
      "vars_time3[7]\n",
      "\n",
      "vars_time3[8]\n",
      "\n",
      "vars_time3[9]\n",
      "\n",
      "vars_time3[10]\n",
      "\n",
      "vars_time3[11]\n",
      "\n",
      "vars_time3[12]\n",
      "\n",
      "vars_time3[13]\n",
      "\n",
      "vars_time3[14]\n",
      "\n",
      "vars_time3[15]\n",
      "\n",
      "vars_time3[16]\n",
      "\n",
      "vars_time3[17]\n",
      "\n",
      "vars_time3[18]\n",
      "\n",
      "vars_time3[19]\n",
      "\n",
      "vars_time3[20]\n",
      "\n",
      "vars_time3[21]\n",
      "\n",
      "vars_time3[22]\n",
      "\n",
      "vars_time3[23]\n",
      "\n",
      "vars_time3[24]\n",
      "\n",
      "vars_time3[25]\n",
      "\n",
      "vars_time3[26]\n",
      "\n",
      "vars_time3[27]\n",
      "\n",
      "vars_time3[28]\n",
      "\n",
      "vars_time3[29]\n",
      "\n",
      "vars_time4[0]\n",
      "\n",
      "vars_time4[1]\n",
      "\n",
      "vars_time4[2]\n",
      "\n",
      "vars_time4[3]\n",
      "\n",
      "vars_time4[4]\n",
      "\n",
      "vars_time4[5]\n",
      "\n",
      "vars_time4[6]\n",
      "\n",
      "vars_time4[7]\n",
      "\n",
      "vars_time4[8]\n",
      "\n",
      "vars_time4[9]\n",
      "\n",
      "vars_time4[10]\n",
      "\n",
      "vars_time4[11]\n",
      "\n",
      "vars_time4[12]\n",
      "\n",
      "vars_time4[13]\n",
      "\n",
      "vars_time4[14]\n",
      "\n",
      "vars_time4[15]\n",
      "\n",
      "vars_time4[16]\n",
      "\n",
      "vars_time4[17]\n",
      "\n",
      "vars_time4[18]\n",
      "\n",
      "vars_time4[19]\n",
      "\n",
      "vars_time4[20]\n",
      "\n",
      "vars_time4[21]\n",
      "\n",
      "vars_time4[22]\n",
      "\n",
      "vars_time4[23]\n",
      "\n",
      "vars_time4[24]\n",
      "\n",
      "vars_time4[25]\n",
      "\n",
      "vars_time4[26]\n",
      "\n",
      "vars_time4[27]\n",
      "\n",
      "vars_time4[28]\n",
      "\n",
      "vars_time4[29]\n",
      "\n",
      "vars_time5[0]\n",
      "\n",
      "vars_time5[1]\n",
      "\n",
      "vars_time5[2]\n",
      "\n",
      "vars_time5[3]\n",
      "\n",
      "vars_time5[4]\n",
      "\n",
      "vars_time5[5]\n",
      "\n",
      "vars_time5[6]\n",
      "\n",
      "vars_time5[7]\n",
      "\n",
      "vars_time5[8]\n",
      "\n",
      "vars_time5[9]\n",
      "\n",
      "vars_time5[10]\n",
      "\n",
      "vars_time5[11]\n",
      "\n",
      "vars_time5[12]\n",
      "\n",
      "vars_time5[13]\n",
      "\n",
      "vars_time5[14]\n",
      "\n",
      "vars_time5[15]\n",
      "\n",
      "vars_time5[16]\n",
      "\n",
      "vars_time5[17]\n",
      "\n",
      "vars_time5[18]\n",
      "\n",
      "vars_time5[19]\n",
      "\n",
      "vars_time5[20]\n",
      "\n",
      "vars_time5[21]\n",
      "\n",
      "vars_time5[22]\n",
      "\n",
      "vars_time5[23]\n",
      "\n",
      "vars_time5[24]\n",
      "\n",
      "vars_time5[25]\n",
      "\n",
      "vars_time5[26]\n",
      "\n",
      "vars_time5[27]\n",
      "\n",
      "vars_time5[28]\n",
      "\n",
      "vars_time5[29]\n",
      "\n",
      "vars_time6[0]\n",
      "\n",
      "vars_time6[1]\n",
      "\n",
      "vars_time6[2]\n",
      "\n",
      "vars_time6[3]\n",
      "\n",
      "vars_time6[4]\n",
      "\n",
      "vars_time6[5]\n",
      "\n",
      "vars_time6[6]\n",
      "\n",
      "vars_time6[7]\n",
      "\n",
      "vars_time6[8]\n",
      "\n",
      "vars_time6[9]\n",
      "\n",
      "vars_time6[10]\n",
      "\n",
      "vars_time6[11]\n",
      "\n",
      "vars_time6[12]\n",
      "\n",
      "vars_time6[13]\n",
      "\n",
      "vars_time6[14]\n",
      "\n",
      "vars_time6[15]\n",
      "\n",
      "vars_time6[16]\n",
      "\n",
      "vars_time6[17]\n",
      "\n",
      "vars_time6[18]\n",
      "\n",
      "vars_time6[19]\n",
      "\n",
      "vars_time6[20]\n",
      "\n",
      "vars_time6[21]\n",
      "\n",
      "vars_time6[22]\n",
      "\n",
      "vars_time6[23]\n",
      "\n",
      "vars_time6[24]\n",
      "\n",
      "vars_time6[25]\n",
      "\n",
      "vars_time6[26]\n",
      "\n",
      "vars_time6[27]\n",
      "\n",
      "vars_time6[28]\n",
      "\n",
      "vars_time6[29]\n",
      "\n",
      "vars_time7[0]\n",
      "\n",
      "vars_time7[1]\n",
      "\n",
      "vars_time7[2]\n",
      "\n",
      "vars_time7[3]\n",
      "\n",
      "vars_time7[4]\n",
      "\n",
      "vars_time7[5]\n",
      "\n",
      "vars_time7[6]\n",
      "\n",
      "vars_time7[7]\n",
      "\n",
      "vars_time7[8]\n",
      "\n",
      "vars_time7[9]\n",
      "\n",
      "vars_time7[10]\n",
      "\n",
      "vars_time7[11]\n",
      "\n",
      "vars_time7[12]\n",
      "\n",
      "vars_time7[13]\n",
      "\n",
      "vars_time7[14]\n",
      "\n",
      "vars_time7[15]\n",
      "\n",
      "vars_time7[16]\n",
      "\n",
      "vars_time7[17]\n",
      "\n",
      "vars_time7[18]\n",
      "\n",
      "vars_time7[19]\n",
      "\n",
      "vars_time7[20]\n",
      "\n",
      "vars_time7[21]\n",
      "\n",
      "vars_time7[22]\n",
      "\n",
      "vars_time7[23]\n",
      "\n",
      "vars_time7[24]\n",
      "\n",
      "vars_time7[25]\n",
      "\n",
      "vars_time7[26]\n",
      "\n",
      "vars_time7[27]\n",
      "\n",
      "vars_time7[28]\n",
      "\n",
      "vars_time7[29]\n",
      "\n",
      "vars_time8[0]\n",
      "\n",
      "vars_time8[1]\n",
      "\n",
      "vars_time8[2]\n",
      "\n",
      "vars_time8[3]\n",
      "\n",
      "vars_time8[4]\n",
      "\n",
      "vars_time8[5]\n",
      "\n",
      "vars_time8[6]\n",
      "\n",
      "vars_time8[7]\n",
      "\n",
      "vars_time8[8]\n",
      "\n",
      "vars_time8[9]\n",
      "\n",
      "vars_time8[10]\n",
      "\n",
      "vars_time8[11]\n",
      "\n",
      "vars_time8[12]\n",
      "\n",
      "vars_time8[13]\n",
      "\n",
      "vars_time8[14]\n",
      "\n",
      "vars_time8[15]\n",
      "\n",
      "vars_time8[16]\n",
      "\n",
      "vars_time8[17]\n",
      "\n",
      "vars_time8[18]\n",
      "\n",
      "vars_time8[19]\n",
      "\n",
      "vars_time8[20]\n",
      "\n",
      "vars_time8[21]\n",
      "\n",
      "vars_time8[22]\n",
      "\n",
      "vars_time8[23]\n",
      "\n",
      "vars_time8[24]\n",
      "\n",
      "vars_time8[25]\n",
      "\n",
      "vars_time8[26]\n",
      "\n",
      "vars_time8[27]\n",
      "\n",
      "vars_time8[28]\n",
      "\n",
      "vars_time8[29]\n",
      "\n",
      "vars_time9[0]\n",
      "\n",
      "vars_time9[1]\n",
      "\n",
      "vars_time9[2]\n",
      "\n",
      "vars_time9[3]\n",
      "\n",
      "vars_time9[4]\n",
      "\n",
      "vars_time9[5]\n",
      "\n",
      "vars_time9[6]\n",
      "\n",
      "vars_time9[7]\n",
      "\n",
      "vars_time9[8]\n",
      "\n",
      "vars_time9[9]\n",
      "\n",
      "vars_time9[10]\n",
      "\n",
      "vars_time9[11]\n",
      "\n",
      "vars_time9[12]\n",
      "\n",
      "vars_time9[13]\n",
      "\n",
      "vars_time9[14]\n",
      "\n",
      "vars_time9[15]\n",
      "\n",
      "vars_time9[16]\n",
      "\n",
      "vars_time9[17]\n",
      "\n",
      "vars_time9[18]\n",
      "\n",
      "vars_time9[19]\n",
      "\n",
      "vars_time9[20]\n",
      "\n",
      "vars_time9[21]\n",
      "\n",
      "vars_time9[22]\n",
      "\n",
      "vars_time9[23]\n",
      "\n",
      "vars_time9[24]\n",
      "\n",
      "vars_time9[25]\n",
      "\n",
      "vars_time9[26]\n",
      "\n",
      "vars_time9[27]\n",
      "\n",
      "vars_time9[28]\n",
      "\n",
      "vars_time9[29]\n",
      "\n",
      "******************************************************************************\n",
      "*Tree    :sgn       : sgn                                                    *\n",
      "*Entries :    10000 : Total =        13449901 bytes  File  Size =   11258248 *\n",
      "*        :          : Tree compression factor =   1.19                       *\n",
      "******************************************************************************\n",
      "*Br    0 :vars_time0 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1124563 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    1 :vars_time1 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1124654 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    2 :vars_time2 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1124757 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    3 :vars_time3 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1124807 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    4 :vars_time4 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1125129 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    5 :vars_time5 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1125625 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    6 :vars_time6 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1125720 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    7 :vars_time7 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1126131 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    8 :vars_time8 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1126511 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    9 :vars_time9 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1126524 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "DataSetInfo              : [dataset] : Added class \"Signal\"\n",
      "                         : Add Tree sgn of type Signal with 10000 events\n",
      "DataSetInfo              : [dataset] : Added class \"Background\"\n",
      "                         : Add Tree bkg of type Background with 10000 events\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataloader =TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "signalTree = inputFile.Get(\"sgn\")\n",
    "background = inputFile.Get(\"bkg\")\n",
    "\n",
    "signalTree.Print()\n",
    "nvar = ninput * ntime\n",
    "\n",
    "# add variables - use new AddVariablesArray function\n",
    "for i in range(ntime):\n",
    "    varName = \"vars_time\"+str(i)\n",
    "    dataloader.AddVariablesArray(varName,ninput,'F')\n",
    "\n",
    "dataloader.AddSignalTree(signalTree, 1.0)\n",
    "dataloader.AddBackgroundTree(background, 1.0)\n",
    "\n",
    "# check given input\n",
    "datainfo = dataloader.GetDataSetInfo()\n",
    "vars = datainfo.GetListOfVariables()\n",
    "print(\"number of variables is \" + str(vars.size())+ \"\\n\")\n",
    "for v in vars:\n",
    "    print(str(v)+\"\\n\")\n",
    "\n",
    "nTrainSig = 0.8 * nTotEvts\n",
    "nTrainBkg = 0.8 *  nTotEvts\n",
    "\n",
    "#build the string options for DataLoader::PrepareTrainingAndTestTree\n",
    "prepareOptions = \"nTrain_Signal=\"+str(nTrainSig)+\":nTrain_Background=\"+str(nTrainBkg)+\":SplitMode=Random:SplitSeed=100:NormMode=NumEvents:!V:!CalcCorrelations\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b9dde",
   "metadata": {},
   "source": [
    "###  Tell the factory how to use the training and testing events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ae30b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared DATA LOADER \n"
     ]
    }
   ],
   "source": [
    "# Apply additional cuts on the signal and background samples (can be different)\n",
    "mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "dataloader.PrepareTrainingAndTestTree(mycuts, mycutb, prepareOptions)\n",
    "\n",
    "print(\"prepared DATA LOADER \" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23491509",
   "metadata": {},
   "source": [
    "### Book TMVA  recurrent models\n",
    "\n",
    "Book the different types of recurrent models in TMVA  (SimpleRNN, LSTM or GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd60f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mTMVA_RNN\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=RNN|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=RNN|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"10|30\" [The Layout of the input]\n",
      "                         :     Layout: \"RNN|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     RandomSeed: \"1234\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     ValidationSize: \"0.2\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n",
      "Factory                  : Booking method: \u001b[1mTMVA_LSTM\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"10|30\" [The Layout of the input]\n",
      "                         :     Layout: \"LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     RandomSeed: \"1234\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     ValidationSize: \"0.2\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n",
      "Factory                  : Booking method: \u001b[1mTMVA_GRU\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=GRU|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=GRU|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"10|30\" [The Layout of the input]\n",
      "                         :     Layout: \"GRU|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     RandomSeed: \"1234\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     ValidationSize: \"0.2\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if (useTMVA_RNN):\n",
    "    for i in range(3):\n",
    "        if (use_rnn_type[i]==None):\n",
    "            continue\n",
    "        rnn_type = str(rnn_types[i])\n",
    "\n",
    "#          define the inputlayout string for RNN\n",
    "#          the input data should be organize as   following:\n",
    "#          input layout for RNN:    time x ndim\n",
    "\n",
    "        inputLayoutString = \"InputLayout=\"+str(ntime)+\"|\"+str(ninput)\n",
    "\n",
    "        # Define RNN layer layout\n",
    "        # it should be   LayerType (RNN or LSTM or GRU) |  number of units | number of inputs | time steps | remember output (typically no=0 | return full sequence\n",
    "        rnnLayout = str(rnn_type) + \"|10|\"+ str(ninput) + \"|\" + str(ntime) + \"|0|1\"\n",
    "\n",
    "        #        add after RNN a reshape layer (needed top flatten the output) and a dense layer with 64 units and a last one\n",
    "        #        Note the last layer is linear because  when using Crossentropy a Sigmoid is applied already\n",
    "        layoutString =\"Layout=\" + rnnLayout + \",RESHAPE|FLAT,DENSE|64|TANH,LINEAR\"\n",
    "\n",
    "        #Defining Training strategies. Different training strings can be concatenate. Use however only one\n",
    "        trainingString1 = \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,\"+\"ConvergenceSteps=5,BatchSize=\"+str(batchSize)+\",TestRepetitions=1,\"+\"WeightDecay=1e-2,Regularization=None,MaxEpochs=\"+str(maxepochs\n",
    "        )+\",\"+\"Optimizer=ADAM,DropConfig=0.0+0.+0.+0.\"\n",
    "\n",
    "        trainingStrategyString=\"TrainingStrategy=\"\n",
    "        trainingStrategyString += trainingString1; # + \"|\" + trainingString2\n",
    "\n",
    "        # Define the full RNN Noption string adding the final options for all network\n",
    "        rnnOptions = \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"+\"WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234\"\n",
    "        rnnOptions +=  \":\" + inputLayoutString\n",
    "        rnnOptions +=  \":\" + layoutString\n",
    "        rnnOptions +=  \":\" + trainingStrategyString\n",
    "        rnnOptions +=  \":\" + \"Architecture=\" + str(archString)\n",
    "\n",
    "        rnnName = \"TMVA_\" + rnn_type\n",
    "        factory.BookMethod(dataloader, TMVA.Types.kDL, rnnName, rnnOptions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff56bbe",
   "metadata": {},
   "source": [
    "### Book TMVA  fully connected dense layer  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1188f225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mTMVA_DNN\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"1|1|300\" [The Layout of the input]\n",
      "                         :     Layout: \"DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIER\" [Weight initialization strategy]\n",
      "                         :     RandomSeed: \"0\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         :     ValidationSize: \"20%\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if (useTMVA_DNN):\n",
    "#    Method DL with Dense Layer\n",
    "    inputLayoutString = \"InputLayout=1|1|\" + str(ntime * ninput)\n",
    "\n",
    "    layoutString = \"Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR\"\n",
    "#   Training strategies.\n",
    "    trainingString1 = \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,\"+\"ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,\"+\"WeightDecay=1e-4,Regularization=None,MaxEpochs=20\"+\"DropConfig=0.0+0.+0.+0.,Optimizer=ADAM\"\n",
    "    trainingStrategyString = \"TrainingStrategy=\"\n",
    "    trainingStrategyString += trainingString1 # + \"|\" + trainingString2\n",
    "\n",
    "      # General Options.\n",
    "    dnnOptions = \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"+\"WeightInitialization=XAVIER:RandomSeed=0\" \n",
    "\n",
    "    dnnOptions +=  \":\" + inputLayoutString\n",
    "    dnnOptions +=  \":\" + layoutString\n",
    "    dnnOptions +=  \":\" + trainingStrategyString\n",
    "    dnnOptions +=  \":\" + \"Architecture=\" + str(archString)\n",
    "\n",
    "\n",
    "    dnnName = \"TMVA_DNN\"\n",
    "    factory.BookMethod(dataloader, TMVA.Types.kDL, dnnName, dnnOptions)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f3f41",
   "metadata": {},
   "source": [
    "### Book Keras recurrent models\n",
    "\n",
    "Book the different types of recurrent models in Keras  (SimpleRNN, LSTM or GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5da87118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 10, 30)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10, 10)            1640      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10, 10)           40        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                6464      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,274\n",
      "Trainable params: 8,254\n",
      "Non-trainable params: 20\n",
      "_________________________________________________________________\n",
      "Factory                  : Booking method: \u001b[1mPyKeras_LSTM\u001b[0m\n",
      "                         : \n",
      "                         : Setting up tf.keras\n",
      "                         : Using TensorFlow version 2\n",
      "                         : Use Keras version from TensorFlow : tf.keras\n",
      "                         : Applying GPU option:  gpu_options.allow_growth=True\n",
      "                         :  Loading Keras Model \n",
      "                         : Loaded model from file: model_LSTM.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-08 19:08:22.035384: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/neel/Root/install/lib:/opt/ros/noetic/lib\n",
      "2022-04-08 19:08:22.035411: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Info in <TMVA_RNN_Classification>: Building recurrent keras model using aLSTM layer\n",
      "2022-04-08 19:08:23.852191: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/neel/Root/install/lib:/opt/ros/noetic/lib\n",
      "2022-04-08 19:08:23.852221: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-08 19:08:23.852240: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (neel-HP-Pavilion-15-Notebook-PC): /proc/driver/nvidia/version does not exist\n",
      "2022-04-08 19:08:23.852458: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "python: can't open file 'make_rnn_model.py': [Errno 2] No such file or directory\n",
      "Info in <TMVA_RNN_Classification>: Booking KerasLSTMmodel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape, BatchNormalization\n",
    "\n",
    "\n",
    "if (useKeras):\n",
    "    for i in range(3):\n",
    "        if (use_rnn_type[i]):\n",
    "            modelName = \"model_\" + str(rnn_types[i]) + \".h5\"\n",
    "            trainedModelName = \"trained_model_\"+ str(rnn_types[i]) + \".h5\"\n",
    "\n",
    "            ROOT.Info(\"TMVA_RNN_Classification\", \"Building recurrent keras model using a\"+str(rnn_types[i])+\" layer\")\n",
    "            # create python script which can be executed\n",
    "            # create 2 conv2d layer + maxpool + dense\n",
    "        \n",
    "            \n",
    "            \n",
    "            model = Sequential()\n",
    "            model.add(Reshape((10, 30), input_shape = (10*30, )))\n",
    "            # add recurrent neural network depending on type / Use option to return the full output\n",
    "            if (rnn_types[i] == \"LSTM\"):\n",
    "               model.add(LSTM(units=10, return_sequences=True) )\n",
    "            elif (rnn_types[i] == \"GRU\"):\n",
    "               model.add(GRU(units=10, return_sequences=True) )\n",
    "            else:\n",
    "               model.add(SimpleRNN(units=10, return_sequences=True) )\n",
    "\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Flatten())# needed if returning the full time output sequen\n",
    "            model.add(Dense(64, activation = 'tanh')) \n",
    "            model.add(Dense(2, activation = 'sigmoid')) \n",
    "            model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), metrics = ['accuracy'])\n",
    "            \n",
    "            model.save(modelName)\n",
    "            model.summary()\n",
    "\n",
    "#             m.SaveSource(\"make_rnn_model.py\");\n",
    "#              execute\n",
    "            ROOT.gSystem.Exec(\"python make_rnn_model.py\")\n",
    "\n",
    "            if (ROOT.gSystem.AccessPathName(modelName)):\n",
    "               Warning(\"TMVA_RNN_Classification\", \"Error creating Keras recurrent model file - Skip using Keras\")\n",
    "               useKeras = False\n",
    "            else:\n",
    "               # book PyKeras method only if Keras model could be created\n",
    "               ROOT.Info(\"TMVA_RNN_Classification\", \"Booking Keras\" + str(rnn_types[i]) +  \"model\")\n",
    "               factory.BookMethod(dataloader, TMVA.Types.kPyKeras,\"PyKeras_\"+ str(rnn_types[i]),\"!H:!V:VarTransform=None:FilenameModel=\"+str(modelName)+\":tf.keras:\"+\"FilenameTrainedModel=\"+str(trainedModelName)+\":GpuOptions=allow_growth=True:\"+\"NumEpochs=\"+str(maxepochs)+\":BatchSize=\"+str(batchSize))\n",
    "                                                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e529e",
   "metadata": {},
   "source": [
    "### Training All Methods\n",
    "\n",
    "Here we train all the previously booked methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da594215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 10, 30)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10, 10)            1640      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10, 10)           40        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                6464      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,274\n",
      "Trainable params: 8,254\n",
      "Non-trainable params: 20\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "122/128 [===========================>..] - ETA: 0s - loss: 0.6418 - accuracy: 0.6298"
     ]
    }
   ],
   "source": [
    "# Train all methods\n",
    "factory.TrainAllMethods()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7594723",
   "metadata": {},
   "source": [
    "### Test all methods\n",
    "\n",
    "Now we test  all methods using the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a088af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nthreads  = \"+ str(ROOT.GetThreadPoolSize()) + \"\\n\")\n",
    "\n",
    "# Evaluate all MVAs using the set of test events\n",
    "factory.TestAllMethods()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef639a",
   "metadata": {},
   "source": [
    "### Evaluate all methods\n",
    "\n",
    "Here we evaluate all methods and compare their performances, computing efficiencies, ROC curves etc.. using both training and tetsing data sets. Several histograms are produced which can be examined with the TMVAGui or directly using the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and compare performance of all configured MVAs\n",
    "factory.EvaluateAllMethods()\n",
    "#  check method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191510e",
   "metadata": {},
   "source": [
    "### Plot ROC Curve\n",
    "Here we plot the ROC curve and display the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot ROC curve\n",
    "c1 = factory.GetROCCurve(dataloader)\n",
    "c1.Draw()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9f421",
   "metadata": {},
   "source": [
    "### Close the Output File\n",
    "Close outputfile to save all output information (evaluation result of methods) and it can be used by TMVAGUI to display additional plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02873c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (outputFile):\n",
    "    outputFile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f11903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
