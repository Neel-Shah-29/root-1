{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1af11d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \\file\n",
    "#  \\ingroup tutorial_tmva\n",
    "#  \\notebook\n",
    "#   TMVA Classification Example Using a Recurrent Neural Network\n",
    "# \n",
    "#  This is an example of using a RNN in TMVA. We do classification using a toy time dependent data set\n",
    "#  that is generated when running this example macro\n",
    "# \n",
    "#  \\macro_image\n",
    "#  \\macro_output\n",
    "#  \\macro_code\n",
    "# \n",
    "#  \\author Neel Shah\n",
    "\n",
    "\n",
    "# TMVA Classification Example Using a Recurrent Neural Network\n",
    "\n",
    "# This is an example of using a RNN in TMVA.\n",
    "# We do the classification using a toy data set containing a time series of data sample ntimes\n",
    "# and with dimension ndim that is generated when running the provided function `MakeTimeData (nevents, ntime, ndim)`\n",
    "\n",
    "import ROOT\n",
    "from ROOT import TMVA\n",
    "\n",
    "ninput = 30\n",
    "# ninput=10\n",
    "ntime = 10\n",
    "# batchSize = 100\n",
    "batchSize = 100\n",
    "maxepochs = 20\n",
    "\n",
    "use_type = 1\n",
    "\n",
    "nTotEvts = 10000 # total events to be generated for signal or background\n",
    "\n",
    "useKeras = True\n",
    "\n",
    "\n",
    "useTMVA_RNN = True\n",
    "useTMVA_DNN = True\n",
    "useTMVA_BDT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cc7e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn_types = [\"RNN\", \"LSTM\", \"GRU\"]\n",
    "use_rnn_type = [1, 1, 1]\n",
    "if (use_type >=0 & use_type < 3) :\n",
    "      use_rnn_type = [0,0,0]\n",
    "      use_rnn_type[use_type] = 1;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "936061e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "archString = \"CPU\"\n",
    "writeOutputFile = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b1d1590",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_type = \"RNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06612027",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT.TMVA.PyMethodBase.PyInitialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f588b1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with nthreads  = 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_threads = 0   # use by default all threads\n",
    "#    do enable MT running\n",
    "if (num_threads >= 0):\n",
    "    ROOT.EnableImplicitMT(num_threads)\n",
    "    if (num_threads > 0):\n",
    "        ROOT.gSystem.Setenv(\"OMP_NUM_THREADS\", num_threads)\n",
    "    else:\n",
    "      ROOT.gSystem.Setenv(\"OMP_NUM_THREADS\", \"1\")\n",
    "\n",
    "ROOT.TMVA.Config.Instance()\n",
    "\n",
    "print(\"Running with nthreads  = \" + str(ROOT.GetThreadPoolSize()) + \"\\n\" )\n",
    "\n",
    "inputFileName = \"time_data_t10_d30.root\";\n",
    "\n",
    "fileExist = ROOT.gSystem.AccessPathName(inputFileName)\n",
    "\n",
    "#if file does not exists create it\n",
    "if (fileExist==None):\n",
    "    MakeTimeData(nTotEvts,ntime, ninput)\n",
    "\n",
    "inputFile = ROOT.TFile.Open(inputFileName)\n",
    "if (inputFile==None):\n",
    "    Error(\"TMVA_RNN_Classification\", \"Error opening input file %s - exit\", inputFileName.Data())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d49cde7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RNNClassification  : Using input file: time_data_t10_d30.root\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    ## Declare Factory\\n\\n    Create the Factory class. Later you can choose the methods\\n    whose performance you\\'d like to investigate.\\n\\n    The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to\\npass\\n\\n    - The first argument is the base of the name of all the output\\n    weightfiles in the directory weight/ that will be created with the\\n    method parameters\\n\\n    - The second argument is the output file for the training results\\n\\n    - The third argument is a string option defining some general configuration for the TMVA session.\\n      For example all TMVA output can be suppressed by removing the \"!\" (not) in front of the \"Silent\" argument in\\nthe option string\\n\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--- RNNClassification  : Using input file: \" + inputFile.GetName()+\"\\n\")\n",
    "\n",
    "#   Create a ROOT output file where TMVA will store ntuples, histograms, etc.\n",
    "outfileName = \"data_RNN_\"+ archString +\".root\"\n",
    "\n",
    "if (writeOutputFile):\n",
    "    outputFile = ROOT.TFile.Open(outfileName, \"RECREATE\")\n",
    "\n",
    "'''\n",
    "    ## Declare Factory\n",
    "\n",
    "    Create the Factory class. Later you can choose the methods\n",
    "    whose performance you'd like to investigate.\n",
    "\n",
    "    The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to\n",
    "pass\n",
    "\n",
    "    - The first argument is the base of the name of all the output\n",
    "    weightfiles in the directory weight/ that will be created with the\n",
    "    method parameters\n",
    "\n",
    "    - The second argument is the output file for the training results\n",
    "\n",
    "    - The third argument is a string option defining some general configuration for the TMVA session.\n",
    "      For example all TMVA output can be suppressed by removing the \"!\" (not) in front of the \"Silent\" argument in\n",
    "the option string\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11738b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of variables is 300\n",
      "\n",
      "vars_time0[0]\n",
      "\n",
      "vars_time0[1]\n",
      "\n",
      "vars_time0[2]\n",
      "\n",
      "vars_time0[3]\n",
      "\n",
      "vars_time0[4]\n",
      "\n",
      "vars_time0[5]\n",
      "\n",
      "vars_time0[6]\n",
      "\n",
      "vars_time0[7]\n",
      "\n",
      "vars_time0[8]\n",
      "\n",
      "vars_time0[9]\n",
      "\n",
      "vars_time0[10]\n",
      "\n",
      "vars_time0[11]\n",
      "\n",
      "vars_time0[12]\n",
      "\n",
      "vars_time0[13]\n",
      "\n",
      "vars_time0[14]\n",
      "\n",
      "vars_time0[15]\n",
      "\n",
      "vars_time0[16]\n",
      "\n",
      "vars_time0[17]\n",
      "\n",
      "vars_time0[18]\n",
      "\n",
      "vars_time0[19]\n",
      "\n",
      "vars_time0[20]\n",
      "\n",
      "vars_time0[21]\n",
      "\n",
      "vars_time0[22]\n",
      "\n",
      "vars_time0[23]\n",
      "\n",
      "vars_time0[24]\n",
      "\n",
      "vars_time0[25]\n",
      "\n",
      "vars_time0[26]\n",
      "\n",
      "vars_time0[27]\n",
      "\n",
      "vars_time0[28]\n",
      "\n",
      "vars_time0[29]\n",
      "\n",
      "vars_time1[0]\n",
      "\n",
      "vars_time1[1]\n",
      "\n",
      "vars_time1[2]\n",
      "\n",
      "vars_time1[3]\n",
      "\n",
      "vars_time1[4]\n",
      "\n",
      "vars_time1[5]\n",
      "\n",
      "vars_time1[6]\n",
      "\n",
      "vars_time1[7]\n",
      "\n",
      "vars_time1[8]\n",
      "\n",
      "vars_time1[9]\n",
      "\n",
      "vars_time1[10]\n",
      "\n",
      "vars_time1[11]\n",
      "\n",
      "vars_time1[12]\n",
      "\n",
      "vars_time1[13]\n",
      "\n",
      "vars_time1[14]\n",
      "\n",
      "vars_time1[15]\n",
      "\n",
      "vars_time1[16]\n",
      "\n",
      "vars_time1[17]\n",
      "\n",
      "vars_time1[18]\n",
      "\n",
      "vars_time1[19]\n",
      "\n",
      "vars_time1[20]\n",
      "\n",
      "vars_time1[21]\n",
      "\n",
      "vars_time1[22]\n",
      "\n",
      "vars_time1[23]\n",
      "\n",
      "vars_time1[24]\n",
      "\n",
      "vars_time1[25]\n",
      "\n",
      "vars_time1[26]\n",
      "\n",
      "vars_time1[27]\n",
      "\n",
      "vars_time1[28]\n",
      "\n",
      "vars_time1[29]\n",
      "\n",
      "vars_time2[0]\n",
      "\n",
      "vars_time2[1]\n",
      "\n",
      "vars_time2[2]\n",
      "\n",
      "vars_time2[3]\n",
      "\n",
      "vars_time2[4]\n",
      "\n",
      "vars_time2[5]\n",
      "\n",
      "vars_time2[6]\n",
      "\n",
      "vars_time2[7]\n",
      "\n",
      "vars_time2[8]\n",
      "\n",
      "vars_time2[9]\n",
      "\n",
      "vars_time2[10]\n",
      "\n",
      "vars_time2[11]\n",
      "\n",
      "vars_time2[12]\n",
      "\n",
      "vars_time2[13]\n",
      "\n",
      "vars_time2[14]\n",
      "\n",
      "vars_time2[15]\n",
      "\n",
      "vars_time2[16]\n",
      "\n",
      "vars_time2[17]\n",
      "\n",
      "vars_time2[18]\n",
      "\n",
      "vars_time2[19]\n",
      "\n",
      "vars_time2[20]\n",
      "\n",
      "vars_time2[21]\n",
      "\n",
      "vars_time2[22]\n",
      "\n",
      "vars_time2[23]\n",
      "\n",
      "vars_time2[24]\n",
      "\n",
      "vars_time2[25]\n",
      "\n",
      "vars_time2[26]\n",
      "\n",
      "vars_time2[27]\n",
      "\n",
      "vars_time2[28]\n",
      "\n",
      "vars_time2[29]\n",
      "\n",
      "vars_time3[0]\n",
      "\n",
      "vars_time3[1]\n",
      "\n",
      "vars_time3[2]\n",
      "\n",
      "vars_time3[3]\n",
      "\n",
      "vars_time3[4]\n",
      "\n",
      "vars_time3[5]\n",
      "\n",
      "vars_time3[6]\n",
      "\n",
      "vars_time3[7]\n",
      "\n",
      "vars_time3[8]\n",
      "\n",
      "vars_time3[9]\n",
      "\n",
      "vars_time3[10]\n",
      "\n",
      "vars_time3[11]\n",
      "\n",
      "vars_time3[12]\n",
      "\n",
      "vars_time3[13]\n",
      "\n",
      "vars_time3[14]\n",
      "\n",
      "vars_time3[15]\n",
      "\n",
      "vars_time3[16]\n",
      "\n",
      "vars_time3[17]\n",
      "\n",
      "vars_time3[18]\n",
      "\n",
      "vars_time3[19]\n",
      "\n",
      "vars_time3[20]\n",
      "\n",
      "vars_time3[21]\n",
      "\n",
      "vars_time3[22]\n",
      "\n",
      "vars_time3[23]\n",
      "\n",
      "vars_time3[24]\n",
      "\n",
      "vars_time3[25]\n",
      "\n",
      "vars_time3[26]\n",
      "\n",
      "vars_time3[27]\n",
      "\n",
      "vars_time3[28]\n",
      "\n",
      "vars_time3[29]\n",
      "\n",
      "vars_time4[0]\n",
      "\n",
      "vars_time4[1]\n",
      "\n",
      "vars_time4[2]\n",
      "\n",
      "vars_time4[3]\n",
      "\n",
      "vars_time4[4]\n",
      "\n",
      "vars_time4[5]\n",
      "\n",
      "vars_time4[6]\n",
      "\n",
      "vars_time4[7]\n",
      "\n",
      "vars_time4[8]\n",
      "\n",
      "vars_time4[9]\n",
      "\n",
      "vars_time4[10]\n",
      "\n",
      "vars_time4[11]\n",
      "\n",
      "vars_time4[12]\n",
      "\n",
      "vars_time4[13]\n",
      "\n",
      "vars_time4[14]\n",
      "\n",
      "vars_time4[15]\n",
      "\n",
      "vars_time4[16]\n",
      "\n",
      "vars_time4[17]\n",
      "\n",
      "vars_time4[18]\n",
      "\n",
      "vars_time4[19]\n",
      "\n",
      "vars_time4[20]\n",
      "\n",
      "vars_time4[21]\n",
      "\n",
      "vars_time4[22]\n",
      "\n",
      "vars_time4[23]\n",
      "\n",
      "vars_time4[24]\n",
      "\n",
      "vars_time4[25]\n",
      "\n",
      "vars_time4[26]\n",
      "\n",
      "vars_time4[27]\n",
      "\n",
      "vars_time4[28]\n",
      "\n",
      "vars_time4[29]\n",
      "\n",
      "vars_time5[0]\n",
      "\n",
      "vars_time5[1]\n",
      "\n",
      "vars_time5[2]\n",
      "\n",
      "vars_time5[3]\n",
      "\n",
      "vars_time5[4]\n",
      "\n",
      "vars_time5[5]\n",
      "\n",
      "vars_time5[6]\n",
      "\n",
      "vars_time5[7]\n",
      "\n",
      "vars_time5[8]\n",
      "\n",
      "vars_time5[9]\n",
      "\n",
      "vars_time5[10]\n",
      "\n",
      "vars_time5[11]\n",
      "\n",
      "vars_time5[12]\n",
      "\n",
      "vars_time5[13]\n",
      "\n",
      "vars_time5[14]\n",
      "\n",
      "vars_time5[15]\n",
      "\n",
      "vars_time5[16]\n",
      "\n",
      "vars_time5[17]\n",
      "\n",
      "vars_time5[18]\n",
      "\n",
      "vars_time5[19]\n",
      "\n",
      "vars_time5[20]\n",
      "\n",
      "vars_time5[21]\n",
      "\n",
      "vars_time5[22]\n",
      "\n",
      "vars_time5[23]\n",
      "\n",
      "vars_time5[24]\n",
      "\n",
      "vars_time5[25]\n",
      "\n",
      "vars_time5[26]\n",
      "\n",
      "vars_time5[27]\n",
      "\n",
      "vars_time5[28]\n",
      "\n",
      "vars_time5[29]\n",
      "\n",
      "vars_time6[0]\n",
      "\n",
      "vars_time6[1]\n",
      "\n",
      "vars_time6[2]\n",
      "\n",
      "vars_time6[3]\n",
      "\n",
      "vars_time6[4]\n",
      "\n",
      "vars_time6[5]\n",
      "\n",
      "vars_time6[6]\n",
      "\n",
      "vars_time6[7]\n",
      "\n",
      "vars_time6[8]\n",
      "\n",
      "vars_time6[9]\n",
      "\n",
      "vars_time6[10]\n",
      "\n",
      "vars_time6[11]\n",
      "\n",
      "vars_time6[12]\n",
      "\n",
      "vars_time6[13]\n",
      "\n",
      "vars_time6[14]\n",
      "\n",
      "vars_time6[15]\n",
      "\n",
      "vars_time6[16]\n",
      "\n",
      "vars_time6[17]\n",
      "\n",
      "vars_time6[18]\n",
      "\n",
      "vars_time6[19]\n",
      "\n",
      "vars_time6[20]\n",
      "\n",
      "vars_time6[21]\n",
      "\n",
      "vars_time6[22]\n",
      "\n",
      "vars_time6[23]\n",
      "\n",
      "vars_time6[24]\n",
      "\n",
      "vars_time6[25]\n",
      "\n",
      "vars_time6[26]\n",
      "\n",
      "vars_time6[27]\n",
      "\n",
      "vars_time6[28]\n",
      "\n",
      "vars_time6[29]\n",
      "\n",
      "vars_time7[0]\n",
      "\n",
      "vars_time7[1]\n",
      "\n",
      "vars_time7[2]\n",
      "\n",
      "vars_time7[3]\n",
      "\n",
      "vars_time7[4]\n",
      "\n",
      "vars_time7[5]\n",
      "\n",
      "vars_time7[6]\n",
      "\n",
      "vars_time7[7]\n",
      "\n",
      "vars_time7[8]\n",
      "\n",
      "vars_time7[9]\n",
      "\n",
      "vars_time7[10]\n",
      "\n",
      "vars_time7[11]\n",
      "\n",
      "vars_time7[12]\n",
      "\n",
      "vars_time7[13]\n",
      "\n",
      "vars_time7[14]\n",
      "\n",
      "vars_time7[15]\n",
      "\n",
      "vars_time7[16]\n",
      "\n",
      "vars_time7[17]\n",
      "\n",
      "vars_time7[18]\n",
      "\n",
      "vars_time7[19]\n",
      "\n",
      "vars_time7[20]\n",
      "\n",
      "vars_time7[21]\n",
      "\n",
      "vars_time7[22]\n",
      "\n",
      "vars_time7[23]\n",
      "\n",
      "vars_time7[24]\n",
      "\n",
      "vars_time7[25]\n",
      "\n",
      "vars_time7[26]\n",
      "\n",
      "vars_time7[27]\n",
      "\n",
      "vars_time7[28]\n",
      "\n",
      "vars_time7[29]\n",
      "\n",
      "vars_time8[0]\n",
      "\n",
      "vars_time8[1]\n",
      "\n",
      "vars_time8[2]\n",
      "\n",
      "vars_time8[3]\n",
      "\n",
      "vars_time8[4]\n",
      "\n",
      "vars_time8[5]\n",
      "\n",
      "vars_time8[6]\n",
      "\n",
      "vars_time8[7]\n",
      "\n",
      "vars_time8[8]\n",
      "\n",
      "vars_time8[9]\n",
      "\n",
      "vars_time8[10]\n",
      "\n",
      "vars_time8[11]\n",
      "\n",
      "vars_time8[12]\n",
      "\n",
      "vars_time8[13]\n",
      "\n",
      "vars_time8[14]\n",
      "\n",
      "vars_time8[15]\n",
      "\n",
      "vars_time8[16]\n",
      "\n",
      "vars_time8[17]\n",
      "\n",
      "vars_time8[18]\n",
      "\n",
      "vars_time8[19]\n",
      "\n",
      "vars_time8[20]\n",
      "\n",
      "vars_time8[21]\n",
      "\n",
      "vars_time8[22]\n",
      "\n",
      "vars_time8[23]\n",
      "\n",
      "vars_time8[24]\n",
      "\n",
      "vars_time8[25]\n",
      "\n",
      "vars_time8[26]\n",
      "\n",
      "vars_time8[27]\n",
      "\n",
      "vars_time8[28]\n",
      "\n",
      "vars_time8[29]\n",
      "\n",
      "vars_time9[0]\n",
      "\n",
      "vars_time9[1]\n",
      "\n",
      "vars_time9[2]\n",
      "\n",
      "vars_time9[3]\n",
      "\n",
      "vars_time9[4]\n",
      "\n",
      "vars_time9[5]\n",
      "\n",
      "vars_time9[6]\n",
      "\n",
      "vars_time9[7]\n",
      "\n",
      "vars_time9[8]\n",
      "\n",
      "vars_time9[9]\n",
      "\n",
      "vars_time9[10]\n",
      "\n",
      "vars_time9[11]\n",
      "\n",
      "vars_time9[12]\n",
      "\n",
      "vars_time9[13]\n",
      "\n",
      "vars_time9[14]\n",
      "\n",
      "vars_time9[15]\n",
      "\n",
      "vars_time9[16]\n",
      "\n",
      "vars_time9[17]\n",
      "\n",
      "vars_time9[18]\n",
      "\n",
      "vars_time9[19]\n",
      "\n",
      "vars_time9[20]\n",
      "\n",
      "vars_time9[21]\n",
      "\n",
      "vars_time9[22]\n",
      "\n",
      "vars_time9[23]\n",
      "\n",
      "vars_time9[24]\n",
      "\n",
      "vars_time9[25]\n",
      "\n",
      "vars_time9[26]\n",
      "\n",
      "vars_time9[27]\n",
      "\n",
      "vars_time9[28]\n",
      "\n",
      "vars_time9[29]\n",
      "\n",
      "******************************************************************************\n",
      "*Tree    :sgn       : sgn                                                    *\n",
      "*Entries :    10000 : Total =        13449901 bytes  File  Size =   11258248 *\n",
      "*        :          : Tree compression factor =   1.19                       *\n",
      "******************************************************************************\n",
      "*Br    0 :vars_time0 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1124563 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    1 :vars_time1 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1124654 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    2 :vars_time2 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1124757 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    3 :vars_time3 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1124807 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    4 :vars_time4 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1125129 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    5 :vars_time5 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1125625 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    6 :vars_time6 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1125720 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    7 :vars_time7 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1126131 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    8 :vars_time8 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1126511 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "*Br    9 :vars_time9 : vector<float>                                         *\n",
      "*Entries :    10000 : Total  Size=    1344905 bytes  File Size  =    1126524 *\n",
      "*Baskets :       44 : Basket Size=      32000 bytes  Compression=   1.19     *\n",
      "*............................................................................*\n",
      "DataSetInfo              : [dataset] : Added class \"Signal\"\n",
      "                         : Add Tree sgn of type Signal with 10000 events\n",
      "DataSetInfo              : [dataset] : Added class \"Background\"\n",
      "                         : Add Tree bkg of type Background with 10000 events\n"
     ]
    }
   ],
   "source": [
    "#  Creating the factory object\n",
    "factory = ROOT.TMVA.Factory(\"TMVAClassification\", outputFile,\"!V:!Silent:Color:DrawProgressBar:Transformations=None:!Correlations:\"+\"AnalysisType=Classification:ModelPersistence\")\n",
    "dataloader =TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "signalTree = inputFile.Get(\"sgn\")\n",
    "background = inputFile.Get(\"bkg\")\n",
    "\n",
    "signalTree.Print()\n",
    "nvar = ninput * ntime\n",
    "\n",
    "# add variables - use new AddVariablesArray function\n",
    "for i in range(ntime):\n",
    "    varName = \"vars_time\"+str(i)\n",
    "    dataloader.AddVariablesArray(varName,ninput,'F')\n",
    "\n",
    "dataloader.AddSignalTree(signalTree, 1.0)\n",
    "dataloader.AddBackgroundTree(background, 1.0)\n",
    "\n",
    "# check given input\n",
    "datainfo = dataloader.GetDataSetInfo()\n",
    "vars = datainfo.GetListOfVariables()\n",
    "print(\"number of variables is \" + str(vars.size())+ \"\\n\")\n",
    "for v in vars:\n",
    "    print(str(v)+\"\\n\")\n",
    "\n",
    "nTrainSig = 0.8 * nTotEvts\n",
    "nTrainBkg = 0.8 *  nTotEvts\n",
    "\n",
    "#build the string options for DataLoader::PrepareTrainingAndTestTree\n",
    "prepareOptions = \"nTrain_Signal=\"+str(nTrainSig)+\":nTrain_Background=\"+str(nTrainBkg)+\":SplitMode=Random:SplitSeed=100:NormMode=NumEvents:!V:!CalcCorrelations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ae30b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared DATA LOADER \n"
     ]
    }
   ],
   "source": [
    "# Apply additional cuts on the signal and background samples (can be different)\n",
    "mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "dataloader.PrepareTrainingAndTestTree(mycuts, mycutb, prepareOptions)\n",
    "\n",
    "print(\"prepared DATA LOADER \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbd60f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mTMVA_RNN\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=RNN|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=RNN|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"10|30\" [The Layout of the input]\n",
      "                         :     Layout: \"RNN|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     RandomSeed: \"1234\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     ValidationSize: \"0.2\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n",
      "Factory                  : Booking method: \u001b[1mTMVA_LSTM\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"10|30\" [The Layout of the input]\n",
      "                         :     Layout: \"LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     RandomSeed: \"1234\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     ValidationSize: \"0.2\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n",
      "Factory                  : Booking method: \u001b[1mTMVA_GRU\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=GRU|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=GRU|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"10|30\" [The Layout of the input]\n",
      "                         :     Layout: \"GRU|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     RandomSeed: \"1234\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     ValidationSize: \"0.2\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "## Book TMVA  recurrent models\n",
    "\n",
    "Book the different types of recurrent models in TMVA  (SimpleRNN, LSTM or GRU)\n",
    "'''\n",
    "\n",
    "if (useTMVA_RNN):\n",
    "    for i in range(3):\n",
    "        if (use_rnn_type[i]==None):\n",
    "            continue\n",
    "        rnn_type = str(rnn_types[i])\n",
    "\n",
    "#          define the inputlayout string for RNN\n",
    "#          the input data should be organize as   following:\n",
    "#          input layout for RNN:    time x ndim\n",
    "\n",
    "        inputLayoutString = \"InputLayout=\"+str(ntime)+\"|\"+str(ninput)\n",
    "\n",
    "        # Define RNN layer layout\n",
    "        # it should be   LayerType (RNN or LSTM or GRU) |  number of units | number of inputs | time steps | remember output (typically no=0 | return full sequence\n",
    "        rnnLayout = str(rnn_type) + \"|10|\"+ str(ninput) + \"|\" + str(ntime) + \"|0|1\"\n",
    "\n",
    "        #        add after RNN a reshape layer (needed top flatten the output) and a dense layer with 64 units and a last one\n",
    "        #        Note the last layer is linear because  when using Crossentropy a Sigmoid is applied already\n",
    "        layoutString =\"Layout=\" + rnnLayout + \",RESHAPE|FLAT,DENSE|64|TANH,LINEAR\"\n",
    "\n",
    "        #Defining Training strategies. Different training strings can be concatenate. Use however only one\n",
    "        trainingString1 = \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,\"+\"ConvergenceSteps=5,BatchSize=\"+str(batchSize)+\",TestRepetitions=1,\"+\"WeightDecay=1e-2,Regularization=None,MaxEpochs=\"+str(maxepochs\n",
    "        )+\",\"+\"Optimizer=ADAM,DropConfig=0.0+0.+0.+0.\"\n",
    "\n",
    "        trainingStrategyString=\"TrainingStrategy=\"\n",
    "        trainingStrategyString += trainingString1; # + \"|\" + trainingString2\n",
    "\n",
    "        # Define the full RNN Noption string adding the final options for all network\n",
    "        rnnOptions = \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"+\"WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234\"\n",
    "        rnnOptions +=  \":\" + inputLayoutString\n",
    "        rnnOptions +=  \":\" + layoutString\n",
    "        rnnOptions +=  \":\" + trainingStrategyString\n",
    "        rnnOptions +=  \":\" + \"Architecture=\" + str(archString)\n",
    "\n",
    "        rnnName = \"TMVA_\" + rnn_type\n",
    "        factory.BookMethod(dataloader, TMVA.Types.kDL, rnnName, rnnOptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1188f225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mTMVA_DNN\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"1|1|300\" [The Layout of the input]\n",
      "                         :     Layout: \"DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIER\" [Weight initialization strategy]\n",
      "                         :     RandomSeed: \"0\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         :     ValidationSize: \"20%\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "## Book TMVA  fully connected dense layer  models\n",
    "'''\n",
    "\n",
    "if (useTMVA_DNN):\n",
    "#    Method DL with Dense Layer\n",
    "    inputLayoutString = \"InputLayout=1|1|\" + str(ntime * ninput)\n",
    "\n",
    "    layoutString = \"Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR\"\n",
    "#   Training strategies.\n",
    "    trainingString1 = \"LearningRate=1e-3,Momentum=0.0,Repetitions=1,\"+\"ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,\"+\"WeightDecay=1e-4,Regularization=None,MaxEpochs=20\"+\"DropConfig=0.0+0.+0.+0.,Optimizer=ADAM\"\n",
    "    trainingStrategyString = \"TrainingStrategy=\"\n",
    "    trainingStrategyString += trainingString1 # + \"|\" + trainingString2\n",
    "\n",
    "      # General Options.\n",
    "    dnnOptions = \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"+\"WeightInitialization=XAVIER:RandomSeed=0\" \n",
    "\n",
    "    dnnOptions +=  \":\" + inputLayoutString\n",
    "    dnnOptions +=  \":\" + layoutString\n",
    "    dnnOptions +=  \":\" + trainingStrategyString\n",
    "    dnnOptions +=  \":\" + \"Architecture=\" + str(archString)\n",
    "\n",
    "\n",
    "    dnnName = \"TMVA_DNN\"\n",
    "    factory.BookMethod(dataloader, TMVA.Types.kDL, dnnName, dnnOptions)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5da87118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 10, 30)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 10, 10)            1640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 10, 10)           40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                6464      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,274\n",
      "Trainable params: 8,254\n",
      "Non-trainable params: 20\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 10, 30)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10, 10)            1640      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                6464      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,234\n",
      "Trainable params: 8,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Factory                  : Booking method: \u001b[1mPyKeras_LSTM\u001b[0m\n",
      "                         : \n",
      "                         : Setting up tf.keras\n",
      "                         : Using TensorFlow version 2\n",
      "                         : Use Keras version from TensorFlow : tf.keras\n",
      "                         : Applying GPU option:  gpu_options.allow_growth=True\n",
      "                         :  Loading Keras Model \n",
      "                         : Loaded model from file: model_LSTM.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info in <TMVA_RNN_Classification>: Building recurrent keras model using aLSTM layer\n",
      "2022-03-29 03:28:51.257561: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/neel/Root/install/lib:/opt/ros/noetic/lib\n",
      "2022-03-29 03:28:51.257585: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-03-29 03:28:52.851468: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/neel/Root/install/lib:/opt/ros/noetic/lib\n",
      "2022-03-29 03:28:52.851499: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-29 03:28:52.851520: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (neel-HP-Pavilion-15-Notebook-PC): /proc/driver/nvidia/version does not exist\n",
      "2022-03-29 03:28:52.851852: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/neel/jupyter/environment/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "Info in <TMVA_RNN_Classification>: Booking KerasLSTMmodel\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    ## Book Keras recurrent models\n",
    "\n",
    "     Book the different types of recurrent models in Keras  (SimpleRNN, LSTM or GRU)\n",
    "'''\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape, BatchNormalization\n",
    "\n",
    "\n",
    "if (useKeras):\n",
    "    for i in range(3):\n",
    "        if (use_rnn_type[i]):\n",
    "            modelName = \"model_\" + str(rnn_types[i]) + \".h5\"\n",
    "            trainedModelName = \"trained_model_\"+ str(rnn_types[i]) + \".h5\"\n",
    "\n",
    "            ROOT.Info(\"TMVA_RNN_Classification\", \"Building recurrent keras model using a\"+str(rnn_types[i])+\" layer\")\n",
    "            # create python script which can be executed\n",
    "            # create 2 conv2d layer + maxpool + dense\n",
    "        \n",
    "            \n",
    "            \n",
    "            model = Sequential()\n",
    "            model.add(Reshape((10, 30), input_shape = (10*30, )))\n",
    "            # add recurrent neural network depending on type / Use option to return the full output\n",
    "            if (rnn_types[i] == \"LSTM\"):\n",
    "               model.add(LSTM(units=10, return_sequences=True) )\n",
    "            elif (rnn_types[i] == \"GRU\"):\n",
    "               model.add(GRU(units=10, return_sequences=True) )\n",
    "            else:\n",
    "               model.add(SimpleRNN(units=10, return_sequences=True) )\n",
    "\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Flatten())# needed if returning the full time output sequen\n",
    "            model.add(Dense(64, activation = 'tanh')) \n",
    "            model.add(Dense(2, activation = 'sigmoid')) \n",
    "            model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), metrics = ['accuracy'])\n",
    "            \n",
    "            model.save(modelName)\n",
    "            model.summary()\n",
    "\n",
    "#             m.SaveSource(\"make_rnn_model.py\");\n",
    "#              execute\n",
    "            ROOT.gSystem.Exec(\"python make_rnn_model.py\")\n",
    "\n",
    "            if (ROOT.gSystem.AccessPathName(modelName)):\n",
    "               Warning(\"TMVA_RNN_Classification\", \"Error creating Keras recurrent model file - Skip using Keras\")\n",
    "               useKeras = False\n",
    "            else:\n",
    "               # book PyKeras method only if Keras model could be created\n",
    "               ROOT.Info(\"TMVA_RNN_Classification\", \"Booking Keras\" + str(rnn_types[i]) +  \"model\")\n",
    "               factory.BookMethod(dataloader, TMVA.Types.kPyKeras,\"PyKeras_\"+ str(rnn_types[i]),\"!H:!V:VarTransform=None:FilenameModel=\"+str(modelName)+\":tf.keras:\"+\"FilenameTrainedModel=\"+str(trainedModelName)+\":GpuOptions=allow_growth=True:\"+\"NumEpochs=\"+str(maxepochs)+\":BatchSize=\"+str(batchSize))\n",
    "                                                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da594215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 10, 30)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10, 10)            1640      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                6464      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,234\n",
      "Trainable params: 8,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "120/128 [===========================>..] - ETA: 0s - loss: 0.6347 - accuracy: 0.6295\n",
      "Epoch 1: val_loss improved from inf to 0.52288, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 5s 16ms/step - loss: 0.6286 - accuracy: 0.6362 - val_loss: 0.5229 - val_accuracy: 0.7475\n",
      "Epoch 2/20\n",
      "126/128 [============================>.] - ETA: 0s - loss: 0.4898 - accuracy: 0.7666\n",
      "Epoch 2: val_loss improved from 0.52288 to 0.46225, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.4897 - accuracy: 0.7666 - val_loss: 0.4622 - val_accuracy: 0.7844\n",
      "Epoch 3/20\n",
      "124/128 [============================>.] - ETA: 0s - loss: 0.4567 - accuracy: 0.7841\n",
      "Epoch 3: val_loss improved from 0.46225 to 0.44255, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.4549 - accuracy: 0.7849 - val_loss: 0.4425 - val_accuracy: 0.7994\n",
      "Epoch 4/20\n",
      "121/128 [===========================>..] - ETA: 0s - loss: 0.4317 - accuracy: 0.8012\n",
      "Epoch 4: val_loss improved from 0.44255 to 0.43065, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.4301 - accuracy: 0.8023 - val_loss: 0.4306 - val_accuracy: 0.7950\n",
      "Epoch 5/20\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.4221 - accuracy: 0.8067\n",
      "Epoch 5: val_loss improved from 0.43065 to 0.42082, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.4221 - accuracy: 0.8067 - val_loss: 0.4208 - val_accuracy: 0.8009\n",
      "Epoch 6/20\n",
      "125/128 [============================>.] - ETA: 0s - loss: 0.4114 - accuracy: 0.8147\n",
      "Epoch 6: val_loss improved from 0.42082 to 0.41552, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.4117 - accuracy: 0.8144 - val_loss: 0.4155 - val_accuracy: 0.8116\n",
      "Epoch 7/20\n",
      "125/128 [============================>.] - ETA: 0s - loss: 0.4051 - accuracy: 0.8146\n",
      "Epoch 7: val_loss improved from 0.41552 to 0.41080, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.4052 - accuracy: 0.8146 - val_loss: 0.4108 - val_accuracy: 0.8138\n",
      "Epoch 8/20\n",
      "122/128 [===========================>..] - ETA: 0s - loss: 0.4015 - accuracy: 0.8198\n",
      "Epoch 8: val_loss did not improve from 0.41080\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.4020 - accuracy: 0.8195 - val_loss: 0.4148 - val_accuracy: 0.8112\n",
      "Epoch 9/20\n",
      "120/128 [===========================>..] - ETA: 0s - loss: 0.4000 - accuracy: 0.8199\n",
      "Epoch 9: val_loss improved from 0.41080 to 0.40214, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3999 - accuracy: 0.8204 - val_loss: 0.4021 - val_accuracy: 0.8125\n",
      "Epoch 10/20\n",
      "123/128 [===========================>..] - ETA: 0s - loss: 0.3953 - accuracy: 0.8228\n",
      "Epoch 10: val_loss did not improve from 0.40214\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3930 - accuracy: 0.8241 - val_loss: 0.4095 - val_accuracy: 0.8138\n",
      "Epoch 11/20\n",
      "122/128 [===========================>..] - ETA: 0s - loss: 0.3879 - accuracy: 0.8261\n",
      "Epoch 11: val_loss improved from 0.40214 to 0.40184, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3898 - accuracy: 0.8261 - val_loss: 0.4018 - val_accuracy: 0.8238\n",
      "Epoch 12/20\n",
      "126/128 [============================>.] - ETA: 0s - loss: 0.3902 - accuracy: 0.8234\n",
      "Epoch 12: val_loss did not improve from 0.40184\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3908 - accuracy: 0.8227 - val_loss: 0.4186 - val_accuracy: 0.8069\n",
      "Epoch 13/20\n",
      "121/128 [===========================>..] - ETA: 0s - loss: 0.3907 - accuracy: 0.8247\n",
      "Epoch 13: val_loss improved from 0.40184 to 0.40086, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3896 - accuracy: 0.8252 - val_loss: 0.4009 - val_accuracy: 0.8159\n",
      "Epoch 14/20\n",
      "121/128 [===========================>..] - ETA: 0s - loss: 0.3866 - accuracy: 0.8280\n",
      "Epoch 14: val_loss improved from 0.40086 to 0.39708, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3862 - accuracy: 0.8277 - val_loss: 0.3971 - val_accuracy: 0.8209\n",
      "Epoch 15/20\n",
      "125/128 [============================>.] - ETA: 0s - loss: 0.3827 - accuracy: 0.8302\n",
      "Epoch 15: val_loss did not improve from 0.39708\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3828 - accuracy: 0.8299 - val_loss: 0.4024 - val_accuracy: 0.8084\n",
      "Epoch 16/20\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.3841 - accuracy: 0.8273\n",
      "Epoch 16: val_loss improved from 0.39708 to 0.39644, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3841 - accuracy: 0.8273 - val_loss: 0.3964 - val_accuracy: 0.8128\n",
      "Epoch 17/20\n",
      "126/128 [============================>.] - ETA: 0s - loss: 0.3868 - accuracy: 0.8261\n",
      "Epoch 17: val_loss improved from 0.39644 to 0.39467, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3857 - accuracy: 0.8267 - val_loss: 0.3947 - val_accuracy: 0.8122\n",
      "Epoch 18/20\n",
      "121/128 [===========================>..] - ETA: 0s - loss: 0.3833 - accuracy: 0.8289\n",
      "Epoch 18: val_loss did not improve from 0.39467\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3827 - accuracy: 0.8292 - val_loss: 0.4020 - val_accuracy: 0.8078\n",
      "Epoch 19/20\n",
      "127/128 [============================>.] - ETA: 0s - loss: 0.3837 - accuracy: 0.8261\n",
      "Epoch 19: val_loss did not improve from 0.39467\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3828 - accuracy: 0.8267 - val_loss: 0.3955 - val_accuracy: 0.8141\n",
      "Epoch 20/20\n",
      "122/128 [===========================>..] - ETA: 0s - loss: 0.3809 - accuracy: 0.8323\n",
      "Epoch 20: val_loss improved from 0.39467 to 0.39430, saving model to trained_model_LSTM.h5\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3816 - accuracy: 0.8309 - val_loss: 0.3943 - val_accuracy: 0.8147\n",
      "Factory                  : \u001b[1mTrain all methods\u001b[0m\n",
      "                         : Rebuilding Dataset dataset\n",
      "                         : Building event vectors for type 2 Signal\n",
      "                         : Dataset[dataset] :  create input formulas for tree sgn\n",
      "                         : Using variable vars_time0[0] from array expression vars_time0 of size 30\n",
      "                         : Using variable vars_time1[0] from array expression vars_time1 of size 30\n",
      "                         : Using variable vars_time2[0] from array expression vars_time2 of size 30\n",
      "                         : Using variable vars_time3[0] from array expression vars_time3 of size 30\n",
      "                         : Using variable vars_time4[0] from array expression vars_time4 of size 30\n",
      "                         : Using variable vars_time5[0] from array expression vars_time5 of size 30\n",
      "                         : Using variable vars_time6[0] from array expression vars_time6 of size 30\n",
      "                         : Using variable vars_time7[0] from array expression vars_time7 of size 30\n",
      "                         : Using variable vars_time8[0] from array expression vars_time8 of size 30\n",
      "                         : Using variable vars_time9[0] from array expression vars_time9 of size 30\n",
      "                         : Building event vectors for type 2 Background\n",
      "                         : Dataset[dataset] :  create input formulas for tree bkg\n",
      "                         : Using variable vars_time0[0] from array expression vars_time0 of size 30\n",
      "                         : Using variable vars_time1[0] from array expression vars_time1 of size 30\n",
      "                         : Using variable vars_time2[0] from array expression vars_time2 of size 30\n",
      "                         : Using variable vars_time3[0] from array expression vars_time3 of size 30\n",
      "                         : Using variable vars_time4[0] from array expression vars_time4 of size 30\n",
      "                         : Using variable vars_time5[0] from array expression vars_time5 of size 30\n",
      "                         : Using variable vars_time6[0] from array expression vars_time6 of size 30\n",
      "                         : Using variable vars_time7[0] from array expression vars_time7 of size 30\n",
      "                         : Using variable vars_time8[0] from array expression vars_time8 of size 30\n",
      "                         : Using variable vars_time9[0] from array expression vars_time9 of size 30\n",
      "DataSetFactory           : [dataset] : Number of events in input trees\n",
      "                         : \n",
      "                         : \n",
      "                         : Number of training and testing events\n",
      "                         : ---------------------------------------------------------------------------\n",
      "                         : Signal     -- training events            : 8000\n",
      "                         : Signal     -- testing events             : 2000\n",
      "                         : Signal     -- training and testing events: 10000\n",
      "                         : Background -- training events            : 8000\n",
      "                         : Background -- testing events             : 2000\n",
      "                         : Background -- training and testing events: 10000\n",
      "                         : \n",
      "Factory                  : Train method: TMVA_RNN for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU using MT,  nthreads = 4\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 4  Input = ( 10, 1, 30 )  Batch size = 100  Loss function = C\n",
      "\tLayer 0\t RECURRENT Layer: \t  (NInput = 30, NState = 10, NTime  = 10 )\tOutput = ( 100 , 10 , 10 )\n",
      "\tLayer 1\t RESHAPE Layer \t Input = ( 1 , 10 , 10 ) \tOutput = ( 1 , 100 , 100 ) \n",
      "\tLayer 2\t DENSE Layer: \t ( Input =   100 , Width =    64 ) \tOutput = (  1 ,   100 ,    64 ) \t Activation Function = Tanh\n",
      "\tLayer 3\t DENSE Layer: \t ( Input =    64 , Width =     1 ) \tOutput = (  1 ,   100 ,     1 ) \t Activation Function = Identity\n",
      "                         : Using 12800 events for training and 3200 for testing\n",
      "                         : Compute initial loss  on the validation data \n",
      "                         : Training phase 1 of 1:  Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.929443\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Val. Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :    Start epoch iteration ...\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |     0.681476    0.658081    0.487402   0.0536044     29506.9           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.630023    0.617435    0.486484   0.0529904     29527.5           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.592102    0.574997    0.489912   0.0535569     29333.9           0\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |     0.550848    0.549153    0.494719   0.0574719     29274.1           0\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |     0.520528    0.518078    0.492255   0.0539728     29204.9           0\n",
      "                         :          6 Minimum Test error found - save the configuration \n",
      "                         :          6 |      0.49195     0.49751    0.496419    0.054438     28960.5           0\n",
      "                         :          7 Minimum Test error found - save the configuration \n",
      "                         :          7 |     0.470456    0.491288    0.498119   0.0548785     28878.3           0\n",
      "                         :          8 Minimum Test error found - save the configuration \n",
      "                         :          8 |     0.457386    0.469806    0.502894   0.0569984     28706.3           0\n",
      "                         :          9 Minimum Test error found - save the configuration \n",
      "                         :          9 |     0.445034     0.45544     0.49658   0.0546186     28961.8           0\n",
      "                         :         10 |     0.435398    0.456206    0.497257   0.0541594     28887.6           1\n",
      "                         :         11 Minimum Test error found - save the configuration \n",
      "                         :         11 |     0.429634    0.450135     0.49822    0.054387     28839.6           0\n",
      "                         :         12 Minimum Test error found - save the configuration \n",
      "                         :         12 |     0.426046    0.435608    0.496874   0.0548666     28958.8           0\n",
      "                         :         13 |     0.421704    0.448529    0.499553   0.0545899     28766.4           1\n",
      "                         :         14 |     0.416236    0.438522    0.497198   0.0544696     28911.6           2\n",
      "                         :         15 Minimum Test error found - save the configuration \n",
      "                         :         15 |     0.414906    0.431005    0.498292   0.0547816     28860.7           0\n",
      "                         :         16 Minimum Test error found - save the configuration \n",
      "                         :         16 |     0.408304     0.43009    0.502822   0.0544643     28548.7           0\n",
      "                         :         17 Minimum Test error found - save the configuration \n",
      "                         :         17 |     0.410414    0.428649    0.495689    0.054532     29014.6           0\n",
      "                         :         18 |     0.403984    0.429797    0.499002   0.0552441     28844.6           1\n",
      "                         :         19 |      0.40319    0.438378    0.501748   0.0546862     28631.4           2\n",
      "                         :         20 Minimum Test error found - save the configuration \n",
      "                         :         20 |     0.400041    0.424189    0.499624   0.0555849     28826.3           0\n",
      "                         : \n",
      "                         : Elapsed time for training with 16000 events: 9.99 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 100\n",
      "                         : \n",
      "TMVA_RNN                 : [dataset] : Evaluation of TMVA_RNN on training sample (16000 events)\n",
      "                         : Elapsed time for evaluation of 16000 events: 0.29 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_RNN.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_RNN.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: TMVA_LSTM for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU using MT,  nthreads = 4\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 4  Input = ( 10, 1, 30 )  Batch size = 100  Loss function = C\n",
      "\tLayer 0\t LSTM Layer: \t  (NInput = 30, NState = 10, NTime  = 10 )\tOutput = ( 100 , 10 , 10 )\n",
      "\tLayer 1\t RESHAPE Layer \t Input = ( 1 , 10 , 10 ) \tOutput = ( 1 , 100 , 100 ) \n",
      "\tLayer 2\t DENSE Layer: \t ( Input =   100 , Width =    64 ) \tOutput = (  1 ,   100 ,    64 ) \t Activation Function = Tanh\n",
      "\tLayer 3\t DENSE Layer: \t ( Input =    64 , Width =     1 ) \tOutput = (  1 ,   100 ,     1 ) \t Activation Function = Identity\n",
      "                         : Using 12800 events for training and 3200 for testing\n",
      "                         : Compute initial loss  on the validation data \n",
      "                         : Training phase 1 of 1:  Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.709671\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Val. Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :    Start epoch iteration ...\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |     0.686673    0.668444     2.10867    0.149403     6533.05           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.630225    0.593114     2.44577    0.159555     5598.78           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.560831    0.539399     2.27926    0.144879     5997.05           0\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |     0.505407     0.49572     1.97307    0.146894     7009.18           0\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |     0.466613    0.468333      2.0776    0.147038     6630.18           0\n",
      "                         :          6 Minimum Test error found - save the configuration \n",
      "                         :          6 |     0.440583    0.449364     2.02981    0.148093     6802.28           0\n",
      "                         :          7 |     0.430692    0.465379     2.03297    0.150716     6800.35           1\n",
      "                         :          8 Minimum Test error found - save the configuration \n",
      "                         :          8 |     0.420473    0.435105     2.06838    0.144624     6653.65           0\n",
      "                         :          9 Minimum Test error found - save the configuration \n",
      "                         :          9 |     0.410565    0.425871     2.03954    0.145444     6757.84           0\n",
      "                         :         10 Minimum Test error found - save the configuration \n",
      "                         :         10 |     0.406882    0.419272     1.99183    0.142336      6920.8           0\n",
      "                         :         11 |     0.403718    0.422049     1.97514    0.147989     7005.44           1\n",
      "                         :         12 Minimum Test error found - save the configuration \n",
      "                         :         12 |     0.403093     0.41696     2.04481    0.143941     6733.76           0\n",
      "                         :         13 Minimum Test error found - save the configuration \n",
      "                         :         13 |     0.398086    0.412449     2.08533    0.145575     6598.78           0\n",
      "                         :         14 Minimum Test error found - save the configuration \n",
      "                         :         14 |     0.394568    0.407053     2.08527    0.145388     6598.35           0\n",
      "                         :         15 |     0.393418    0.407509     2.10546    0.144466     6527.32           1\n",
      "                         :         16 |     0.389071    0.418958     2.07923    0.155294     6653.04           2\n",
      "                         :         17 Minimum Test error found - save the configuration \n",
      "                         :         17 |     0.392416     0.40564     1.99421    0.140885     6906.49           0\n",
      "                         :         18 Minimum Test error found - save the configuration \n",
      "                         :         18 |     0.390365    0.403835     1.97982    0.139515     6955.38           0\n",
      "                         :         19 |      0.39019    0.408447     2.03659    0.139947     6748.75           1\n",
      "                         :         20 Minimum Test error found - save the configuration \n",
      "                         :         20 |     0.387259    0.401651     1.99189    0.138618     6906.71           0\n",
      "                         : \n",
      "                         : Elapsed time for training with 16000 events: 41.6 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 100\n",
      "                         : \n",
      "TMVA_LSTM                : [dataset] : Evaluation of TMVA_LSTM on training sample (16000 events)\n",
      "                         : Elapsed time for evaluation of 16000 events: 0.708 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: TMVA_GRU for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU using MT,  nthreads = 4\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 4  Input = ( 10, 1, 30 )  Batch size = 100  Loss function = C\n",
      "\tLayer 0\t GRU Layer: \t  (NInput = 30, NState = 10, NTime  = 10 )\tOutput = ( 100 , 10 , 10 )\n",
      "\tLayer 1\t RESHAPE Layer \t Input = ( 1 , 10 , 10 ) \tOutput = ( 1 , 100 , 100 ) \n",
      "\tLayer 2\t DENSE Layer: \t ( Input =   100 , Width =    64 ) \tOutput = (  1 ,   100 ,    64 ) \t Activation Function = Tanh\n",
      "\tLayer 3\t DENSE Layer: \t ( Input =    64 , Width =     1 ) \tOutput = (  1 ,   100 ,     1 ) \t Activation Function = Identity\n",
      "                         : Using 12800 events for training and 3200 for testing\n",
      "                         : Compute initial loss  on the validation data \n",
      "                         : Training phase 1 of 1:  Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.709404\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Val. Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :    Start epoch iteration ...\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |     0.683836    0.675681     0.90808   0.0825852     15505.9           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.661734    0.660563    0.917709   0.0824996     15325.5           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.633841    0.619115    0.919903    0.082499     15285.3           0\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |       0.6107    0.589355     0.93869   0.0807827       14920           0\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |     0.590247    0.577662     0.90637   0.0810015     15508.2           0\n",
      "                         :          6 Minimum Test error found - save the configuration \n",
      "                         :          6 |     0.563742    0.550545    0.913538   0.0819289     15391.8           0\n",
      "                         :          7 Minimum Test error found - save the configuration \n",
      "                         :          7 |     0.545018    0.544448     0.92704   0.0820899     15148.8           0\n",
      "                         :          8 Minimum Test error found - save the configuration \n",
      "                         :          8 |     0.529968    0.521708    0.999687   0.0837003       13974           0\n",
      "                         :          9 Minimum Test error found - save the configuration \n",
      "                         :          9 |     0.515032    0.514816    0.993993   0.0830508     14051.4           0\n",
      "                         :         10 Minimum Test error found - save the configuration \n",
      "                         :         10 |      0.49921    0.500196     1.06007   0.0806238     13068.6           0\n",
      "                         :         11 Minimum Test error found - save the configuration \n",
      "                         :         11 |     0.491648    0.492723     1.02951   0.0828517     13521.2           0\n",
      "                         :         12 Minimum Test error found - save the configuration \n",
      "                         :         12 |     0.480009    0.486195     1.13125   0.0941128     12341.7           0\n",
      "                         :         13 Minimum Test error found - save the configuration \n",
      "                         :         13 |     0.472961    0.483426     1.16965   0.0963026     11925.3           0\n",
      "                         :         14 Minimum Test error found - save the configuration \n",
      "                         :         14 |     0.470983    0.473198     1.03054   0.0805891     13474.3           0\n",
      "                         :         15 |      0.46673    0.478858     1.15442   0.0893036     12017.4           1\n",
      "                         :         16 Minimum Test error found - save the configuration \n",
      "                         :         16 |     0.458625    0.469393     1.15865   0.0843213     11914.5           0\n",
      "                         :         17 Minimum Test error found - save the configuration \n",
      "                         :         17 |     0.450367    0.458926     1.09084   0.0873036       12755           0\n",
      "                         :         18 |     0.447508    0.462523     1.04101   0.0804697     13325.9           1\n",
      "                         :         19 Minimum Test error found - save the configuration \n",
      "                         :         19 |     0.445449    0.453093     1.04221   0.0810616     13317.4           0\n",
      "                         :         20 Minimum Test error found - save the configuration \n",
      "                         :         20 |      0.43637    0.448977     1.03107   0.0801399     13460.4           0\n",
      "                         : \n",
      "                         : Elapsed time for training with 16000 events: 20.5 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 100\n",
      "                         : \n",
      "TMVA_GRU                 : [dataset] : Evaluation of TMVA_GRU on training sample (16000 events)\n",
      "                         : Elapsed time for evaluation of 16000 events: 0.396 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_GRU.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_GRU.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: TMVA_DNN for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU using MT,  nthreads = 4\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 4  Input = ( 1, 1, 300 )  Batch size = 256  Loss function = C\n",
      "\tLayer 0\t DENSE Layer: \t ( Input =   300 , Width =    64 ) \tOutput = (  1 ,   256 ,    64 ) \t Activation Function = Tanh\n",
      "\tLayer 1\t DENSE Layer: \t ( Input =    64 , Width =    64 ) \tOutput = (  1 ,   256 ,    64 ) \t Activation Function = Tanh\n",
      "\tLayer 2\t DENSE Layer: \t ( Input =    64 , Width =    64 ) \tOutput = (  1 ,   256 ,    64 ) \t Activation Function = Tanh\n",
      "\tLayer 3\t DENSE Layer: \t ( Input =    64 , Width =     1 ) \tOutput = (  1 ,   256 ,     1 ) \t Activation Function = Identity\n",
      "                         : Using 12800 events for training and 3200 for testing\n",
      "                         : Compute initial loss  on the validation data \n",
      "                         : Training phase 1 of 1:  Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.19269\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Val. Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :    Start epoch iteration ...\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |      0.70857    0.669143    0.247048     0.03101     59248.9           0\n",
      "                         :          2 |     0.671279    0.683392    0.291483   0.0327778     49477.1           1\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.655954    0.635499    0.258323   0.0309499     56295.1           0\n",
      "                         :          4 |     0.651812    0.693923    0.259216   0.0334019     56683.8           1\n",
      "                         :          5 |     0.654024    0.636483    0.250093    0.029947     58143.1           2\n",
      "                         :          6 Minimum Test error found - save the configuration \n",
      "                         :          6 |     0.636868    0.622612    0.253808    0.030383       57290           0\n",
      "                         :          7 Minimum Test error found - save the configuration \n",
      "                         :          7 |     0.627852    0.616865    0.253454   0.0305706       57429           0\n",
      "                         :          8 Minimum Test error found - save the configuration \n",
      "                         :          8 |     0.591318    0.616265    0.255418   0.0309096     57013.5           0\n",
      "                         :          9 Minimum Test error found - save the configuration \n",
      "                         :          9 |     0.613904    0.597259    0.254418   0.0306185       57194           0\n",
      "                         :         10 Minimum Test error found - save the configuration \n",
      "                         :         10 |     0.610529    0.574827     0.25279   0.0304292     57564.1           0\n",
      "                         :         11 |     0.602733    0.606436    0.251648   0.0298404     57707.6           1\n",
      "                         :         12 |     0.610898    0.629617    0.249606   0.0279523     57747.9           2\n",
      "                         :         13 |     0.587866    0.580357    0.283347   0.0337895     51290.8           3\n",
      "                         :         14 Minimum Test error found - save the configuration \n",
      "                         :         14 |      0.57069     0.56665     0.24999   0.0249798     56886.3           0\n",
      "                         :         15 Minimum Test error found - save the configuration \n",
      "                         :         15 |     0.558786    0.554431    0.253372   0.0332113     58139.3           0\n",
      "                         :         16 |     0.569673    0.592352    0.322448   0.0274838       43395           1\n",
      "                         :         17 |     0.554204    0.564776    0.265876   0.0244829     53025.6           2\n",
      "                         :         18 Minimum Test error found - save the configuration \n",
      "                         :         18 |     0.552893    0.532458    0.251847   0.0250945     56449.2           0\n",
      "                         :         19 |     0.547366    0.543558    0.246672   0.0247877     57687.7           1\n",
      "                         :         20 |     0.537028    0.546295    0.252595   0.0246529     56154.5           2\n",
      "                         : \n",
      "                         : Elapsed time for training with 16000 events: 5.23 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 256\n",
      "                         : \n",
      "TMVA_DNN                 : [dataset] : Evaluation of TMVA_DNN on training sample (16000 events)\n",
      "                         : Elapsed time for evaluation of 16000 events: 0.127 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: PyKeras_LSTM for Classification\n",
      "                         : \n",
      "                         : Split TMVA training data in 12800 training events and 3200 validation events\n",
      "                         : Training Model Summary\n",
      "                         : Option SaveBestOnly: Only model weights with smallest validation loss will be stored\n",
      "                         : Getting training history for item:0 name = 'loss'\n",
      "                         : Getting training history for item:1 name = 'accuracy'\n",
      "                         : Getting training history for item:2 name = 'val_loss'\n",
      "                         : Getting training history for item:3 name = 'val_accuracy'\n",
      "                         : Elapsed time for training with 16000 events: 26 sec         \n",
      "PyKeras_LSTM             : [dataset] : Evaluation of PyKeras_LSTM on training sample (16000 events)\n",
      "                         : Elapsed time for evaluation of 16000 events: 1.35 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "                         : Ranking input variables (method specific)...\n",
      "                         : No variable ranking supplied by classifier: TMVA_RNN\n",
      "                         : No variable ranking supplied by classifier: TMVA_LSTM\n",
      "                         : No variable ranking supplied by classifier: TMVA_GRU\n",
      "                         : No variable ranking supplied by classifier: TMVA_DNN\n",
      "                         : No variable ranking supplied by classifier: PyKeras_LSTM\n",
      "TH1.Print Name  = TrainingHistory_TMVA_RNN_trainingError, Entries= 0, Total sum= 9.40966\n",
      "TH1.Print Name  = TrainingHistory_TMVA_RNN_valError, Entries= 0, Total sum= 9.6429\n",
      "TH1.Print Name  = TrainingHistory_TMVA_LSTM_trainingError, Entries= 0, Total sum= 8.90113\n",
      "TH1.Print Name  = TrainingHistory_TMVA_LSTM_valError, Entries= 0, Total sum= 9.06455\n",
      "TH1.Print Name  = TrainingHistory_TMVA_GRU_trainingError, Entries= 0, Total sum= 10.454\n",
      "TH1.Print Name  = TrainingHistory_TMVA_GRU_valError, Entries= 0, Total sum= 10.4614\n",
      "TH1.Print Name  = TrainingHistory_TMVA_DNN_trainingError, Entries= 0, Total sum= 12.1142\n",
      "TH1.Print Name  = TrainingHistory_TMVA_DNN_valError, Entries= 0, Total sum= 12.0632\n",
      "TH1.Print Name  = TrainingHistory_PyKeras_LSTM_'accuracy', Entries= 0, Total sum= 16.1623\n",
      "TH1.Print Name  = TrainingHistory_PyKeras_LSTM_'loss', Entries= 0, Total sum= 8.29354\n",
      "TH1.Print Name  = TrainingHistory_PyKeras_LSTM_'val_accuracy', Entries= 0, Total sum= 16.1275\n",
      "TH1.Print Name  = TrainingHistory_PyKeras_LSTM_'val_loss', Entries= 0, Total sum= 8.33558\n",
      "Factory                  : === Destroy and recreate all methods via weight files for testing ===\n",
      "                         : \n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_RNN.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_GRU.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.weights.xml\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train all methods\n",
    "factory.TrainAllMethods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a088af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nthreads  = 4\n",
      "\n",
      "Factory                  : \u001b[1mTest all methods\u001b[0m\n",
      "Factory                  : Test method: TMVA_RNN for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TMVA_RNN                 : [dataset] : Evaluation of TMVA_RNN on testing sample (4000 events)\n",
      "                         : Elapsed time for evaluation of 4000 events: 0.0698 sec       \n",
      "Factory                  : Test method: TMVA_LSTM for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TMVA_LSTM                : [dataset] : Evaluation of TMVA_LSTM on testing sample (4000 events)\n",
      "                         : Elapsed time for evaluation of 4000 events: 0.156 sec       \n",
      "Factory                  : Test method: TMVA_GRU for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TMVA_GRU                 : [dataset] : Evaluation of TMVA_GRU on testing sample (4000 events)\n",
      "                         : Elapsed time for evaluation of 4000 events: 0.0852 sec       \n",
      "Factory                  : Test method: TMVA_DNN for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TMVA_DNN                 : [dataset] : Evaluation of TMVA_DNN on testing sample (4000 events)\n",
      "                         : Elapsed time for evaluation of 4000 events: 0.0215 sec       \n",
      "Factory                  : Test method: PyKeras_LSTM for Classification performance\n",
      "                         : \n",
      "                         : Setting up tf.keras\n",
      "                         : Using TensorFlow version 2\n",
      "                         : Use Keras version from TensorFlow : tf.keras\n",
      "                         : Applying GPU option:  gpu_options.allow_growth=True\n",
      "                         :  Loading Keras Model \n",
      "                         : Loaded model from file: trained_model_LSTM.h5\n",
      "PyKeras_LSTM             : [dataset] : Evaluation of PyKeras_LSTM on testing sample (4000 events)\n",
      "                         : Elapsed time for evaluation of 4000 events: 0.692 sec       \n"
     ]
    }
   ],
   "source": [
    "print(\"nthreads  = \"+ str(ROOT.GetThreadPoolSize()) + \"\\n\")\n",
    "\n",
    "# Evaluate all MVAs using the set of test events\n",
    "factory.TestAllMethods()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2aba5b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mEvaluate all methods\u001b[0m\n",
      "Factory                  : Evaluate classifier: TMVA_RNN\n",
      "                         : \n",
      "TMVA_RNN                 : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 300 , it is larger than 200\n",
      "Factory                  : Evaluate classifier: TMVA_LSTM\n",
      "                         : \n",
      "TMVA_LSTM                : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 300 , it is larger than 200\n",
      "Factory                  : Evaluate classifier: TMVA_GRU\n",
      "                         : \n",
      "TMVA_GRU                 : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 300 , it is larger than 200\n",
      "Factory                  : Evaluate classifier: TMVA_DNN\n",
      "                         : \n",
      "TMVA_DNN                 : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 300 , it is larger than 200\n",
      "Factory                  : Evaluate classifier: PyKeras_LSTM\n",
      "                         : \n",
      "PyKeras_LSTM             : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 300 , it is larger than 200\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       TMVA_LSTM      : 0.910\n",
      "                         : dataset       PyKeras_LSTM   : 0.907\n",
      "                         : dataset       TMVA_RNN       : 0.898\n",
      "                         : dataset       TMVA_GRU       : 0.878\n",
      "                         : dataset       TMVA_DNN       : 0.812\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              TMVA_LSTM      : 0.295 (0.322)       0.742 (0.732)      0.921 (0.919)\n",
      "                         : dataset              PyKeras_LSTM   : 0.342 (0.325)       0.718 (0.745)      0.908 (0.915)\n",
      "                         : dataset              TMVA_RNN       : 0.325 (0.284)       0.686 (0.711)      0.908 (0.902)\n",
      "                         : dataset              TMVA_GRU       : 0.208 (0.223)       0.655 (0.667)      0.870 (0.876)\n",
      "                         : dataset              TMVA_DNN       : 0.080 (0.095)       0.503 (0.520)      0.731 (0.763)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "Dataset:dataset          : Created tree 'TestTree' with 4000 events\n",
      "                         : \n",
      "Dataset:dataset          : Created tree 'TrainTree' with 16000 events\n",
      "                         : \n",
      "Factory                  : \u001b[1mThank you for using TMVA!\u001b[0m\n",
      "                         : \u001b[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and compare performance of all configured MVAs\n",
    "factory.EvaluateAllMethods()\n",
    "#  check method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe9d486f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArgAAAHYCAIAAAApvgy/AAAABmJLR0QAAAAAAAD5Q7t/AAAgAElEQVR4nO2dUdakuLFuJS+P6Lx0+QnwIO6yR1EegSE9gVM9Crc9Bxvy6XRPqrkPqlQpBRIECBDKvVetbpIfhPhQoshQKKTHcVQAAAAAc/zh6goAAABAvmAoAAAAQBAMBQAAAAiCoQAAAABBMBQAAAAgCIbCDei6rq5r7dB13fQwrXVd12dXzqHrOq31MAypSjN3bfcMw2D2mEtsuFxd19dKlDlGUg8jWqrHGrroQYXnxtFf0tDLITlpv+yQOyPkTeTZ9X0/PfKiao7jOFZVNa3VNtq2nTZRu6dtW3uM2V7J5RJljnmCIaqqOu6iSZpN/hzaAvu+P+gx9X3vPaCPemqARyFr7I+P2W9p0zTuwVVVuf3rrTG/VMxdu3vMS9D8Zqrruqoq0e+zqqrifSGoie3V971pV8/n85xfq7CNDd+IlQzD0DSN+/TNtZJfCPLkj1dXAGI8n0+l1DjxKwzDYLy1XdfZb29JbkBz4+4rz9ydu2eDP7wkiU7Djtc8Ho/H44GtkDOntXCawUeBoZAv8e9827aPx2PNe2EYBjO6v/hTY/FIc7mVpa2pmHrv+1NhZVlf+MrKrBFzw9WT4DaG5Jfuuu7xeOy5bhJZRI15eq20j2ZNaXsaeejcDWWuvPEkX8n1Il/1TQExJw1xgBwz4rj+GU0PtiVYpmUqpaqqmj0yXpSa+KhXDltOi3JHVaejJ+PEoRKJUZi6Q+MSzZ4SCv6I3/5s5W1R5iqhU0KRFrP1sWdZ3WafzmyBcUKVHAOtcXaoa1rbafXcS0ybjT1+dqd7IbdK5mPbtna/e5XIowlVw+Ddtf3oNZtpZECkJU/ldcs3RU2fY/yLMwZiFOLie7c/W/Ls6bNyrRR5jXqQFRgKWRP6vkUOth/tC8LELnjfTPcs8ydzWN/39shp4eYYt7T4G3+KV6tp0KIdETeFW5vAq+Q418W677JIOOSswt59uXfhvSVdidzDXE3corwbn31qIdHsXUfOWpR0PRFDYfqn6XVnW46tj9cObVFes4lbCZHGbLtJe6RnpUXqMK2GZbYJWTW85uqVFq/wLOodW2boKbttY2oohMT3WpR7uWn7mf32TeVaL/KsetgKOYOhkDWehW6+gaGDZ99ood9k3mGzLw7vJbX482WNoTB9d8xWdfo+nZoF3h7vR7ZbJXuMV+zsKdOds+93r+TID25b1PSARafR7AHezqkys9dag9sfWEK9y6xVMd0ZaYfuKZ7RM+vUmTVT1MQUC/2kju8UGQrxBx2/6/hzscfMKhDfOf1KRs6KVH564rSNhcw7t5CQyPH2ABnCs8kd9/ery9RicL9ss1/+0fmizp5l8d4LxvBffIcuGgqzBsc496YIvYsjhsLsO9G74pp7Dx0WLzn0W9wtanpM5Bd85Orejc8W0vf9Gi+Ux2xLCzU5Y0Z4Jayx3rybss0mZCWEOpKVPpuQyN5+kaEwVXv6oNd8+6bMKhCS0WuE3sdZC3Ja2myVvGMWDQWRyKH2MEKu8Gxug+mtIz7MeJ9kC5l98XmHhV4xLtO33qKhECl2sVYrDYVIhb1jIr9jvBuZPSxugswS+sEXP2t6495Z1u0Uf15rsO/x/p2V/mEradyImb1oO/F4W0LNxnuCoQcaEnnW1FtpKMQPCxW15qfz7AERGacKLN7R6ERChGx3j0VDIXRfa4zayOmQCeRRuA11XXddNwzD6PQN8ZR2aWOJbWJEg5nBKC1BKfV4PKbp//bXTTnj0yKmldlwa2swj8MWvrLOZh6anXEwPavrOvPRCrszkWI9oeu6cRzVXCoFN22ol9jDLXDxou6UCu8SoXuZLXZbG5ASv6Pp5N41Zy0i/eKYajRNM3vWQY3cw9yy9wSZ43A7mB6ZL5GpSubtHHovH1ET91pVVZn+Y1sFznmVr+fM+lRVZfpamwBj/Xx0Y6jNnmUmDXZdZ97+z+ezaZqqqtLOqm/fZ+S6EyZtkxiGwe3yRb3ROI6mpZ2ZreGc/jIJ2xpq/Cwyi8AaMBTyxXTDfd/P2gp2p+k/ZkuI/GlDTfZ3PHVdP59P29slxPuxvp7kvWmcruuapjFXDP3unGJ7aCPg7FmuF8F04dYiSVf9N4xB0LatewlPTGMYrSnN+MDrujanuLdj79ojyYM7wkw0tzD99u2ssPl5IK1G/Ky6rkPpMZKAIVIGDD3ki3mFLb7oI2bE9Fu6p9vwStvwCgjVSr0GVrZUK8owDKFleCK2xZp8PqHLeXs8/7C9qLnZlb2UHX2YHa2Y9gRd15nBqbSvadfFZesff2ruwd7+0JJC1orysvFMD155d6HvkU0EHjl3j4CbKzwlJKPrZBJVw54V+hZEvjjSaynGGgrg4hgJCDMNDXOZxg97D9R8nJ0ZNT3MK3xNnOCGYMZQraYhTtMrLgYzzgZUeyXPVjgewT5bmTEQYT5b59kqzeoQIXLW7M7ppWcnKcxeZfYwe4O9M48xFL4enx45LoX+TcVc05jXz6mZ3TkbMjlt5LONYQzEyS5+++LlTPfHvzizkyCmpXlFzT50b+diMONs+w9Nj1yMGIXc4Nlkjds9tA6zb5zZF5+aJDZZ8+IL9cE2DN4tynvjxDs/r1Z9NL9TpEqze8xZVThzVOhC9hSvO4xIFJrCMBU8NNlP9GZcnBTgSjq9dKjr8lj0cEzVnl5UTQyUWVm81jXbp3od1bQQV5BIAH/o0bi349rlXstUQkPBVTL+7VssZ1FG9+CpAqFquMfYG7df8OkXx9qdkS/7GpExFG4KzyZ3vFeMxf3SGqZfNu/c6pWqOeKHcE+c9grey86+UPpAsrZZPDvDLWHxXuKGwmw9413+tsqMgW5p2tHOqmH+tPj7fv1Zsx28e+n9hsK0vU2lqwK5h6dteNqZhQqf/mZ1b9C9UMRQmL21qZKzj8/stMd4txbZP1ta6PTF8mcV8G52VoHFs8YV3wL3gDacwnlRZAyFm6LHuXcr5IYJa7cfRcP5djDSBIhtjoe3dXCH8KWh+/HSkrCh5ISVWVOUiVpI/tWzTyG5pIsXda9oFZidmjHdL8IUsq0xDyuWNHO/aEmCZmwhqUJwIg0sIsiaZhk/ZuXjO+57DVdytaUCh2B+CU1/M83+CoczWfOzEjxozGuI+1QANoNHoVjMz9bemV05vNIh8NAvxDyX9n1WIcShMa9hj78QIALTI4vF/N5qmsb4AG3ivFDQAxyNO1USK0GE15jrV7IvGrPFaoLDH9JzsUcDjsSL3J7Go8GZ2KdwdUVuCY05gjtl4+q6QIEw9AAAAABBGHoAAACAIBgKAAAAEARDAQAAAIJgKAAAAEAQDAUAAAAIgqEAAAAAQTAUAAAAIAiGAgAAAATBUAAAAIAgGAoAAAAQBEMBAAAAgmAoAAAAQBAMBQAAAAiCoQAAAABBMBQAAAAgCIYCAAAABMFQAAAAgCAYCgAAABDkj/uLGIZhGAalVF3XdV3vL3APWutrKwAAAB/OOI5XVyElevP9dF33eDxm/9S2bdd12yu1A6233xEAAMBOyuuGtgw9DMOgtR6Goe/7cULf9+aAq2wFAAAASMVGQ8FYA7MDDXVdD8NQgD3FKIYUFBOBXCKQSwRySUGxCKV5SMrz+QAAwI0orxvaNetBa80QAwAAQMHsMhTGcWzb9vF4aK3NiEOiWmUBnigpKCYCuUQglwjkkoJiEfbmUei6zgQwKqWapinJYijMd3QCKCYCuUQglwjkkoJiEZINpZhsCu6Eyb7vz0+rUN7gEAAA3IjyuqG9HoVhGLqu01o3TTMMQ9u2ZpJk27ZN0ySp4lXgiZKCYiKQSwRyiUAuKSgWYZfhY5WdzbCktT7fqVCeKQcAADeivG5oVwrnuB1QmFIAAAAfyK6hh9mJkcUEM+KJkoJiIpBLBHKJQC4pKBZho0fBmAjP59NbCGoYhufzmaJigpoclMgBj4gUFBOBXCKQSwRySUGxCBsNBddn4PkP2rY9LS7BzLPIYdVKAACAItllKFw4ymBmWxzqvSgvIOVoUEwEcolALhHIJQXFIuyKUbg2FqGu67ZtjyufRiMFxUQglwjkEoFcUlAswhYbSmtt5kPWdT37m/5Mxb1JmEmtwpvHtqysPl8PAIB0lOec2DL0YKMQuq7LcILDtuBV81ztA9Za3/5Br61/DvbQ+K48G2z4G+Z7fXk17rJhtLq8GjfaSKtYYdz+rvSBHoVdbDBXFiuuV3fqWUhwBWtkz6SFAECR5NMNpWJvCue6rvXr3Wy2M/QxXMI4zv+LoPWPf/NlBkv1/6l1/7RSB/2THHqG7O+PIMH9AQB8CLsyM5oYBbN0pFJqGIa6rpumKcOYOsgqnC1yahnM2grJqzMmcj1MXR1aWPCYpPM9r91tru6PKpb3s+NQkEsEcklBsQi7PArGSvASLqmrZ0Ok4sxGs8HlsP7fGfX3/Rli9Cj+N+cr2P7vu1aHuxJ+lCX0bXw6vMRFIJcUFIuwy6MAR7DS5bAe79wTvg5SW2F97MXiWZt9JJHXhHYVDI4KLV5AXCWvFgeVCwAQZ5ezxQw9uCVM95xMQvdRAZ6oI9wJEUlOU2ybYTEl1ciLQb/LnazoBAXduxkbCvg+nglySaHviLDLo2CCErz3ow1ZuDsFPGnvDpLYDdFCRvvXQ8Wb7eA3WA/bDI6QeTFtMHq14jG1ImWsFXllNbJu8AV8H88EuaSgWIQEhs8wDDaj8+VrLpRnyuXDQeEOlzyuVD6JReJOi/WWxHuZRxx6YBEAH0V53VBx94P76DpeiXF2FZKz5HvMi6nFsLmBTc0Liekwe+y2+zr1UfF9FIFcUug7Imy5H611VVXDMIR+D5URowBHc8KMjGvbwgbDYk/YhPt93Hvf34UT1Z/vHYBSJXZDW2IU7JTIYsIR4BKOCKHwiJd59HfZ9vrrLQbvSJHdEA+SEE5EiefxmL2d+D0W9d4E+Ci2GAomhlG91npOW6F8KM8qPJqdiiUXe9HyOCerlQpFX77LNWtMRCyMNTaE9zjWhETEQyq80t2/LRVsD9ioL99HEcglBcUibBx6MOtCNU0z61S4MKSRhw1rWOm9uNHIxeYxi5UBleLSF9wPG0sFyJ/yuqEt99N13ePxiBxAjALckUiPmVWb2hxTmWoKxg6vUeLyADKkvG5o1/14KzfmAJGrF1KSYicEN6SSa+dUz/WuiJS+B7nFUFLrOgHkkkLfEaG4+ynuCcG1rA+xzLndSRYoF9yGKA/ETLnHJ74GOJ/yuqEtwYxd15ncSqFIxoIjHOHTWD8147TQyA2sz2Xp7lw0GtYukPG95PeztF4R4Lg3BBIA9rNx1oNSqq7rPFeJTJXdoTyr8Gg+QTHpkl2RiQKXy+XZATNrhQf8ENIJF95X0nycv/cfOcAndZnWAsJc3rpuB4pFKE0aHjZkwqJXPvN2ujhasWGexawRP/OF9c2rpYoA5ER53dDe+zGjDOa/NrnC7lptp7wnBGVwd7vBsCbWYdGAiEQ2BL+87ik7oiUBTqC8bugPe06u69qdJ2nWmN62zk2GFHMjp4FiEcbxx79ZtP7+L2fsTUSOWXZFvJg598X0HDNcocZRKfsvWouZfx8EX0YpKBYh/fTIa+dMlmfKQfHkHAUpIm4ibMviIPg6mxLWHn5DfeEmlNcNbQlmdMkqiQLAHbGvFLev/N7r3eptM6oxYivE01HbF+ts5KNaYzFEoiNn53wE/gAAHruGHtRkJqSdELGz2BzAEyUFxUTMrBY9GZi4naLj2xjLuGaoQnlzMgMDE/qd1RVyRitmxh9eoxJ68u/m8GWUgmIRdnlIhmFomkYpVVWVmS35fD7btr0wj0J5Ph/4TO6ST3ona0Yr1maElOkSmdLqlyspFqDAbijB/XRdZ2c6mFxMOwvcQ3lPCD6ZMuZKxNmQNXLVMphrpZkUtbBc9v0Vh4Mprxsq7n7I130dKCZig1wf4mZQwnmYguWsFmSSGA15K86XUQp9R4QEMQpmyLDrusvdCWkp7EmfAIqJ2CCXN/LvUsTA+g/WRDnYgILxnUixSyEOk3O/hzfMzWrNO6yBL6MUFIuwa9aDWW+6bVsz9GCNBhQHOAG7ZoLlLS9RWd/CSMJp3/0wzh2/djKF/egeP5mCMmsWFKw+fDa7PAqPx6Pve9eRYL51ea4BIYUgWCkoJiKVXKEkTrn+1t2IJ9eavE/2n1oxmWJSwDSn07sdEHHvqOs9PHwZpaBYBPIoBMEvIgXFRKSVKz6vsoDghlm51q+KOS1n3VJV4/cibdlv+9/KtQW5hb796UT4MkpBsQjkUQAokHi6aJfCHA/qPb7B7HFdCz8Om/MxBBwMs96FQIroUPxISRLDh7HLo9D3fdM0dqDBrPXQtm2CemUAwRZSUEzEOXJFrjDttrTT2eVGErm+D0O89/qzPgZ/pGMcJ96FueKVeoVITOJHXuW6ha6vuRS+jFJQLMLeRaH6vldKPZ/P5/OplDIhC0lqdjk0GikoJuJyuRbnUGT1G3ibXLOTJmYdDOrlY5gtx/E0jHP/3ou3/9bEMRyj8uWt63agWITSbCisQoA9FBDNEGHDslWzMW6Bl8xs4eEVKOYrcX+VP57yuqG9MQrDMNR1bc3tHNwJOsCGco6oXsGgmIg85Vr5A/j8uieRK772xIZQhvd3y6yPYW6R6+NVzrN15QyKRdhlKHRdZ9Z6aNu27/u2bR+Px+WRjGOADeUcUb2CQTER+csV6c6UOttiSDxJJJrNafPAxLvRMCk1bjQEit6mcv6tKzdQLMIuD4nW2lsCyiwTdaHi5fl8ADJETzq7MgiNTQSdEIEu/P0ttHpI4r3oaaELp0AelNcN7TUUZhOY9H1/lV+BfN0XgmIiCpDrzL7sTLmk5sL3s8K/+5fshuh9zRa7JEUBretk6DsiJM6jYLh89CEJhT3pE0AxEQXIdeZ0iTPlCo1KzA5J/DhLNjbxXnCsNoFQhigFtK6TQbEIafIoGHNhGIbH41FVlZtZYW8FASBvIikDCnj3GlthdmmJWQfDNOzROyCQSWGa4skv1z//urSP8GnsHXqIH3D+GATuowtBMREFy3VEVqEc5Jp1J8THI/wSJu/M8D1JUmXNDQFfLte9oO+IUNz9FPeEAO5IqdGOBn8KpcRWeCtnrd0QnhOx4mQ4mfK6oWT3k8kqD+U9IYCb8glh+67FsNlc+F7Uj5WvZ/++zlYInw+nUV43tDGYses6rbWNRdBaN03TNI278+6Qf0MKiokoXq5QqOM2PkCu77GQgYDQwO3PqvwBciUHxSJsMRS6rns8Hm3bGv+B+W/f9+M4VlVlUjAVQGEm4QmgmIgPkSuSTEhYTo5yLa5RKS7Qj4V8+xQ57U3l4n7RngCKRdjSnrw8S9OPJFwCgCmlDqnPpHHcNwyh3n/gbhyMKEbfu1FeN7R96MFszIYmlDH6gCdKCoqJQC6RADnLNU26sN+78J5+fv0aVO7fc1r9M3tybmCXk2BRKJVBDOMRFGYSngCKifhAuabLGqzPznQLuVYuG7GxbDXOBS7MLR7xdgj93ypu0cCuYq9HwWRYsvsLthsAIBXyTIO3YUNKR0Hh4ziO45xW7ytOTc0xgB1sMRTsKpHGV+MOQzRN49oNtwZPlBQUE/Hhcs16FyLcTq5ZcyFNyeMY9cS8Zj3wE1nC7RrYmWyc9dD3vdm2cx/MktNu/ua7gydKCoqJQC7DShluKtc0diFt+aG5lN/lchM/E7IQ5aYN7BxKC84sL9wU4BM4IutzViRMzfSjzIXEjoFFOCbHQVrK64a2eBRW+gyuci3oABvKOaJ6BYNiIpDLZRrh6HF3uaZzIhKU+WNaxKvkaajjdIxnchyo+zewQ9k49KC1nl1g2jAMQ13XV2VeGgNsKOeI6hUMiolALo94GscC5JqdQpmm5KCt8HYQEY4RCmhgx7FlmelhGMzS0lrrqqrcOQ7DMDyfT6VU27bFBCsAwDlMV6wuLyZvVOP6RasFxb7ZClr9MLzstV5RC95C1YqRCFhg71BK13XGIHg+n8ZoMCSp3AZYKvRCUEwEcsXxohbKk2v/otXBkoPrSzmxjW+7ixJ2G/QdEYq7n+KeEMDHUnyEoyXVutV+sW+uhVfZ70e8dhet77mU1w3tzcwIAHAQ23I43pHjMi6oldEIBYsLu8FQCEIQrBQUE4Fcayg4h6NH2owLXuuKRTh6x5Wq7xJ8HyOU5iEpz+cDAIYPGYlInnFheQBCfYy4p1BeN4RHAQDuwYdM7kueyXGu05oU606eLFhc2ASGQhA8UVJQTARyiZj+LHazFxcWwbDfVpi2rpc+tuSFpTVK03QJvo8RdhkKwzAkyYGYJ4X5jk4AxUQglwgr1zTNoEsxXds0NZPIYgi1rvf385xfYe6c9de9L3wfI2xJuGQxuRftulAAAOcQD3LUev6Ye+GlZlI7sjON42hNhPccVvp7ec6hzvU+wkSARXbFXGit+77PykogacaFoJgI5BKxUq5p11aGxtLsTLNyue6EYC6myTnOIUVIGYC+I8JeQyG5HGYJiXh6R5MOsq7r6XoT5T0hAJBSqrmgUsyJCJsLS+MOxYh4MOV1Q7tiFNK6E0zEg1lIomma0KJTWuvH46GUejwe5vhUFQCAMpiumFiME91Nt7AhdkG9ls37XsJ0tcnJ0W+zIYrRESTsMnzqujZLQHlsK9MsMWU6/q7rHo/HtBxvv3uK3YP76CpQTARyidgsV5E/iRdHItbLpbWW+RVUQTo60HdE2BXMGFlpemeBxiAw4wuR46uqSlsBl8Ke9AmgmAjkErFZLne5xGJwnQrzB6yWy8Y5xs7wlvUsb01Pvo9RdhkKaccdpgVODQVjQHRdV9e1WdK6bdtUdQCAIrG2QhmzIVyMxbAzL5M7J0IpHQxsnC5RXZKUEGHcR9/37s/6tm03l+NVRilVVdX0SNcymB6wUwf1PoDn7WEjvoFibBy3Efqerj599t/195Vkw70nVyt5gfZf+BhPwavvPZmGGxWb2bAfi2FXMGPXdTaVguHxeCR0M0yLGobh8Xj0fT+OY9/3z+dzesw2Iey5biHeHjbiGyjGxnEboe/p6tPVa4fL9feVZMPLzrTzy/i9GPUjduHtmHcpR/Xdu3C5CDs39igW0LAcdg09PB6Ptm3dSIW6ro3pcBBN09j8TnVd931/6OUAoCTsa7y80XY3O5PZ2DR5cpxMfJhkZPp+4HvoRzE6whx713rw4hlNF75hyuLsidemciomF/VpoJgI5BKRVq4iOzXPMtgauDDOzYKcK2osagYq38cIew2FWZtgWwdfVZV1D9i0S/ajuZAZ3bCnJJ924VKqE+k4UEwEcolILtfUu1AAbpYFtdVWMH74ibkQKKqUZsz3McKuoYe2bd2xABNAsHnKol1iynw04Y22WHMJYzG4pp89DABgG4XlWkgyDGE6TjfLQjA9wFjurBJQSu1MuKRe8xXtRy/90QZm50muP4ykGReCYiKQS8RxchWZTEhrPRNXsDXls+t9CT6Fm1tb9B0Riruf4p4QAJzDzXu6ebzRh222wveSzP/KtRVSUV43tGvoQSWNUQAAuJBxkk9I3b+/8zIy7ZgQ8SrQlGfKnv6tpIgPeLHFULArLITCRMswpsqzCo8GxUQgl4hz5Jr2dDed9+fJ5UYtKJNrQWwrjN9PXTjKiVe4lXB8HyOUJg0PGwCSUORa1UlWqfbyLU2P8PcUIJyE8rqhvZkZZ3ey9DMA3J3pq74At3qSyZPOp0B+BY8ChPtsNsYoGFPAzHdwIxLcqYx3pzyr8GhQTARyibhErvtmcozI5U2e3OBXeBdhLl7hhsLxfYyweYn3oIW4f4bkHnjYAHAE5c2itOZCijGIV0lzx73+eH/J1lFeN7TrfjKUI8MqAUAxFGYu7JwE4dkKwZmTH2YrlNcN7YpR8LQoLDSB1N9SUEwEconIRC7v/T+3LEIWiOTauiqE6QJGZxHOpevmKZZSKpsGlie7DAUv6XLXdVrrYsyFwkzCE0AxEcglIh+5xsmK1RmaC1K5NtsKk+sqs0T1W5xjNs8uQj4NLEN2GQpN01RVZfUdhsGs/pCiYgAA+TJrLtyOPZmX/JJmmLMVMrSqYIm9MQp933tzHGZ3nkbEfSQ2sYsbZzoaFBOBXCJylivDxAEiudKlef7+Bn6/8twynZcLNIG1HiLsTeGcIameUGFP+gRQTARyichZrtl8jvZPl3CVXOM4muHomfmTZleu7oScG9jl7Bp6qKqqaRoblDAMg3EklJFHAQBgJWYk4qbjEbbuCctcuPFb6AIv9npI6rp+Pp/unmvtMtxHF4JiIpBLxO3kutapsE2u/Qme3Qp8L+c1JcL+4cdBOT1Q+o4Iae7HOBVycCSU94QA4KZcPgCxgZ1ZmH6U4xsKr1JVvrZCKsrrhnYNPRiGYShmSiQAQHJu5GhPNQAxjmOwsyyrE/0EdgUzDsPgTYZsmqZt29nFom5HeVbh0aCYCOQScUe53CBHs3HaHeyXa9syEB4mtlEpG8j4qlV+q1HfsYGdRpo8Cm3bKqXqum7b1qwUVQA0GikoJgK5RNxUrqtSMyWRSyu9PxGTW5Pl1I3XcdMGdg57hx68QQfjS2AkAgDAcLupEJ4jYb+54NkK/lbmckByQ6EksrV8swXFRCCXiLvL5ZkLR9/NTrmmsyV3uxZy/71+9wZ2KAnyKLh7SsqjgCdKCoqJQC4RZch12k0kkcszF3a7FmxRk9kgGXTSZTSwg0ifR+HC/M2KgBQAyJ7brVWdLsezW877VMn8VVhNed1Qgvux0yPrur7cl0DSjAtBMRHIJaIwuY5OJXCEXIlSLARshasfLn1HhOLup7gnBAClcruf0/tTN2qt3dBGs+v16SYqLFFeN7QlRkFrbTwHOsrl3gUAAEiIF7KwpYRxDAYksAJ1rmwxfOziT/EpD03TnB+vgPvoQlBMBHKJKFWug/zuh8qVYkkIrb4nW9iNL5wAACAASURBVJpbUvKKB03fEeHA++m67vwUjeU9IQAomPv63feFLFjLYG7O6O20eKe8bihBHoW6rrXWXdcNw+BaBmUkcgYAgCnWPtgzZ1IfHdUJKdhlKHRdZ/IoVFVl9jwej8tDE0IxExvKOaJ6BYNiIpBLRKlyHdQ5niyX3FZ4LT49m9T5imddagNLwi5D4fF4tG1rQxbquu773kurcD5jgA3lHFG9gkExEcgloni50kbynSNXknRMmdgKxTewPewdevDGF9YEOQIAgOXM1M7J2ZHpOboABOTEIWs9XD76kAQ8UVJQTARyiShbruS2wplyTReGEJz6Qk8jGU8ePSm6ge1kl6HQtm3TNCaM0aC1tvEKdwdPlBQUE4FcIoqXK+39nS/X1tjGuXpe8ayLb2B72DuLo+u6x+NhP1ZVde24Q3nzUgDgc7j1DMGtEybDUyXvqEKJ3VBx90PSjOtAMRHIJeJD5EplKFwl1yZbwfNAXLMABH1HhF1DD1rrguMWC3vSJ4BiIpBLxIfI5Q7Q7xk0v1yubVGN30/9sXlipMXViuXMLkPh8oEGAICCuV2A3bbFICZW0TW2AoTY5SEZhqFpmqqqvGkOF+ZkxH10ISgmArlEfJpcO8cgrpVrwwCEmXTgV9kUc8qN0HdE2HU/dV3Pple6soEW94QA4DP5qMBGOzvx7WZ/hDneSYLyuqHi7qe4JwQAH8t9bYUdToXxewEKQyEX9iZcKhjyb0hBMRHIJQK5RFwuV5IlozamcdrE5YrlDIZCkMJMwhNAMRHIJeIz5fImQazvy7KSa5et8L2IpCthzJGVYrmBoQAAAOnZOgPiewSjLQUuB0MhCJ4oKSgmArlEfKxc4/j9n2GlDJnItc1WmJRy0qyHE65yU7bEXCzmTrhwUajyokgAANTERLjRe259YON7PKP6Ec14q6jG8rqhLfezaHkxPRIAIDnTV+9d3nbGVsBQuClbhh7GF33fK6XatvU+Jq6jEB1gQzlHVK9gUEwEcolALqXexiAMNsjRi/a7tVxO5SeRCofd160VO5pdho/Wuu97b6Dh4oxgxZlyAABTRP3a5S9FkUdB+W7pV3jjTZaULK8b2hvMOBuOwAIQAACHYoMcvWjHWaSzKw/C1CJygO1ft7mB4SASGwrGRLgwmDEhNFMpKCYCuUQg1yLvRoOetR7yV9H9Lf7+0I+tOg0swh/3nNz3fdM0WmsTlzAMw/P5vDxGIRWF+Y5OAMVEIJcI5BJh5ZrOq7wkLbQZdFg5Q3IcR9tta63PqScNLMLeoZRhGLquM0tDVVXVdd217oTyBocAAFIx+7P5tFemaAGI93iFOy39UF43VNz9sFTodaCYCOQSgVwi4nJdZS7sXVLSrXbq6tJ3RNh7P13XTUMXLwxmLO8JAQAcxPmJGdzRB8kkiB/n/yDXV3153dCuGAXzFKuqKiN6EQDgozDd2XsChoz6XydY4TX6MDq2QlZ1LZpdhoJSappHoRjKswqPBsVEIJcI5BIhksszFw6NdnSjGrXS68cgflRmat2kgAYWYW/CpdyUzbBKAAC34LTYhfUDEDMhjerdUMjvhV9eN7Qrj0LbtqW6EwAAPo3Z3E1H5BdY6UhQ7ymYbI3SVwii7B16eD6fWuuqqtydZWRmLM8qPBoUE4FcIpBLxE65vOwLR6ROHtW4feFppdU4pjVhaGAR9hoKnomwn67rlFJ1XUd8FcMw2BSQx7k0aDRSUEwEcolALhFHyHVQ7OBipIINaXyvQGK/Ag0sQkY21DAMTdMYy8NkeDRGg0fXdY/Hwx7mRVNiFQIApOWgeIAN+ZecSIV88y+V1w3tup/QEMO2X/lmCMOUaayBad2MMWGNg7qun8+nlxucpBlXgWIikEsEcolILpfr5k9Y8Hpb4ehEjfQdEfbOepjdv61Mb9Hq2TWsQwaEW0hhTwgA4HK8l32qt6wXphAxF+amP9hpEHm988vrhnbFKEy1mE3UuIbZZSeHYZjuMV6HE2IUAADAMJtrYX9vuD6k0YlUOGmZKPjBmJptZfZ9752olKqqalq4oaoqE6bQtu3sAdt0cDfUyxKa/okNFGPj5I3Q95SNk7+MSo3v/xKV/KO8hft63d3rX6I7TaiY/VgMu/IohEg1PTLkLRjH0TgV2rZ9PB7Tv27AnusW4u1hI76BYmwctxH6nrJx8pdx9BMtpCn5R3HHn3WCYoWxa+hhahDYyY17io3gzcas63pqKAAAwKGMY+JhCDsGsTqv80jmpdPYZSg0TTPd2bbthqKMbeEFJUwNjrquT8vmVF5AytGgmAjkEoFcIk6Qa3zPeHTZCk1jmmvTwCJkJI0719Gb3dB1nYlbnE6PVO+ODR42AMCZJMyyYJwKd5/7UF43tDczo3r108YZsGfQYRgGrbVtBya80ex/PB7WMmjb1vVkFPY8AADuxZg4mfIqXp3xZO1pOIC9ho9xA7h7di48PTtPcv1hJM24EBQTgVwikEvEyXKlciqsyb8UdCrsuzZ9R4Rd92OsBNcymKZKPJnynhAAQOakytu4cvlpJ52z+pGlMZs3f3nd0N7MjFP/wezO0yjvCQEA5E8BToVUlNcN7c2jUHBixFCCagiBYiKQSwRyiThfLtsz7ryytQ/Ei1CPak+oAg0sQmJD4eg8CmdSmEl4AigmArlEIJeIW8u1mEfh/e7S3OmtFTuaXbMe+r5vmsas+qiUMlGN2/IoAADArbHTH07LqfBy8o9K60QGA8yQYCjFLgRV17XxKFwIkasXgmIikEsEcom4UK4kwQqLkQrvYQrKMRQ2XpW+I8Ku++m67nLLwKO8JwQAcC/22wprpj+8zX3YbSgkpLxuKP2sh2sp7wkBANyLJLMlVzoVMBROYFcwo5cksTAIgpWCYiKQSwRyibhWriS95MrpD5M73XjjNLAICVaPnOpbhjFVxl2cCYqJQC4RyCXicrnSBjYuLynpJlTYxOWK5cwuQ8GGMQIAAKTFrj09/9dxfPuZqnMYdiiT0oZSIu4j6Z2WN850NCgmArlEIJeITOQ6OljBn/vww6oQX4xZDxH2rvUQ+pNZGHpzyZsp7wkBANwU74dbclshoaGQkPK6oV3BjGYJKHf1SLvdNE1uMycBAOBMxvHNONA68YLUfn9MPOIx7PUoeEmWhmFommYcR7uRoI4ScB9dCIqJQC4RyCUiN7n2j0EYp0Jo9GEySVJJnQr0HRH25lGYnm6TK1ySZaG8JwQAUAZ7EjHFRx+c1/71ow/ldUN7F4Vi1gMAAIjYMACxuFLUq2hxybDIrumRJuFS27bWbWDyL5khCXXzZSTLswqPBsVEIJcI5BKRp1w2uYJ62Qqp6rj/fvNULBP25lFQSj0ej8fjYfZUVWV9DH3f76ra1dBopKCYCOQSgVwispXLtRXUJnNhIfnSj8xLsrwK2SqWA6XZUFiFAAD5syG80Uu+9CPHc2ZhCuV1Q7tiFKYBCsMwFJMxu5gbOQ0UE4FcIpBLRP5yeTMnV50S7fh/3PKmW89fsQvZZSh4yRLqujYhC3srlQeFmYQngGIikEsEcokoVa5RjWujGqUlF6pYEnbFKPR9b1ePNGEKua06DQAAZeMv+vADln9Iw96hFJNYSSnVtm0OqRhJmnEhKCYCuUQgl4i7yLUts8I0p8JbLudNaZfoOyLszaNQ17WZ3VCeI6GwJ30CKCYCuUQgl4iy5fphH8zGI4yjs3tt5EHZiu1ki+GzGPRxoeLlmXIAAAWT1qmwP5fzfsrrhrbEKNw9QcJKynvYR4NiIpBLBHKJKF6uUY3z7gSLMD6heMX2kECaYRjMuIPduBAeNgDAvdiWpdFbJsr3KChrKOBR2MvePApaazvxoes6rfXlIY06wLW1AgCAo+FVfwR7V490czYrpbquezweZcQolGcVHg2KiUAuEcgl4l5yJQlT8Cc+KJlHgb4jwl5DYZo44ZLVpd2rF/aEAADKZkM6ZyWIZ2ToYS97p0cCAADswe1V1w8dLEyShHTsMhTMMtM2KMEu9HB5SGMSGOuSgmIikEsEcom4nVwJf4Fvu/fbKXYmez0kJijBfvRCFs6nPJ8PAMCHsG3VaRUKU2DoIRHJ7ieHuZGqxCcEAPAhbIhqDE6SxFBIR7IYBWsl1HV9rVMhFXiipKCYCOQSgVwiPk2uaJjCKik+TTERuwwfuyKURxnTIwEA4GSkToXZSZKbl4ZKQnnd0C6PQtM0VVWZjM5t2/Z9X1VV27aJ6gYAABBjjBgB+AgSkSaPgsFMf7jWmCJpxoWgmAjkEoFcIm4t1x6nwls8o/mj+cvyRek7gqSJUfDiEsqIUSjsSZ8AiolALhHIJeKj5Io5FdYX8kmKSdlrKBgvQl3Xz+czQXUAAOCzsV32hvhC+vsj2GUo9H3/fD67rjNTHuzaSznMk9wPQbBSUEwEcolALhHIJQXFIqQcShmGYRiGa1ePLG9wCADgAxFFKkzTLkljFBJSXjeUMuGSysCXUN4TAgD4QEQrRU3jGd8NBXWmrVBeN7Rx6KHrOq21jWHUWjdN0zRNSd6bku7lHFBMBHKJQC4RBci1rau1FoNUgQIUO44thoJZ36GqKqWUMQ6qqhrH0SRUuNypkIrCTMITQDERyCUCuUSUIdf6m3ib+DC+/8WwZAeUodhBbPGQGMvA+BKM0WALMbkar82jEPoT7QAA4F5sG4AwH95GH3TSFSrj1WDowWAjFjP0H4wBpOXgiZKCYiKQSwRyiShGLvfNvXhPoxo351QoRrEjSLYoVHkUZhKeAIqJQC4RyCWiJLnOuZWSFEsOhgIAAGTNthRMOAlS8cdtp3nJEjIcgNhPeeNMR4NiIpBLBHKJKFgurdf5GMb3RaEW4xvKVWw/W6RZTNhcxqJQAACQDytTMP0IadTXpF0qrxsq7n6Ke0IAAGAwtsLa6Q8YCokgRiEI41tSUEwEcolALhHIpZTMMECxCBgKQQozCU8AxUQglwjkEoFchvXdP4pFwFAAAICi2JxNAWbBUAiCJ0oKiolALhHIJaJsuY64ubIV2wmGQhA8UVJQTARyiUAuEcg1R8wUQLEIGAoAAHAP6M0vITtDoeu6ruvMilNxhmHw8j6lBU+UFBQTgVwikEsEcr2xQgwUi5CRoTAMg9Z6GAazBOWiEdA0zRp7YjN4oqSgmAjkEoFcIoqXa223bmT4oQbLC28hI0OhaRqzevUwDG3bPh6PyMFYfwAAACeQkaGgnCUkzEbIYWD+WlXVoZXBFpGCYiKQSwRyiShYrpW//P0Zkkt6FKzYfnIxFIxN4C0uNWsoDMPweDxOcBPhiZKCYiKQSwRyiUAuy0oLAMUi5GIozDJrKDRN0/d95Cy9CXsuG2ywwQYbOW9YRAeLzkpVwzLYuMz0OUxXr67ruqqq+KrWewxDe+44jlrrcRzdPWzEN1BMtGHJpD6Zb2hnoR02FjfK/jJaFkQw4w3+SfMNKa1ihZG1oTDFLG9tDAW73XVd3HTYRqmP/DhQTARyiUAuEZ8gl9Yq4V1+gmKbycVQMD39MAxulz/t/tu2tdvWUDjCSgAAgFszqpdTwaCn3gVYRUbLZtd1/Xw+TX26rnMjFo3PwDMIrG3h7tTpFgJPWNSHgGIikEsEcokoXi4TDLB4i9ZQGM3/X0MEcwXSdwTJxaOgXgmXbDCIjVg00xzOdxsU9qRPAMVEIJcI5BLxIXIlHH34EMW2kZ3hMztPcj3lmXIAAOBhpxfE3/daa+M+GNWoXtvmvCPrVlo3VNz94D66DhQTgVwikEvEJ8glGn1YNBToOyJknUfhWgp70ieAYiKQSwRyiUCuecJpDlAsAoYCAAAABMFQCFJqjq3jQDERyCUCuUQglxQUi4ChEARPlBQUE4FcIpBLxOfIlap//xzFNoChAAAAAEEwFILgiZKCYiKQSwRyifgEuUQuAL20zvQnKLYZDIUgeKKkoJgI5BKBXCKQSwqKRcBQAACAGxP3BYws8LAbDIUgeKKkoJgI5BKBXCKQSwqKRcBQCIInSgqKiUAuEcgl4kPkYpnpc8hoUahUhAxD2gEAwMeix9cakkqx5rSIAg0F8nVfBYqJQC4RyCUCuaSgWASGHoLQaKSgmAjkEoFcIpDL5T2ecV4ZFIuAoQAAAB+BxhjYBIZCEIJgpaCYCOQSgVwikCtIQBkUi4ChEARPlBQUE4FcIpBLBHJ5LGZTQLEIGAoAAHBvNrkDcCGsBUMhCJ4oKSgmArlEIJcI5Ioy4zxAsQgYCkHwRElBMRHIJQK5RHyOXFtudM4m+BzFNoChAAAAAEEwFILgiZKCYiKQSwRyiUAuKSgWAUMhCJ4oKSgmArlEIJcI5FrHD+MAxSJgKAAAAEAQDIUgeKKkoJgI5BKBXCI+UK6Vd/xKzuj7Dz5QsfVgKATBEyUFxUQglwjkEoFcUlAsAoYCAADcmJVd/GJyRgiBoRAET5QUFBOBXCKQSwRyLTDRB8UiYCgEwRMlBcVEIJcI5BKBXFJQLMIfr65AekKGIe0AAABASoGGQiqDQGuNbSECxUQglwjkEvGZcmm9KaPz93M/UbGVMPQQhEYjBcVEIJcI5BKBXBH0nDYoFgFDAQAA7g29/KFgKAQhCFYKiolALhHIJQK5ZonMkESxCBgKQfBESUExEcglArlEIJcUFIuAoQAAAABBMBSC4ImSgmIikEsEcolArtV8FwrFImAoBMETJQXFRCCXCOQSgVyLeIYBikXAUAAAAIAgGApB8ERJQTERyCUCuUR8rFzCxabdEz9UsTVgKATBEyUFxUQglwjkEoFcUlAsAoYCAADcHhabPg4MhSB4oqSgmAjkEoFcIj5Zrm23/smKLYKhECTuidIOv/zyi/fR7JnuD50S2jl7mOgAy7///e+VR27Gk+VyzmopG8HVKQK5RCCXFBSLMZbFaXdkL/TPf/5TKfXPf/7T/dNPP/3kVebr169fv361294p3onuYbNXb9vWXNo9Ps6XL1/+9a9/rTy4AMpr2wCwiFLj4ldfjd9HIMbR/Eteh9JePngUgoh+kn779s1s/PLLL+5++/Hnn3+2x/z8889fv361H11+++03u/3zzz+7fxqGYRgGs/14POy2cWB4R7of//3vf//73/9efy9wDvn7PLICuUQg1yLexAcUi4ChEGRc7Yn66aefbAf/7ds34wlQSn39+vXXX39V79bDL7/88tNPP3379s21CSxfv37929/+Zg8zO4dh0FoPw9A0jTUXzH9/++23b9++/frrr1++fHGPNP9VSmmtf/311//93/81NYF8WN/AQCGXEOSSgmIxLvVnpOe0O1XO0MNPP/309etXM5SglHJHBMxh7rjDTz/9ZI60G16Z5lxToNlTVVXf9+4xVVWNr1EPd79SyhzZ931VVf/617++fPliD2DoAQDKZs3Qw2hHHxh6WMcfD7VCLmFMZBhqrdcX9eXLFzOUYN0Jhp9++umXX375+eefbVG//fbbX//617/+9a9KqW/fvv3lL3+ZLdAMVZjDns9n13Vmf1VVXvneiXVdm/8+n8//+Z//+dOf/mRruPJe4BxEDQyQSwRySUGxCAw9BBE1mr/85S9mFMDrj//2t799+/bNdudmQMGaabOjD1++fPny5YtrAVRV1XWdGXQwdkCIqqrswERVVV++fPm///s/8yeGHnKDt5II5BKBXFJQLAKGQjK+fv3622+/eR4CY0BY6+Hbt28mBMFg/A1eOSZ8wT3MRCfUda21toaC9TG4dF1njmyapuu6//f//p9S6k9/+pP1KwAAAIgozdmS0H0UL+p8P5XnToh4FxYdD59A/o7E/GuYFcgl4mPlsnMX4nevlVbK5mgc1Yl9xx0p7n7OekLlNYXC4AEBfCArDQWltX5L5pzyXVHey4ehBwAAKARRB03mhJVgKAQh/wYcCg1MBHKJQK44U3VQLAKGQpBMfEfDBDXJvRjZP7sdv0roFPtxfbEQIZMGdheQSwRySUGxGAfmaLiC0+7Iu9DviZheqG3btm1N+gSzba5uNmxlTAomt1bmLLutXrmYQrdTVZV7oXEczbZ3y6You9NNBpUV5bVtAFjDquUelDou51J5L58CEy6lIpOAFDMNsuu6uq7dKZGzv+9NHgUz5WEYBnv84/Fo29YkYwhdyC3Qze/UdZ03FbOqqrqu8SXsJJMGdheQSwRySUGxCAw9BMm50bj5GYdhML/ylVJup/58Pt1Jkl3XPZ/PNYW7syu7rns8Ht4B1hDZUHOw5NzAMgS5RCCXNOQAxSJgKNwV62B4PB62XzeZm5VSXde51oPZNu6BUIH6xePxcA/r+36alcHkgEp1LwAAkC3ZDT2YLqqu60jKIONCr18cVBORJ+oPf0hmcq28qBmMMKma3f0237M77mD/u+a63Qvz0Sg8tTDatiWt0x5wdYpALhGfLNc4LrsTxnHU71MfPlmxRTLyKNjFkc2v1dBvX/OTV71+1EZ+Iu/kFo1mGkBgBgvsuIMZILAxKWtGH6bxB8MwTO0Mc92Vwxkw5RYNLB+QSwRySUGxCBkZCk3TmFg8M+g++wvYdE7jOMYPO5+Esx7WX9QMNHg/681HO+7gRjCopdEHt1hvZ9/30yMJUwAAKJ6MnC1aa3c43Pto8MLojFPBvYWr1npIeNEk5UD+jsT8a5gVyCXiw+Vak8j5fbkHpXXK13hh4ucSo2D6fs8smC5uFEkolJzCnrTFy6qkliJC4CBKbWAHgVwikEsKikXIxVCYJW4HmMF4169u2Paj3LQSawmu2dh/0Uv4NLNA9EzZYIONAjZGJ54x/g7X7+tBpapGYWRtKESWUTZz82Zn7u15TvbccRxfDW6c/mlz+XA+kSd47Ub+Ncxqw30Fs7G4sfj6Kn7DsvIdnlaxwsgomHElXdc1TWNyDB/6yziTR37aWg+hS4dqMj1g8SrgkkkDuwvIJQK5DDFX7/ufUCzGmA3qfTECNbc2gYm9j69ZcEjlli6UcNaDx2lrPUz3mNUf7LXM1e3+vu+9xSAWn86ZZNW2AeBklPr+L3bMqI5Y7qG8l09G9+P2Ot7KQ6Zbssf077iFJHxC8aJOMxQM1kSwV7dGgOmezUd3iSZ323T29pTF2zFGwHTbO9I+jtmP15L/dzX/GmYFcolArnGUGQqn9R13JK/IC+34iWz8gYlIMB/1nCPJvYXTYkm8C6W66OwNKmdpKPPRnSZqdtoRAbvy01vAjtbja9gydF3vT1prs45U5EgTF2kHILTWJk9DDsGSpUYVAcBKzNs08hp4nyGZ7HVR3ssnrxiFcRytk8Bdv8B+nDV2LqywRf/hD0n+RYfU3ki+1oPH+Iog01pHIg/sFb3MTgAAUAZ5GQoqp8l7oR/32wtMWprxHAzhtR6sjI/H4/F4aK2fz+fKYEPrqzCmW3z9p7ZtjScjkwd3F5I3sLJBLhHIJQXFImQ9PfJaRL6K8fffF4/5fYVvS9pYZ9d6MP26t9aDqHw7iqHCk1TdK84mk4Y4mTjD7gJyiUAuKSgWITuPAqwn7VoP+oUxPobX+pwmWCFek+fzybgDAECRlBZzkTCKJF7UycGMVzHNon0X8o8nyr+GWYFcIpBLCYMZWeshQnH3U+ish4R8yFoP5X1XAUDE4tJQzHpYSXH3c5GhALnBAwKAuFMBQ2ElxCgEyW0UAAqDBiYCuUQglxQUi8CshyCZmITTCY0mx5E7WDC8Fume7rcf10Qb2Gu5Z7nXnS3qvnEM15JJA7sLyCUCuVy0jkUqGFAsAh6FNOzJjhlPHmVnKjZNY8MLmqZxJy/Yj27CA3fmpD09cgtmvoPJg2k7fnvRruts5iX3KnYWJQBAtuAv2EWqHi4TEt5RvCjvr3df68E9eHRW5HJv012Syu70VoLIh/zbdv41zArkEoFcLqFFH1jrYSV4FIKMGXui3GyMbqYE14vgpVjous7kWp7FO3icW8KbIYa05NzAMgS5RCCXFBSLUKChoAMcetE0Kz3oP+jViZ4PWuvBDDGYYQizxxUQQwEA4NMo0FAIOU+k5aS3LZKWd9BaD2Z9BzfNoivgyqUiYA1EWYtALhHINSUuCYpFKNBQSIXItvh9/H353+/Lx4zCubyzaz08Hg87lGDXejCERh88Z8Pj8ZgeY2ZVmIOtxYDpsBlcnSKQSwRySUGxCEyPvDF1XZtxh+kkxvhaD9MBCOOB0Fob/0Ro4Qa7qmTTNObI5/PJFwwAcmYcmfWwi9ISSLHWw2lkHtuYf3K0/GuYFcglArk8ZtM5s9bDSoq7H1I4L8FaDwDwaSwaCqRwjlDc/WAogFKKBwQA70zXfcBQWAnBjEFuNAoAd4QGJgK5RCCXFBSLgKEQJBOTcJigJnMNIvtnt+NXCZ0yO9OBWQ+byaSB3QXkEoFca3BnmaFYhNI8JFcNPRwXzGhmKAzD8Hw+zWQEkxOpbVs3J5KZsujWyiZaUK85k33fh2IRzH4zgcJOZDDpm2yBpnBTlN1p8j5lGOJQnvcPAPYQClNg6GERPApBMvFEmdmMdV0by8AaB7O/770MB/bgx+PhGhaRCw3D4NoTs8kcq6rK0DK4HZk0sLuAXCKQSwqKRcBQCJKzSZh8rQcXmwdavVwR0wMUgw67ybmBZQhyiUCuKXFJUCwChkIa0qz08AeBUXvQWg9TZgcszGrU0qIAAC4H34EUMjMGOWCcSSccBjPjEZG1HtxxBxXIyrwGYyVMLYy2bRmA2EN5A5mHglwikEsKikXAUAgiW+vh9zUH/75oKEjHyWbXejC/9b21HtaXb+Ih3D0mTHJ6aXeQAqTwVhKBXCKQa5ZILmcUi8DQw40x/bT3s37NWg+zpTVNY5eTnh7T9/30FMIUAACKpzRnC2s9gCF/R2L+NcwK5BKBXCG8SZJ2eiRrPURg6CGI6EnfqIP/kLUe8qewV8nRIJcI5JKCYhFKM3wiHXbaOy3PZiwMHhAAzOIu+kDCpTUU6FEoz33kOgBmCI7Q8gAADAtJREFUf/p7Kz4nXwB6tkBTK7t/6qXwDjiiYrcmnwZ2C5BLBHJJQbEIBDMGyafRNE1jxguapgkZCraftj102gp4e7TWZhKm2bB1aJrGpo72ci2Yj8Q/WvJpYLcAuUQg1yLf/QovRwKKxRjL4rQ78i70eyIWr2W2+753D2jb1uypqqptW7u/73vvSPsx8qfFm+37vqoq+9Hddo80sy1ssSbfQ+QqaSmvbQNAEpT6/s/uGMeUr4vyXj54FIJkGJ9of47bX+rWf2C8+jZdo3qlbrQpEEzaA/NXrbXJyGSTKZlyVt6ymZY5XXViirvAxPP59HJDfTgZNrCcQS4RyBVh1neAYhEwFIKMOXmiTIaDpmlMPoO2bU33bPMoTNMvPp9PYw3YlaKsVdH3vbEhTLokY2R0Xbf+ls2RplYRQ8FmZPLSOYDKrIHlD3KJQC4pKBYBQyENWv8hyT+l5q1a6wKy60GbnIy29zWmg82nZC2D2ZgG17ugXuEFWuuVkYZ26epxHPu+jy/6YCqW51LUAPDJ4ERYSYGzHlKRPAhW64Xly9Zjfqk/n09bQ5uw2fT3bv5mO7Lg7jT/NR4I411QL5fDYo9uDAtjZywebDM9Yyh4EGUtArlEIJcUFIuAoRBE1GjG8ffFY37/fdlQWD9OZkcfPMxP/HEc27bVWk/DArxu2xRijox057ZiJuzA2iLP53NxTGHNMR8IbyURyCUCueJMF31AsQil2VCnWYWXp3B2e+sNJElpkHNeBH4fAEAEJ+2SeeuScClIcffzGWs9dF33eDyOeHbFJHjO/7uafw2zArlEINcizqIPWrHWQ5Ti7ucijwLkBg8IACJ4hgIehQjMegAAgI+jrK78WAhmDLJoFZKgA/ZQ3s+OQ0EuEcglBcUiYCgEiTcamhTshCYkArlEIJcUFIvA0AMAAAAEwVAIwsiCFBQTgVwikEsEcklBsQgYCkHwRElBMRHIJQK5RCCXFBSLUGCMQsgwpB0AAEAAnXCGZGEUaCiQNOMqUEwEcolALhHIBQlh6CEIXzMpKCYCuUQglwjkWg+xCYtgKAAAAEAQDIUgBMFKQTERyCUCuUQglwCN92UBDIUg+O6koJgI5BKBXCKQCxKCoQAAAJ8I1tRKMBSC4LuTgmIikEsEcolALkgIhgIAAAAEwVAAAACAIBgKAAAAEARDAQAAPhTiGdeAoQAAAABB7rrWQ9d1Sqm6ruu6vrgq60iYej3PotKS7T3mqVi294hcVxWVljzvMVu5yuN+HoVhGLTWwzAMw9A0jbEYAAAA4AjuZ5FprauqGoZBKdV13ePxcG8hW3M1z4pxjxeWlmdRaUsrvqi0peVZVNrSMixKazWOJu1EXhXLh/vdj9a673s74jD9mFsrTF5ankWlLS3PotKWlmdRaUsrvqi0peVZVNrSMiwKQ2GRmw09GEeCF5dgdgIAAEBy7hrM6OIZCglzl6ZNg5pnxbjHC0vLs6i0pRVfVNrS8iwqbWl5FpW8tJIowVBwHQyFOXwAAOAc6D1C3GzoAQAAAM7kZoaCcR54Yw13SaUAAABwO25mKCilqqpqmsZs27RLF9YHAACgYG45i8MNOXHnRibkdpkfz2SNOF3XDcNQvzirajmyvi2ZNGIfnkNsjVxGqMXDPgHRl/HDm9YiXdch0TzjPen7vu/7g0pWSlVVVVWVUqpt2yOuclNWimOaFhpK25I5+IyaZclKudq2dQ876D2QP9Ivo9n4WLkWMXqizyx3NRSOw31Zm1fSpdXJizXiePs/WUNRW7Iv9BMqlidr5PLe5qb/O6uCebHhy/jhDSxE3/cYUnHuF6NwAtb7ZDZI6OSyKM4wDPZbpz4+gmRlWzJ/dXX7TNa0LuU0qmEYxhsOnqZC+qaigYWo69oYVTALhsIbZH6MsFIcO34cOuBDWN+WhmHwliz5QNa3LrPUixlOpnVNd7oYA8II1XXd8/n8cMN9FhO9QXRChBISLh3Nx76M1hAXx6zahalumZWraRrjUQePqVzP51Mp1TSN+XFsWheveMNs62rb9vF4PB4PpVRVVWgFG8CjsAw2eISQOGY18Mfj0fc97ybLVK66rquqoo3NEpJlHEfjuDK94LmVypepXMZZZcbd+77HowDbwFCA9HRd1zSNCcPmxRTn+Xya13dd13YbJ1YIG+RvoHXFMV9Do1Jd18ZWuLpScD8wFN4g82OEleLYHzEf7khYKVfbtuZVbv/0mbkBVsr1gcrMwpsKTuXCGRd54k64+uSpfbNExGnb1ng47ex2lysqez1r5PKO/+TZa2vkmk6P/FjF1sjl7f/k2aRrUEyPDECjmcE1pGg3HrPiuK9v7FGXRblcPrnbM6yRy4uNvaqqObBGLm9KJC+0COgT4pYpnE9gdvYRGBBHBHKJWCkXqhqQC04AQwEAAACCEMwIAAAAQTAUAAAAIAiGAgAAAATBUAAAiNF1nXZwE4RorZNnxzJLD0hP0Vq722Zxh3j04oUZO4ZhcC9t6mw4IuGYSRQ73b8otej5FhwriqEAABDErldi5omZpNG2dzko/baopxyG4fl8mlmRdrt+ETnxQkOhaRqroenC3TwZTdMkv6KdJipKbmbPCpkaHsVmmbtsYiYAQPYopayVYDg6D5s0nYZbH9PRHlKtdLiVnK2wOjKfwbZsJWuEvYX428CjAAAQw/t933WdXe3TdU3bEQrj0LZZlo0v3brW7fHuzpUubtdFby9qlsUyJZjf4tOhB/dadqe3trIt3P2RbYqyf3KPd0dkTOW9wY7Qr/Cu67ysWd69G49IpOa2YqHxIE8lWxOz1vbz+TQf7dCDp7/1NJj9nrBa6//+97/uweZ2ZvNqF8LVlgoAQL7YLs3zKxjU67evOczNYm5+thqToqoq90/2XLO/73t3f+gnr6nG9HjXoxDa9upj7sW90LTC3kW9e7TbbmW8n9SRG3EdBuYSVqLpwaa2kYqFVPVualqrqqpM4W7J7kW9+529L6XUf/7zH6/AwsBQAACIYfvCqcWgnMzl3n63i/LyK5tt9/hQTzZ7gHfp0NCD3T9dCcKUby80HUyZWhjx+7XdvHuzam4EYdY/7zoYXIvBq5h77poHYReaWTQUQhqq9+AJs/M///mP3f773/8+q21h/HHR5QAA8MnYOHzjz388Ho/HY5zktHUd494KC7MRc13XDS8WV3+2vn1v55poxGEY3PpMA+7ihYcu4Y0ymI2qqsx9mdJWBkvaQRAjr1kd25TjVsw+hUjFqqp6PB7mmPWhhea69izv8Xn8+c9/Vkr997///fOf//yPf/zD2AqGuq7NSFBhEKMAABDEG8UfhsHOL9hZsta6aRrTOXlj9mswq5PvrEOkcLu9xlCwmCAApZSZKrJ4IWMnuaeP42g6e6XU8/l0+2wjVPyu7QN6PB5eWEMcY+KYiy5aGFVV/eMf/zDbRVoGHhgKAABBzA9Nd8+avmelh2AcxzUJD+xFO4eVNTGHufWZ+iG2Fe5F/7nrTln3wGLdbJzgLMZK8Cq2WKCpzDiObdsuPgiLUWnlvf/9739/Pp9TS6jMSEYMBQCACFVVmd/9ds+s39scZrY39BaLXaDbAStnssMavGj8acfs9etr/Ofu/Vovgv3T4/EIee9nbRRvFMP2waZkW/M1FXMzNIgwZ610hJjRB2/cwRAftrgr14ZIAABkzvTVb/+k3mP3LDZizovd8wLlLLZzGsOTBeycTHuK2b8YzDi+RwuqubA+r/BQQGLofqeBmZHgf69M79LeubM1j1TMO96dMeEWaMM5vZBMFYgYHd/nWcwePK1VMbDMNADAMq5rffGYlVPqvTLXBCeuqcbmc6WFzx5vRhMiPYuN4pwWFbp6koptLs090Z41fcSLN35fMBQAAPbidhumw3CzBn0UWuuqquJGktb67vporf/zn/+YMQiDMRCLzOKMoQAAsBcvKM/M7ruuOtdgRVjsVkwI501D/0zY49QY0rrY/rTYGwMAOJk94wJlsDK1w90xSRSursV5YCgAAABAEKZHAgAAQBAMBQAAAAiCoQAAAABBMBQAAAAgCIYCAAAABMFQAAAAgCAYCgAAABAEQwEAAACCYCgAAABAEAwFAAAACIKhAAAAAEH+P9zdLt+ZvmymAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  plot ROC curve\n",
    "c1 = factory.GetROCCurve(dataloader)\n",
    "c1.Draw()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02873c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (outputFile):\n",
    "    outputFile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f11903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
