{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f33311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "from ROOT import TMVA \n",
    "import os\n",
    "from array import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7188db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT.TMVA.Tools.Instance()\n",
    "TMVA.PyMethodBase.PyInitialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f51601bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/neel/jupyter/environment/lib/python3.8/site-packages/torch']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__path__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f388d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print (torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66abd9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "print (torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dabbdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFile = ROOT.TFile.Open(\"CNN_ClassificationOutput.root\", \"RECREATE\")\n",
    "\n",
    "factory = ROOT.TMVA.Factory(\"TMVA_CNN_Classification\", outputFile,\n",
    "                      \"!V:ROC:!Silent:Color:!DrawProgressBar:AnalysisType=Classification\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dd06255",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=[1,1,1,1,1]\n",
    "useTMVACNN = opt[0] if (len(opt) > 0) else False\n",
    "useKerasCNN = opt[0] if (len(opt) > 1) else False\n",
    "useTMVADNN = opt[0] if (len(opt) > 2) else False\n",
    "useTMVABDT = opt[0] if (len(opt) > 3) else False\n",
    "usePyTorchCNN = opt[0] if (len(opt) > 4) else False\n",
    "# useTMVACNN = False\n",
    "\n",
    "writeOutputFile = True\n",
    "\n",
    "num_threads = 0  # use default threads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24d424b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with nthreads  = 4\n"
     ]
    }
   ],
   "source": [
    "# do enable MT running\n",
    "if (num_threads >= 0):\n",
    "  ROOT.EnableImplicitMT(num_threads)\n",
    "  if (num_threads > 0):\n",
    "     ROOT.gSystem.Setenv(\"OMP_NUM_THREADS\", ROOT.TString.Format(\"%d\",num_threads))\n",
    "\n",
    "else:\n",
    "  ROOT.gSystem.Setenv(\"OMP_NUM_THREADS\", \"1\")\n",
    "\n",
    "print(\"Running with nthreads  = \" + str(ROOT.GetThreadPoolSize()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44aa7b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __debug__:\n",
    "    ROOT.gSystem.Setenv(\"KERAS_BACKEND\", \"tensorflow\")\n",
    "    # for using Keras\n",
    "#     TMVA.PyMethodBase.PyInitialize()\n",
    "else:\n",
    "    useKerasCNN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c2630a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ROOT.TMVA.DataLoader(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d092fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************\n",
      "*Tree    :sig_tree  : signal_tree                                            *\n",
      "*Entries :     5000 : Total =         5207924 bytes  File  Size =    4659345 *\n",
      "*        :          : Tree compression factor =   1.12                       *\n",
      "******************************************************************************\n",
      "*Br    0 :vars      : vector<float>                                          *\n",
      "*Entries :     5000 : Total  Size=    5207506 bytes  File Size  =    4657486 *\n",
      "*Baskets :      167 : Basket Size=      32000 bytes  Compression=   1.12     *\n",
      "*............................................................................*\n"
     ]
    }
   ],
   "source": [
    "imgSize = 16 * 16\n",
    "inputFileName = \"images_data_16x16.root\"\n",
    "inputFile = ROOT.TFile.Open(inputFileName)\n",
    "if (inputFile == None):\n",
    "    Error(\"TMVA_CNN_Classification\", \"Error opening input file %s - exit\", inputFileName.Data())\n",
    "signalTree     = inputFile.Get(\"sig_tree\")\n",
    "backgroundTree = inputFile.Get(\"bkg_tree\")\n",
    "\n",
    "signalTree.Print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fea575dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSetInfo              : [dataset] : Added class \"Signal\"\n",
      "                         : Add Tree sig_tree of type Signal with 5000 events\n",
      "DataSetInfo              : [dataset] : Added class \"Background\"\n",
      "                         : Add Tree bkg_tree of type Background with 5000 events\n"
     ]
    }
   ],
   "source": [
    "nEventsSig = signalTree.GetEntries()\n",
    "nEventsBkg = backgroundTree.GetEntries()\n",
    "# global event weights per tree (see below for setting event-wise weights)\n",
    "signalWeight = 1.0\n",
    "backgroundWeight = 1.0\n",
    "\n",
    "# You can add an arbitrary number of signal or background trees\n",
    "loader.AddSignalTree(signalTree, signalWeight)\n",
    "loader.AddBackgroundTree(backgroundTree, backgroundWeight)\n",
    "\n",
    "## add event variables (image)\n",
    "## use new method (from ROOT 6.20 to add a variable array for all image data)\n",
    " \n",
    "loader.AddVariablesArray(\"vars\",imgSize,'F')\n",
    "\n",
    "# Set individual event weights (the variables must exist in the original TTree)\n",
    "#    for signal    : factory.SetSignalWeightExpression    (\"weight1*weight2\")\n",
    "#    for background: factory.SetBackgroundWeightExpression(\"weight1*weight2\")\n",
    "# loader.SetBackgroundWeightExpression( \"weight\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e752461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply additional cuts on the signal and background samples (can be different)\n",
    "mycuts = ROOT.TCut(\"\") # for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "mycutb = ROOT.TCut(\"\") # for example: TCut mycutb = \"abs(var1)<0.5\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00e75b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  If no numbers of events are given, half of the events in the tree are used\n",
    "#  for training, and the other half for testing:\n",
    "#     loader.PrepareTrainingAndTestTree( mycut, \"SplitMode=random:!V\" );\n",
    "#  It is possible also to specify the number of training and testing events,\n",
    "#  note we disable the computation of the correlation matrix of the input variables\n",
    "\n",
    "nTrainSig = 0.8 * nEventsSig\n",
    "nTrainBkg = 0.8 * nEventsBkg\n",
    "\n",
    "#  build the string options for DataLoader::PrepareTrainingAndTestTree\n",
    "prepareOptions = \"nTrain_Signal=\"+str(nTrainSig)+\":nTrain_Background=\"+str(nTrainBkg)+\":SplitMode=Random:SplitSeed=100:NormMode=NumEvents:!V:!CalcCorrelations\"\n",
    "  \n",
    "loader.PrepareTrainingAndTestTree(mycuts, mycutb, prepareOptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98c16b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mBDT\u001b[0m\n",
      "                         : \n",
      "                         : Rebuilding Dataset dataset\n",
      "                         : Building event vectors for type 2 Signal\n",
      "                         : Dataset[dataset] :  create input formulas for tree sig_tree\n",
      "                         : Using variable vars[0] from array expression vars of size 256\n",
      "                         : Building event vectors for type 2 Background\n",
      "                         : Dataset[dataset] :  create input formulas for tree bkg_tree\n",
      "                         : Using variable vars[0] from array expression vars of size 256\n",
      "DataSetFactory           : [dataset] : Number of events in input trees\n",
      "                         : \n",
      "                         : \n",
      "                         : Number of training and testing events\n",
      "                         : ---------------------------------------------------------------------------\n",
      "                         : Signal     -- training events            : 4000\n",
      "                         : Signal     -- testing events             : 1000\n",
      "                         : Signal     -- training and testing events: 5000\n",
      "                         : Background -- training events            : 4000\n",
      "                         : Background -- testing events             : 1000\n",
      "                         : Background -- training and testing events: 5000\n",
      "                         : \n"
     ]
    }
   ],
   "source": [
    "if (useTMVABDT):\n",
    "  factory.BookMethod(loader, ROOT.TMVA.Types.kBDT, \"BDT\",\"!V:NTrees=400:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:\"+\"UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e98f863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cppyy.gbl.TMVA.MethodDL object at 0xd4ae640>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mTMVA_DNN_CPU\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     Layout: \"DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIER\" [Weight initialization strategy]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     InputLayout: \"0|0|0\" [The Layout of the input]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         :     RandomSeed: \"0\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     ValidationSize: \"20%\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n"
     ]
    }
   ],
   "source": [
    "# Define the DNN layout\n",
    "if (useTMVADNN):\n",
    "  layoutString = \"Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR\"\n",
    "\n",
    "  #  Training strategies\n",
    "  #  one can catenate several training strings with different parameters (e.g. learning rates or regularizations\n",
    "  #  parameters) The training string must be concatenates with the `|` delimiter\n",
    "  trainingString1 = \"LearningRate=1e-3,Momentum=0.9,Repetitions=1,\"+ \"ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,\"+\"MaxEpochs=20,WeightDecay=1e-4,Regularization=None,\"+\"Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.\"\n",
    "                          \n",
    "\n",
    "  trainingStrategyString = \"TrainingStrategy=\"\n",
    "  trainingStrategyString += trainingString1 # + \"|\" + trainingString2 + ....\n",
    "\n",
    "  # Build now the full DNN Option string\n",
    "\n",
    "  dnnOptions = \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"+\"WeightInitialization=XAVIER\"\n",
    "  dnnOptions+= \":\"\n",
    "  dnnOptions+= layoutString\n",
    "  dnnOptions+= \":\"\n",
    "  dnnOptions+= trainingStrategyString\n",
    "\n",
    "  dnnMethodName = \"TMVA_DNN_CPU\"\n",
    "\n",
    "  dnnOptions += \":Architecture=CPU\"\n",
    "\n",
    "\n",
    "factory.BookMethod(loader, ROOT.TMVA.Types.kDL, dnnMethodName, dnnOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "861dd642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cppyy.gbl.TMVA.MethodDL object at 0xd6b11d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mTMVA_CNN_CPU\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER::Architecture=CPU:InputLayout=1|16|16:Layout=CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER::Architecture=CPU:InputLayout=1|16|16:Layout=CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU\"\n",
      "<WARNING>                : Value for option architecture was previously set to CPU\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"1|16|16\" [The Layout of the input]\n",
      "                         :     Layout: \"CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIER\" [Weight initialization strategy]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     BatchLayout: \"0|0|0\" [The Layout of the batch]\n",
      "                         :     RandomSeed: \"0\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         :     ValidationSize: \"20%\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n",
      "                         : Will now use the CPU architecture with BLAS and IMT support !\n"
     ]
    }
   ],
   "source": [
    "inputLayoutString =\"InputLayout=1|16|16\"\n",
    "\n",
    "#  Batch Layout\n",
    "layoutString = \"Layout=CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,\"+\"RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR\"\n",
    "\n",
    "#  Training strategies.\n",
    "trainingString1 = \"LearningRate=1e-3,Momentum=0.9,Repetitions=1,\"+\"ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,\"+\"MaxEpochs=20,WeightDecay=1e-4,Regularization=None,\"+\"Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0\"\n",
    "\n",
    "trainingStrategyString = \"TrainingStrategy=\"\n",
    "trainingStrategyString += trainingString1 # + \"|\" + trainingString2 + \"|\" + trainingString3; for concatenating more training strings\n",
    "\n",
    "# Build full CNN Options.\n",
    "\n",
    "\n",
    "cnnOptions = \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\" +\"WeightInitialization=XAVIER::Architecture=CPU\"\n",
    "\n",
    "cnnOptions +=  \":\" + inputLayoutString\n",
    "cnnOptions +=  \":\" + layoutString\n",
    "cnnOptions +=  \":\" + trainingStrategyString\n",
    "  ## New DL (CNN)\n",
    "cnnMethodName = \"TMVA_CNN_CPU\"\n",
    "# use GPU if available\n",
    "\n",
    "\n",
    "cnnOptions += \":Architecture=CPU\"\n",
    "cnnMethodName = \"TMVA_CNN_CPU\"\n",
    "\n",
    "\n",
    "factory.BookMethod(loader, ROOT.TMVA.Types.kDL, cnnMethodName, cnnOptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ece92475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom objects for loading model :  {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7fe9297815e0>, 'predict_func': <function predict at 0x7fe8b9519310>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cppyy.gbl.TMVA.MethodPyTorch object at 0xd6e7780>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mPyTorch\u001b[0m\n",
      "                         : \n",
      "                         : Using PyTorch - setting special configuration options \n",
      "                         : Using PyTorch version 1\n",
      "                         :  Setup PyTorch Model \n",
      "                         :  Executing user initialization code from  /home/neel/Root/install/tutorials/tmva/PyTorch_Generate_CNN_Model.py\n",
      "                         : Loaded pytorch train function: \n",
      "                         : Loaded pytorch optimizer: \n",
      "                         : Loaded pytorch loss function: \n",
      "                         : Loaded pytorch predict function: \n",
      "                         : Load model from file: PyTorchModelCNN.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info in <TMVA_CNN_Classification>: Booking PyTorch CNN model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pyTorchFileName = str(ROOT.gROOT.GetTutorialDir()) + \"/tmva/PyTorch_Generate_CNN_Model.py\"\n",
    "\n",
    "ROOT.Info(\"TMVA_CNN_Classification\", \"Booking PyTorch CNN model\")\n",
    "methodOpt = \"H:!V:VarTransform=None:FilenameModel=PyTorchModelCNN.pt:\" + \"FilenameTrainedModel=PyTorchTrainedModelCNN.pt:NumEpochs=20:BatchSize=100\"\n",
    "methodOpt += \":UserCode=\" + pyTorchFileName\n",
    "factory.BookMethod(loader, ROOT.TMVA.Types.kPyTorch, \"PyTorch\", methodOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4275b444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=Sequential\n",
      "  (0): RecursiveScriptModule(original_name=Reshape)\n",
      "  (1): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (2): RecursiveScriptModule(original_name=ReLU)\n",
      "  (3): RecursiveScriptModule(original_name=BatchNorm2d)\n",
      "  (4): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (5): RecursiveScriptModule(original_name=ReLU)\n",
      "  (6): RecursiveScriptModule(original_name=MaxPool2d)\n",
      "  (7): RecursiveScriptModule(original_name=Flatten)\n",
      "  (8): RecursiveScriptModule(original_name=Linear)\n",
      "  (9): RecursiveScriptModule(original_name=ReLU)\n",
      "  (10): RecursiveScriptModule(original_name=Linear)\n",
      "  (11): RecursiveScriptModule(original_name=Sigmoid)\n",
      ")\n",
      "[1, 4] train loss: 1.661\n",
      "[1, 8] train loss: 0.747\n",
      "[1, 12] train loss: 0.714\n",
      "[1, 16] train loss: 0.703\n",
      "[1, 20] train loss: 0.702\n",
      "[1, 24] train loss: 0.693\n",
      "[1, 28] train loss: 0.695\n",
      "[1, 32] train loss: 0.693\n",
      "[1, 36] train loss: 0.692\n",
      "[1, 40] train loss: 0.693\n",
      "[1, 44] train loss: 0.693\n",
      "[1, 48] train loss: 0.692\n",
      "[1, 52] train loss: 0.694\n",
      "[1, 56] train loss: 0.692\n",
      "[1, 60] train loss: 0.692\n",
      "[1, 64] train loss: 0.692\n",
      "[1] val loss: 0.687\n",
      "[2, 4] train loss: 0.691\n",
      "[2, 8] train loss: 0.686\n",
      "[2, 12] train loss: 0.683\n",
      "[2, 16] train loss: 0.682\n",
      "[2, 20] train loss: 0.679\n",
      "[2, 24] train loss: 0.683\n",
      "[2, 28] train loss: 0.669\n",
      "[2, 32] train loss: 0.656\n",
      "[2, 36] train loss: 0.660\n",
      "[2, 40] train loss: 0.628\n",
      "[2, 44] train loss: 0.683\n",
      "[2, 48] train loss: 0.720\n",
      "[2, 52] train loss: 0.700\n",
      "[2, 56] train loss: 0.660\n",
      "[2, 60] train loss: 0.677\n",
      "[2, 64] train loss: 0.680\n",
      "[2] val loss: 0.663\n",
      "[3, 4] train loss: 0.668\n",
      "[3, 8] train loss: 0.665\n",
      "[3, 12] train loss: 0.654\n",
      "[3, 16] train loss: 0.666\n",
      "[3, 20] train loss: 0.626\n",
      "[3, 24] train loss: 0.627\n",
      "[3, 28] train loss: 0.561\n",
      "[3, 32] train loss: 0.582\n",
      "[3, 36] train loss: 0.543\n",
      "[3, 40] train loss: 0.501\n",
      "[3, 44] train loss: 0.500\n",
      "[3, 48] train loss: 0.503\n",
      "[3, 52] train loss: 0.461\n",
      "[3, 56] train loss: 0.448\n",
      "[3, 60] train loss: 0.465\n",
      "[3, 64] train loss: 0.634\n",
      "[3] val loss: 0.655\n",
      "[4, 4] train loss: 0.556\n",
      "[4, 8] train loss: 0.559\n",
      "[4, 12] train loss: 0.522\n",
      "[4, 16] train loss: 0.534\n",
      "[4, 20] train loss: 0.454\n",
      "[4, 24] train loss: 0.461\n",
      "[4, 28] train loss: 0.465\n",
      "[4, 32] train loss: 0.474\n",
      "[4, 36] train loss: 0.525\n",
      "[4, 40] train loss: 0.468\n",
      "[4, 44] train loss: 0.597\n",
      "[4, 48] train loss: 0.574\n",
      "[4, 52] train loss: 0.558\n",
      "[4, 56] train loss: 0.591\n",
      "[4, 60] train loss: 0.594\n",
      "[4, 64] train loss: 0.604\n",
      "[4] val loss: 0.548\n",
      "[5, 4] train loss: 0.542\n",
      "[5, 8] train loss: 0.487\n",
      "[5, 12] train loss: 0.489\n",
      "[5, 16] train loss: 0.491\n",
      "[5, 20] train loss: 0.438\n",
      "[5, 24] train loss: 0.421\n",
      "[5, 28] train loss: 0.410\n",
      "[5, 32] train loss: 0.439\n",
      "[5, 36] train loss: 0.441\n",
      "[5, 40] train loss: 0.434\n",
      "[5, 44] train loss: 0.514\n",
      "[5, 48] train loss: 0.512\n",
      "[5, 52] train loss: 0.520\n",
      "[5, 56] train loss: 0.549\n",
      "[5, 60] train loss: 0.502\n",
      "[5, 64] train loss: 0.547\n",
      "[5] val loss: 0.607\n",
      "[6, 4] train loss: 0.524\n",
      "[6, 8] train loss: 0.503\n",
      "[6, 12] train loss: 0.468\n",
      "[6, 16] train loss: 0.477\n",
      "[6, 20] train loss: 0.434\n",
      "[6, 24] train loss: 0.424\n",
      "[6, 28] train loss: 0.390\n",
      "[6, 32] train loss: 0.395\n",
      "[6, 36] train loss: 0.403\n",
      "[6, 40] train loss: 0.393\n",
      "[6, 44] train loss: 0.429\n",
      "[6, 48] train loss: 0.471\n",
      "[6, 52] train loss: 0.504\n",
      "[6, 56] train loss: 0.543\n",
      "[6, 60] train loss: 0.490\n",
      "[6, 64] train loss: 0.504\n",
      "[6] val loss: 0.584\n",
      "[7, 4] train loss: 0.469\n",
      "[7, 8] train loss: 0.462\n",
      "[7, 12] train loss: 0.458\n",
      "[7, 16] train loss: 0.473\n",
      "[7, 20] train loss: 0.403\n",
      "[7, 24] train loss: 0.407\n",
      "[7, 28] train loss: 0.380\n",
      "[7, 32] train loss: 0.387\n",
      "[7, 36] train loss: 0.396\n",
      "[7, 40] train loss: 0.386\n",
      "[7, 44] train loss: 0.422\n",
      "[7, 48] train loss: 0.474\n",
      "[7, 52] train loss: 0.519\n",
      "[7, 56] train loss: 0.560\n",
      "[7, 60] train loss: 0.530\n",
      "[7, 64] train loss: 0.560\n",
      "[7] val loss: 0.548\n",
      "[8, 4] train loss: 0.523\n",
      "[8, 8] train loss: 0.466\n",
      "[8, 12] train loss: 0.459\n",
      "[8, 16] train loss: 0.482\n",
      "[8, 20] train loss: 0.411\n",
      "[8, 24] train loss: 0.396\n",
      "[8, 28] train loss: 0.357\n",
      "[8, 32] train loss: 0.380\n",
      "[8, 36] train loss: 0.394\n",
      "[8, 40] train loss: 0.376\n",
      "[8, 44] train loss: 0.394\n",
      "[8, 48] train loss: 0.455\n",
      "[8, 52] train loss: 0.501\n",
      "[8, 56] train loss: 0.547\n",
      "[8, 60] train loss: 0.515\n",
      "[8, 64] train loss: 0.547\n",
      "[8] val loss: 0.526\n",
      "[9, 4] train loss: 0.512\n",
      "[9, 8] train loss: 0.458\n",
      "[9, 12] train loss: 0.443\n",
      "[9, 16] train loss: 0.474\n",
      "[9, 20] train loss: 0.404\n",
      "[9, 24] train loss: 0.391\n",
      "[9, 28] train loss: 0.353\n",
      "[9, 32] train loss: 0.378\n",
      "[9, 36] train loss: 0.386\n",
      "[9, 40] train loss: 0.365\n",
      "[9, 44] train loss: 0.386\n",
      "[9, 48] train loss: 0.447\n",
      "[9, 52] train loss: 0.468\n",
      "[9, 56] train loss: 0.518\n",
      "[9, 60] train loss: 0.510\n",
      "[9, 64] train loss: 0.549\n",
      "[9] val loss: 0.536\n",
      "[10, 4] train loss: 0.500\n",
      "[10, 8] train loss: 0.445\n",
      "[10, 12] train loss: 0.433\n",
      "[10, 16] train loss: 0.472\n",
      "[10, 20] train loss: 0.397\n",
      "[10, 24] train loss: 0.391\n",
      "[10, 28] train loss: 0.346\n",
      "[10, 32] train loss: 0.379\n",
      "[10, 36] train loss: 0.387\n",
      "[10, 40] train loss: 0.354\n",
      "[10, 44] train loss: 0.381\n",
      "[10, 48] train loss: 0.436\n",
      "[10, 52] train loss: 0.451\n",
      "[10, 56] train loss: 0.501\n",
      "[10, 60] train loss: 0.513\n",
      "[10, 64] train loss: 0.561\n",
      "[10] val loss: 0.485\n",
      "[11, 4] train loss: 0.481\n",
      "[11, 8] train loss: 0.440\n",
      "[11, 12] train loss: 0.442\n",
      "[11, 16] train loss: 0.475\n",
      "[11, 20] train loss: 0.392\n",
      "[11, 24] train loss: 0.380\n",
      "[11, 28] train loss: 0.342\n",
      "[11, 32] train loss: 0.375\n",
      "[11, 36] train loss: 0.389\n",
      "[11, 40] train loss: 0.362\n",
      "[11, 44] train loss: 0.382\n",
      "[11, 48] train loss: 0.446\n",
      "[11, 52] train loss: 0.452\n",
      "[11, 56] train loss: 0.505\n",
      "[11, 60] train loss: 0.521\n",
      "[11, 64] train loss: 0.575\n",
      "[11] val loss: 0.440\n",
      "[12, 4] train loss: 0.483\n",
      "[12, 8] train loss: 0.444\n",
      "[12, 12] train loss: 0.459\n",
      "[12, 16] train loss: 0.489\n",
      "[12, 20] train loss: 0.387\n",
      "[12, 24] train loss: 0.391\n",
      "[12, 28] train loss: 0.351\n",
      "[12, 32] train loss: 0.373\n",
      "[12, 36] train loss: 0.393\n",
      "[12, 40] train loss: 0.368\n",
      "[12, 44] train loss: 0.398\n",
      "[12, 48] train loss: 0.451\n",
      "[12, 52] train loss: 0.416\n",
      "[12, 56] train loss: 0.461\n",
      "[12, 60] train loss: 0.466\n",
      "[12, 64] train loss: 0.523\n",
      "[12] val loss: 0.420\n",
      "[13, 4] train loss: 0.453\n",
      "[13, 8] train loss: 0.425\n",
      "[13, 12] train loss: 0.431\n",
      "[13, 16] train loss: 0.468\n",
      "[13, 20] train loss: 0.373\n",
      "[13, 24] train loss: 0.385\n",
      "[13, 28] train loss: 0.338\n",
      "[13, 32] train loss: 0.369\n",
      "[13, 36] train loss: 0.392\n",
      "[13, 40] train loss: 0.363\n",
      "[13, 44] train loss: 0.392\n",
      "[13, 48] train loss: 0.434\n",
      "[13, 52] train loss: 0.388\n",
      "[13, 56] train loss: 0.438\n",
      "[13, 60] train loss: 0.434\n",
      "[13, 64] train loss: 0.509\n",
      "[13] val loss: 0.417\n",
      "[14, 4] train loss: 0.432\n",
      "[14, 8] train loss: 0.406\n",
      "[14, 12] train loss: 0.403\n",
      "[14, 16] train loss: 0.457\n",
      "[14, 20] train loss: 0.367\n",
      "[14, 24] train loss: 0.367\n",
      "[14, 28] train loss: 0.331\n",
      "[14, 32] train loss: 0.362\n",
      "[14, 36] train loss: 0.385\n",
      "[14, 40] train loss: 0.356\n",
      "[14, 44] train loss: 0.375\n",
      "[14, 48] train loss: 0.427\n",
      "[14, 52] train loss: 0.373\n",
      "[14, 56] train loss: 0.429\n",
      "[14, 60] train loss: 0.428\n",
      "[14, 64] train loss: 0.504\n",
      "[14] val loss: 0.417\n",
      "[15, 4] train loss: 0.428\n",
      "[15, 8] train loss: 0.393\n",
      "[15, 12] train loss: 0.391\n",
      "[15, 16] train loss: 0.446\n",
      "[15, 20] train loss: 0.364\n",
      "[15, 24] train loss: 0.358\n",
      "[15, 28] train loss: 0.324\n",
      "[15, 32] train loss: 0.351\n",
      "[15, 36] train loss: 0.375\n",
      "[15, 40] train loss: 0.346\n",
      "[15, 44] train loss: 0.369\n",
      "[15, 48] train loss: 0.413\n",
      "[15, 52] train loss: 0.365\n",
      "[15, 56] train loss: 0.431\n",
      "[15, 60] train loss: 0.422\n",
      "[15, 64] train loss: 0.498\n",
      "[15] val loss: 0.440\n",
      "[16, 4] train loss: 0.431\n",
      "[16, 8] train loss: 0.389\n",
      "[16, 12] train loss: 0.378\n",
      "[16, 16] train loss: 0.440\n",
      "[16, 20] train loss: 0.366\n",
      "[16, 24] train loss: 0.356\n",
      "[16, 28] train loss: 0.323\n",
      "[16, 32] train loss: 0.351\n",
      "[16, 36] train loss: 0.371\n",
      "[16, 40] train loss: 0.342\n",
      "[16, 44] train loss: 0.360\n",
      "[16, 48] train loss: 0.415\n",
      "[16, 52] train loss: 0.347\n",
      "[16, 56] train loss: 0.412\n",
      "[16, 60] train loss: 0.403\n",
      "[16, 64] train loss: 0.479\n",
      "[16] val loss: 0.429\n",
      "[17, 4] train loss: 0.416\n",
      "[17, 8] train loss: 0.380\n",
      "[17, 12] train loss: 0.366\n",
      "[17, 16] train loss: 0.434\n",
      "[17, 20] train loss: 0.361\n",
      "[17, 24] train loss: 0.359\n",
      "[17, 28] train loss: 0.325\n",
      "[17, 32] train loss: 0.349\n",
      "[17, 36] train loss: 0.355\n",
      "[17, 40] train loss: 0.338\n",
      "[17, 44] train loss: 0.358\n",
      "[17, 48] train loss: 0.394\n",
      "[17, 52] train loss: 0.342\n",
      "[17, 56] train loss: 0.399\n",
      "[17, 60] train loss: 0.390\n",
      "[17, 64] train loss: 0.461\n",
      "[17] val loss: 0.419\n",
      "[18, 4] train loss: 0.409\n",
      "[18, 8] train loss: 0.376\n",
      "[18, 12] train loss: 0.361\n",
      "[18, 16] train loss: 0.432\n",
      "[18, 20] train loss: 0.356\n",
      "[18, 24] train loss: 0.351\n",
      "[18, 28] train loss: 0.318\n",
      "[18, 32] train loss: 0.354\n",
      "[18, 36] train loss: 0.364\n",
      "[18, 40] train loss: 0.337\n",
      "[18, 44] train loss: 0.362\n",
      "[18, 48] train loss: 0.403\n",
      "[18, 52] train loss: 0.331\n",
      "[18, 56] train loss: 0.388\n",
      "[18, 60] train loss: 0.384\n",
      "[18, 64] train loss: 0.460\n",
      "[18] val loss: 0.426\n",
      "[19, 4] train loss: 0.402\n",
      "[19, 8] train loss: 0.367\n",
      "[19, 12] train loss: 0.348\n",
      "[19, 16] train loss: 0.424\n",
      "[19, 20] train loss: 0.353\n",
      "[19, 24] train loss: 0.349\n",
      "[19, 28] train loss: 0.316\n",
      "[19, 32] train loss: 0.343\n",
      "[19, 36] train loss: 0.355\n",
      "[19, 40] train loss: 0.328\n",
      "[19, 44] train loss: 0.356\n",
      "[19, 48] train loss: 0.374\n",
      "[19, 52] train loss: 0.331\n",
      "[19, 56] train loss: 0.379\n",
      "[19, 60] train loss: 0.371\n",
      "[19, 64] train loss: 0.446\n",
      "[19] val loss: 0.438\n",
      "[20, 4] train loss: 0.390\n",
      "[20, 8] train loss: 0.356\n",
      "[20, 12] train loss: 0.338\n",
      "[20, 16] train loss: 0.418\n",
      "[20, 20] train loss: 0.339\n",
      "[20, 24] train loss: 0.341\n",
      "[20, 28] train loss: 0.317\n",
      "[20, 32] train loss: 0.338\n",
      "[20, 36] train loss: 0.353\n",
      "[20, 40] train loss: 0.323\n",
      "[20, 44] train loss: 0.351\n",
      "[20, 48] train loss: 0.378\n",
      "[20, 52] train loss: 0.340\n",
      "[20, 56] train loss: 0.388\n",
      "[20, 60] train loss: 0.408\n",
      "[20, 64] train loss: 0.447\n",
      "[20] val loss: 0.455\n",
      "Finished Training on 20 Epochs!\n",
      "Factory                  : \u001b[1mTrain all methods\u001b[0m\n",
      "Factory                  : [dataset] : Create Transformation \"I\" with events from all classes.\n",
      "                         : \n",
      "                         : Transformation, Variable selection : \n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "                         : Input : variable 'vars' <---> Output : variable 'vars'\n",
      "TFHandler_Factory        : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     vars:     9.3370     9.8030   [    -14.762     56.283 ]\n",
      "                         :     vars:     12.364     11.399   [    -16.029     66.050 ]\n",
      "                         :     vars:     15.676     12.851   [    -12.840     75.394 ]\n",
      "                         :     vars:     18.883     14.132   [    -13.639     83.501 ]\n",
      "                         :     vars:     22.008     15.098   [    -10.815     84.116 ]\n",
      "                         :     vars:     24.787     15.914   [    -11.474     85.153 ]\n",
      "                         :     vars:     26.547     16.419   [    -10.838     95.010 ]\n",
      "                         :     vars:     27.747     16.624   [    -8.3274     82.611 ]\n",
      "                         :     vars:     27.825     16.648   [    -11.920     87.044 ]\n",
      "                         :     vars:     26.959     16.532   [    -11.665     90.615 ]\n",
      "                         :     vars:     24.740     16.004   [    -10.313     84.211 ]\n",
      "                         :     vars:     21.942     15.298   [    -15.362     80.222 ]\n",
      "                         :     vars:     18.853     14.189   [    -16.368     85.406 ]\n",
      "                         :     vars:     15.602     12.925   [    -12.630     76.342 ]\n",
      "                         :     vars:     12.403     11.474   [    -20.051     66.783 ]\n",
      "                         :     vars:     9.4088     9.8240   [    -15.978     53.761 ]\n",
      "                         :     vars:     12.242     11.326   [    -18.319     65.590 ]\n",
      "                         :     vars:     15.922     12.998   [    -14.045     73.842 ]\n",
      "                         :     vars:     20.175     14.875   [    -15.603     89.858 ]\n",
      "                         :     vars:     24.421     16.332   [    -11.759     94.407 ]\n",
      "                         :     vars:     28.313     17.184   [    -9.1018     97.551 ]\n",
      "                         :     vars:     31.856     18.048   [    -5.9800     99.128 ]\n",
      "                         :     vars:     34.475     18.199   [    -8.7106     95.325 ]\n",
      "                         :     vars:     35.717     18.639   [    -12.591     104.08 ]\n",
      "                         :     vars:     35.757     18.620   [    -4.5598     101.09 ]\n",
      "                         :     vars:     34.274     18.374   [    -7.7766     101.66 ]\n",
      "                         :     vars:     31.776     18.193   [    -10.050     95.730 ]\n",
      "                         :     vars:     28.407     17.403   [    -11.599     108.72 ]\n",
      "                         :     vars:     24.316     16.400   [    -12.603     89.885 ]\n",
      "                         :     vars:     20.013     15.013   [    -15.992     87.268 ]\n",
      "                         :     vars:     16.046     13.152   [    -13.620     79.399 ]\n",
      "                         :     vars:     12.068     11.539   [    -15.132     69.030 ]\n",
      "                         :     vars:     15.164     12.637   [    -16.434     78.915 ]\n",
      "                         :     vars:     19.941     14.705   [    -13.968     91.164 ]\n",
      "                         :     vars:     24.999     16.651   [    -10.075     103.33 ]\n",
      "                         :     vars:     30.480     18.105   [    -9.5426     100.76 ]\n",
      "                         :     vars:     35.500     18.989   [    -8.0568     112.01 ]\n",
      "                         :     vars:     39.425     19.536   [    -4.2979     112.22 ]\n",
      "                         :     vars:     42.654     19.639   [    -6.5195     106.97 ]\n",
      "                         :     vars:     44.422     19.689   [   0.011513     106.50 ]\n",
      "                         :     vars:     44.521     19.840   [    -3.5169     113.62 ]\n",
      "                         :     vars:     42.797     19.936   [   -0.14370     110.34 ]\n",
      "                         :     vars:     39.793     19.827   [    -4.8557     108.34 ]\n",
      "                         :     vars:     35.319     19.376   [    -8.3389     115.22 ]\n",
      "                         :     vars:     30.193     18.463   [    -10.811     104.71 ]\n",
      "                         :     vars:     24.956     16.741   [    -12.507     101.61 ]\n",
      "                         :     vars:     19.850     15.033   [    -11.733     87.794 ]\n",
      "                         :     vars:     15.123     12.876   [    -13.249     73.256 ]\n",
      "                         :     vars:     18.163     13.973   [    -14.809     87.492 ]\n",
      "                         :     vars:     23.840     16.047   [    -10.229     86.669 ]\n",
      "                         :     vars:     29.953     18.048   [    -7.6564     100.84 ]\n",
      "                         :     vars:     36.260     19.362   [    -9.1860     108.82 ]\n",
      "                         :     vars:     42.258     19.983   [    -1.8440     117.29 ]\n",
      "                         :     vars:     47.426     20.192   [    -2.2545     117.86 ]\n",
      "                         :     vars:     51.157     20.025   [   -0.17809     115.06 ]\n",
      "                         :     vars:     53.279     19.942   [     4.8095     114.26 ]\n",
      "                         :     vars:     53.266     19.939   [     4.0980     118.47 ]\n",
      "                         :     vars:     51.224     20.219   [     2.6047     119.73 ]\n",
      "                         :     vars:     47.485     20.411   [    -2.7450     118.59 ]\n",
      "                         :     vars:     42.079     20.075   [    -2.3179     119.00 ]\n",
      "                         :     vars:     36.450     19.603   [    -5.0753     106.41 ]\n",
      "                         :     vars:     29.840     18.301   [    -8.1571     111.24 ]\n",
      "                         :     vars:     23.604     16.260   [    -10.984     87.940 ]\n",
      "                         :     vars:     18.125     14.121   [    -11.510     81.765 ]\n",
      "                         :     vars:     21.149     14.900   [    -12.037     86.914 ]\n",
      "                         :     vars:     27.549     17.224   [    -10.998     95.846 ]\n",
      "                         :     vars:     34.572     18.938   [    -10.139     116.51 ]\n",
      "                         :     vars:     42.175     20.364   [    -3.7267     116.34 ]\n",
      "                         :     vars:     48.887     20.419   [  -0.095978     126.92 ]\n",
      "                         :     vars:     54.868     20.071   [     4.7572     125.74 ]\n",
      "                         :     vars:     59.217     19.247   [     12.205     126.65 ]\n",
      "                         :     vars:     61.388     18.644   [     15.671     120.17 ]\n",
      "                         :     vars:     61.444     18.801   [     13.615     121.38 ]\n",
      "                         :     vars:     59.093     19.518   [     1.2093     127.75 ]\n",
      "                         :     vars:     54.888     20.277   [    0.83397     134.97 ]\n",
      "                         :     vars:     48.802     20.868   [   -0.81508     121.91 ]\n",
      "                         :     vars:     41.837     20.435   [    -5.3761     115.09 ]\n",
      "                         :     vars:     34.621     19.276   [    -6.1175     110.52 ]\n",
      "                         :     vars:     27.475     17.524   [    -11.196     101.94 ]\n",
      "                         :     vars:     20.900     15.236   [    -11.808     82.793 ]\n",
      "                         :     vars:     23.533     15.705   [    -12.131     87.359 ]\n",
      "                         :     vars:     30.782     17.965   [    -7.4272     103.24 ]\n",
      "                         :     vars:     38.787     19.683   [    -4.7278     105.36 ]\n",
      "                         :     vars:     46.819     20.368   [     1.7779     118.67 ]\n",
      "                         :     vars:     54.518     20.116   [     4.8446     127.08 ]\n",
      "                         :     vars:     60.969     19.066   [     5.4193     140.15 ]\n",
      "                         :     vars:     66.130     17.639   [     13.550     121.51 ]\n",
      "                         :     vars:     68.491     16.524   [     19.193     130.70 ]\n",
      "                         :     vars:     68.383     16.407   [     13.730     118.95 ]\n",
      "                         :     vars:     65.863     17.866   [     14.641     121.43 ]\n",
      "                         :     vars:     61.075     19.284   [     8.5449     124.52 ]\n",
      "                         :     vars:     54.412     20.522   [     5.4310     119.22 ]\n",
      "                         :     vars:     46.664     20.696   [   -0.61436     123.54 ]\n",
      "                         :     vars:     38.376     19.906   [    -4.8607     106.11 ]\n",
      "                         :     vars:     30.677     18.053   [    -9.4423     108.88 ]\n",
      "                         :     vars:     23.215     15.915   [    -10.138     88.278 ]\n",
      "                         :     vars:     25.214     16.214   [    -10.508     80.815 ]\n",
      "                         :     vars:     33.061     18.443   [    -6.3694     94.094 ]\n",
      "                         :     vars:     41.526     19.960   [    -3.7101     110.26 ]\n",
      "                         :     vars:     50.334     20.595   [   -0.45020     130.74 ]\n",
      "                         :     vars:     58.565     19.699   [     5.6830     125.60 ]\n",
      "                         :     vars:     65.657     17.651   [     1.9979     132.52 ]\n",
      "                         :     vars:     70.877     15.509   [     18.904     118.59 ]\n",
      "                         :     vars:     73.435     13.717   [     25.173     123.70 ]\n",
      "                         :     vars:     73.635     13.858   [     24.826     124.58 ]\n",
      "                         :     vars:     70.671     15.646   [     22.325     124.63 ]\n",
      "                         :     vars:     65.439     18.191   [     10.859     121.76 ]\n",
      "                         :     vars:     58.258     19.812   [     5.3046     131.89 ]\n",
      "                         :     vars:     50.068     20.512   [     1.4102     123.00 ]\n",
      "                         :     vars:     41.208     20.009   [    -5.3703     108.51 ]\n",
      "                         :     vars:     32.787     18.552   [    -6.8392     94.784 ]\n",
      "                         :     vars:     24.981     16.207   [    -12.095     87.776 ]\n",
      "                         :     vars:     26.075     16.466   [    -9.8785     81.779 ]\n",
      "                         :     vars:     34.180     18.597   [    -8.2136     96.116 ]\n",
      "                         :     vars:     43.147     20.019   [    -1.9735     112.21 ]\n",
      "                         :     vars:     52.149     20.451   [     3.8377     115.10 ]\n",
      "                         :     vars:     60.758     19.235   [     9.0260     121.07 ]\n",
      "                         :     vars:     68.183     16.973   [     21.483     123.11 ]\n",
      "                         :     vars:     73.475     14.014   [     26.487     123.18 ]\n",
      "                         :     vars:     76.283     11.732   [     38.230     118.57 ]\n",
      "                         :     vars:     76.315     11.645   [     32.507     132.19 ]\n",
      "                         :     vars:     73.172     13.918   [     22.511     122.90 ]\n",
      "                         :     vars:     67.906     17.037   [     17.483     120.54 ]\n",
      "                         :     vars:     60.468     19.351   [     10.102     123.59 ]\n",
      "                         :     vars:     51.616     20.193   [     1.4534     112.60 ]\n",
      "                         :     vars:     42.752     20.047   [    -1.2829     112.35 ]\n",
      "                         :     vars:     33.972     18.680   [    -9.0813     98.002 ]\n",
      "                         :     vars:     25.980     16.444   [    -9.3348     82.997 ]\n",
      "                         :     vars:     26.146     16.551   [    -12.038     82.183 ]\n",
      "                         :     vars:     34.304     18.752   [    -7.3177     97.272 ]\n",
      "                         :     vars:     42.985     20.043   [    -6.0120     104.61 ]\n",
      "                         :     vars:     51.929     20.264   [     5.1780     119.15 ]\n",
      "                         :     vars:     60.552     19.432   [     11.628     120.50 ]\n",
      "                         :     vars:     68.220     17.052   [     13.008     121.43 ]\n",
      "                         :     vars:     73.378     13.846   [     27.457     121.96 ]\n",
      "                         :     vars:     76.137     11.705   [     35.879     115.37 ]\n",
      "                         :     vars:     76.114     11.754   [     35.014     122.67 ]\n",
      "                         :     vars:     73.080     13.819   [     28.416     118.52 ]\n",
      "                         :     vars:     67.703     16.856   [     19.112     121.39 ]\n",
      "                         :     vars:     60.141     19.109   [     13.324     126.82 ]\n",
      "                         :     vars:     51.856     20.308   [     1.3409     116.35 ]\n",
      "                         :     vars:     42.493     20.021   [    -2.5509     114.04 ]\n",
      "                         :     vars:     33.805     18.486   [    -8.8778     94.192 ]\n",
      "                         :     vars:     25.737     16.403   [    -12.888     86.560 ]\n",
      "                         :     vars:     25.204     16.168   [    -12.137     87.534 ]\n",
      "                         :     vars:     33.024     18.710   [    -6.5512     106.38 ]\n",
      "                         :     vars:     41.604     20.173   [    -7.3639     112.64 ]\n",
      "                         :     vars:     50.236     20.552   [    0.53189     121.89 ]\n",
      "                         :     vars:     58.496     19.792   [     6.0769     119.49 ]\n",
      "                         :     vars:     65.623     17.843   [     13.422     125.37 ]\n",
      "                         :     vars:     70.638     15.647   [     21.825     130.22 ]\n",
      "                         :     vars:     73.557     13.781   [     29.542     123.54 ]\n",
      "                         :     vars:     73.569     13.756   [     21.070     119.89 ]\n",
      "                         :     vars:     70.467     15.380   [     23.946     124.56 ]\n",
      "                         :     vars:     65.058     17.789   [     15.253     129.53 ]\n",
      "                         :     vars:     58.174     19.551   [     9.2374     120.84 ]\n",
      "                         :     vars:     49.766     20.358   [     3.9729     112.45 ]\n",
      "                         :     vars:     41.139     19.907   [    -4.0912     116.77 ]\n",
      "                         :     vars:     32.586     18.264   [    -10.645     106.92 ]\n",
      "                         :     vars:     24.787     16.161   [    -9.8683     89.159 ]\n",
      "                         :     vars:     23.550     15.907   [    -9.5390     87.833 ]\n",
      "                         :     vars:     30.728     18.220   [    -11.203     99.423 ]\n",
      "                         :     vars:     38.509     19.830   [    -9.9137     111.75 ]\n",
      "                         :     vars:     46.619     20.597   [    -2.1897     112.96 ]\n",
      "                         :     vars:     54.448     20.555   [     4.3699     124.71 ]\n",
      "                         :     vars:     61.048     19.371   [     5.1294     126.17 ]\n",
      "                         :     vars:     65.838     17.791   [     13.622     121.77 ]\n",
      "                         :     vars:     68.187     16.524   [     24.059     116.02 ]\n",
      "                         :     vars:     68.324     16.394   [     20.645     126.61 ]\n",
      "                         :     vars:     65.571     17.362   [     18.785     129.91 ]\n",
      "                         :     vars:     60.707     18.929   [     10.380     138.49 ]\n",
      "                         :     vars:     54.252     20.109   [     3.0040     122.94 ]\n",
      "                         :     vars:     46.324     20.332   [    -1.4133     118.63 ]\n",
      "                         :     vars:     38.199     19.700   [    -7.7154     116.28 ]\n",
      "                         :     vars:     30.335     18.011   [    -9.5865     96.809 ]\n",
      "                         :     vars:     23.036     15.607   [    -9.3259     79.014 ]\n",
      "                         :     vars:     20.896     15.140   [    -11.454     91.225 ]\n",
      "                         :     vars:     27.640     17.585   [    -11.678     104.71 ]\n",
      "                         :     vars:     34.490     19.172   [    -7.9235     114.83 ]\n",
      "                         :     vars:     42.005     20.570   [    -1.7551     119.64 ]\n",
      "                         :     vars:     48.892     20.768   [    0.43672     137.29 ]\n",
      "                         :     vars:     54.809     20.368   [     3.2044     128.69 ]\n",
      "                         :     vars:     58.962     19.394   [     7.9963     118.46 ]\n",
      "                         :     vars:     61.288     18.784   [     13.802     120.82 ]\n",
      "                         :     vars:     61.204     18.700   [     14.798     122.84 ]\n",
      "                         :     vars:     58.884     19.250   [     6.5120     128.74 ]\n",
      "                         :     vars:     54.409     19.864   [   -0.96757     125.34 ]\n",
      "                         :     vars:     48.399     20.355   [    0.89506     120.72 ]\n",
      "                         :     vars:     41.525     20.205   [   -0.52817     117.04 ]\n",
      "                         :     vars:     34.257     18.955   [    -8.2549     107.93 ]\n",
      "                         :     vars:     27.168     17.104   [    -13.363     108.75 ]\n",
      "                         :     vars:     20.674     14.881   [    -11.022     93.646 ]\n",
      "                         :     vars:     18.249     14.271   [    -13.126     81.164 ]\n",
      "                         :     vars:     23.824     16.508   [    -10.226     89.119 ]\n",
      "                         :     vars:     30.002     18.164   [    -8.6649     110.18 ]\n",
      "                         :     vars:     36.277     19.868   [    -6.1553     111.99 ]\n",
      "                         :     vars:     42.313     20.434   [    -6.8448     116.05 ]\n",
      "                         :     vars:     47.564     20.569   [    -7.6792     113.62 ]\n",
      "                         :     vars:     51.134     20.409   [     1.9458     123.48 ]\n",
      "                         :     vars:     53.035     20.031   [   -0.53965     112.46 ]\n",
      "                         :     vars:     52.865     19.786   [     2.6197     119.77 ]\n",
      "                         :     vars:     51.002     19.996   [     5.7956     116.69 ]\n",
      "                         :     vars:     47.055     20.144   [    0.13718     126.10 ]\n",
      "                         :     vars:     42.015     20.065   [    -8.0103     127.21 ]\n",
      "                         :     vars:     35.746     19.272   [    -11.423     113.64 ]\n",
      "                         :     vars:     29.730     17.871   [    -6.4468     103.12 ]\n",
      "                         :     vars:     23.411     16.060   [    -13.132     93.278 ]\n",
      "                         :     vars:     17.854     13.736   [    -14.274     74.369 ]\n",
      "                         :     vars:     15.237     12.928   [    -11.894     72.579 ]\n",
      "                         :     vars:     19.882     15.035   [    -14.289     88.289 ]\n",
      "                         :     vars:     25.195     16.855   [    -13.187     94.092 ]\n",
      "                         :     vars:     30.351     18.201   [    -9.4310     108.00 ]\n",
      "                         :     vars:     35.161     19.239   [    -6.7272     102.65 ]\n",
      "                         :     vars:     39.429     19.746   [    -6.0050     111.05 ]\n",
      "                         :     vars:     42.596     19.849   [    -1.1404     116.68 ]\n",
      "                         :     vars:     44.100     19.697   [     1.0035     112.60 ]\n",
      "                         :     vars:     44.072     19.662   [   -0.29242     109.31 ]\n",
      "                         :     vars:     42.313     19.645   [    -6.1163     109.12 ]\n",
      "                         :     vars:     39.094     19.495   [    -3.9868     108.06 ]\n",
      "                         :     vars:     34.914     18.969   [    -9.7668     105.20 ]\n",
      "                         :     vars:     29.886     17.881   [    -7.7143     103.00 ]\n",
      "                         :     vars:     24.618     16.405   [    -10.578     99.103 ]\n",
      "                         :     vars:     19.525     14.514   [    -11.324     97.412 ]\n",
      "                         :     vars:     14.835     12.597   [    -15.356     75.934 ]\n",
      "                         :     vars:     12.210     11.441   [    -13.508     64.487 ]\n",
      "                         :     vars:     16.006     13.292   [    -12.635     80.212 ]\n",
      "                         :     vars:     20.146     15.195   [    -12.868     85.787 ]\n",
      "                         :     vars:     24.468     16.631   [    -10.856     96.788 ]\n",
      "                         :     vars:     28.432     17.510   [    -7.5773     100.94 ]\n",
      "                         :     vars:     31.802     18.086   [    -6.6776     100.60 ]\n",
      "                         :     vars:     34.277     18.444   [    -6.5148     94.938 ]\n",
      "                         :     vars:     35.374     18.464   [    -5.9679     94.650 ]\n",
      "                         :     vars:     35.663     18.563   [    -5.1483     92.195 ]\n",
      "                         :     vars:     33.905     18.493   [    -11.243     99.145 ]\n",
      "                         :     vars:     31.557     17.785   [    -4.9010     99.110 ]\n",
      "                         :     vars:     28.062     17.326   [    -8.4174     103.12 ]\n",
      "                         :     vars:     24.065     16.024   [    -8.7342     87.937 ]\n",
      "                         :     vars:     19.725     14.670   [    -12.085     87.484 ]\n",
      "                         :     vars:     15.720     12.968   [    -15.118     76.855 ]\n",
      "                         :     vars:     11.919     11.132   [    -14.158     64.638 ]\n",
      "                         :     vars:     9.4707     9.9254   [    -14.312     58.515 ]\n",
      "                         :     vars:     12.381     11.654   [    -14.460     72.171 ]\n",
      "                         :     vars:     15.590     13.029   [    -12.826     77.558 ]\n",
      "                         :     vars:     19.001     14.355   [    -11.122     83.093 ]\n",
      "                         :     vars:     22.067     15.482   [    -12.809     79.924 ]\n",
      "                         :     vars:     24.721     16.027   [    -8.8781     85.848 ]\n",
      "                         :     vars:     26.723     16.498   [    -8.7378     93.698 ]\n",
      "                         :     vars:     27.535     16.692   [    -10.534     89.221 ]\n",
      "                         :     vars:     27.386     16.335   [    -11.429     85.310 ]\n",
      "                         :     vars:     26.345     16.498   [    -10.340     89.988 ]\n",
      "                         :     vars:     24.418     15.860   [    -12.316     89.535 ]\n",
      "                         :     vars:     21.646     15.036   [    -13.559     83.997 ]\n",
      "                         :     vars:     18.568     13.971   [    -17.301     76.427 ]\n",
      "                         :     vars:     15.386     12.834   [    -18.696     72.151 ]\n",
      "                         :     vars:     12.099     11.210   [    -16.116     73.273 ]\n",
      "                         :     vars:     9.2775     9.6773   [    -15.707     60.965 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 256 input variables and 0 target values would require 32640 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n",
      "                         : Ranking input variables (method unspecific)...\n",
      "IdTransformation         : Ranking result (top variable is best ranked)\n",
      "                         : ------------------------------\n",
      "                         : Rank : Variable  : Separation\n",
      "                         : ------------------------------\n",
      "                         :    1 : [5]       : 9.836e-03\n",
      "                         :    2 : [199]     : 9.804e-03\n",
      "                         :    3 : [249]     : 9.679e-03\n",
      "                         :    4 : [129]     : 9.439e-03\n",
      "                         :    5 : [81]      : 9.293e-03\n",
      "                         :    6 : [8]       : 8.982e-03\n",
      "                         :    7 : [23]      : 8.939e-03\n",
      "                         :    8 : [234]     : 8.937e-03\n",
      "                         :    9 : [96]      : 8.894e-03\n",
      "                         :   10 : [244]     : 8.688e-03\n",
      "                         :   11 : [113]     : 8.580e-03\n",
      "                         :   12 : [247]     : 8.542e-03\n",
      "                         :   13 : [97]      : 8.529e-03\n",
      "                         :   14 : [99]      : 8.491e-03\n",
      "                         :   15 : [7]       : 8.474e-03\n",
      "                         :   16 : [128]     : 8.331e-03\n",
      "                         :   17 : [25]      : 8.310e-03\n",
      "                         :   18 : [248]     : 8.279e-03\n",
      "                         :   19 : [144]     : 8.276e-03\n",
      "                         :   20 : [71]      : 8.264e-03\n",
      "                         :   21 : [127]     : 8.257e-03\n",
      "                         :   22 : [114]     : 8.201e-03\n",
      "                         :   23 : [9]       : 8.081e-03\n",
      "                         :   24 : [160]     : 8.052e-03\n",
      "                         :   25 : [158]     : 8.004e-03\n",
      "                         :   26 : [149]     : 7.971e-03\n",
      "                         :   27 : [39]      : 7.829e-03\n",
      "                         :   28 : [62]      : 7.821e-03\n",
      "                         :   29 : [115]     : 7.809e-03\n",
      "                         :   30 : [202]     : 7.801e-03\n",
      "                         :   31 : [112]     : 7.774e-03\n",
      "                         :   32 : [176]     : 7.678e-03\n",
      "                         :   33 : [142]     : 7.651e-03\n",
      "                         :   34 : [95]      : 7.647e-03\n",
      "                         :   35 : [22]      : 7.638e-03\n",
      "                         :   36 : [58]      : 7.629e-03\n",
      "                         :   37 : [24]      : 7.622e-03\n",
      "                         :   38 : [177]     : 7.543e-03\n",
      "                         :   39 : [246]     : 7.489e-03\n",
      "                         :   40 : [80]      : 7.468e-03\n",
      "                         :   41 : [219]     : 7.459e-03\n",
      "                         :   42 : [54]      : 7.412e-03\n",
      "                         :   43 : [110]     : 7.399e-03\n",
      "                         :   44 : [48]      : 7.377e-03\n",
      "                         :   45 : [148]     : 7.277e-03\n",
      "                         :   46 : [143]     : 7.274e-03\n",
      "                         :   47 : [116]     : 7.250e-03\n",
      "                         :   48 : [55]      : 7.202e-03\n",
      "                         :   49 : [184]     : 7.170e-03\n",
      "                         :   50 : [231]     : 7.128e-03\n",
      "                         :   51 : [232]     : 7.099e-03\n",
      "                         :   52 : [74]      : 7.083e-03\n",
      "                         :   53 : [12]      : 7.029e-03\n",
      "                         :   54 : [216]     : 7.016e-03\n",
      "                         :   55 : [131]     : 7.005e-03\n",
      "                         :   56 : [10]      : 6.999e-03\n",
      "                         :   57 : [130]     : 6.979e-03\n",
      "                         :   58 : [56]      : 6.952e-03\n",
      "                         :   59 : [11]      : 6.911e-03\n",
      "                         :   60 : [41]      : 6.901e-03\n",
      "                         :   61 : [45]      : 6.892e-03\n",
      "                         :   62 : [57]      : 6.843e-03\n",
      "                         :   63 : [111]     : 6.839e-03\n",
      "                         :   64 : [122]     : 6.807e-03\n",
      "                         :   65 : [107]     : 6.776e-03\n",
      "                         :   66 : [65]      : 6.760e-03\n",
      "                         :   67 : [163]     : 6.697e-03\n",
      "                         :   68 : [192]     : 6.690e-03\n",
      "                         :   69 : [193]     : 6.685e-03\n",
      "                         :   70 : [145]     : 6.670e-03\n",
      "                         :   71 : [253]     : 6.667e-03\n",
      "                         :   72 : [64]      : 6.647e-03\n",
      "                         :   73 : [89]      : 6.619e-03\n",
      "                         :   74 : [53]      : 6.606e-03\n",
      "                         :   75 : [20]      : 6.606e-03\n",
      "                         :   76 : [26]      : 6.599e-03\n",
      "                         :   77 : [2]       : 6.562e-03\n",
      "                         :   78 : [126]     : 6.542e-03\n",
      "                         :   79 : [86]      : 6.533e-03\n",
      "                         :   80 : [40]      : 6.514e-03\n",
      "                         :   81 : [14]      : 6.448e-03\n",
      "                         :   82 : [250]     : 6.423e-03\n",
      "                         :   83 : [233]     : 6.414e-03\n",
      "                         :   84 : [140]     : 6.405e-03\n",
      "                         :   85 : [43]      : 6.389e-03\n",
      "                         :   86 : [161]     : 6.379e-03\n",
      "                         :   87 : [51]      : 6.337e-03\n",
      "                         :   88 : [91]      : 6.274e-03\n",
      "                         :   89 : [186]     : 6.263e-03\n",
      "                         :   90 : [124]     : 6.257e-03\n",
      "                         :   91 : [218]     : 6.228e-03\n",
      "                         :   92 : [123]     : 6.214e-03\n",
      "                         :   93 : [36]      : 6.195e-03\n",
      "                         :   94 : [117]     : 6.181e-03\n",
      "                         :   95 : [27]      : 6.165e-03\n",
      "                         :   96 : [66]      : 6.136e-03\n",
      "                         :   97 : [230]     : 6.130e-03\n",
      "                         :   98 : [125]     : 6.128e-03\n",
      "                         :   99 : [168]     : 6.112e-03\n",
      "                         :  100 : [185]     : 6.080e-03\n",
      "                         :  101 : [194]     : 6.074e-03\n",
      "                         :  102 : [201]     : 6.057e-03\n",
      "                         :  103 : [154]     : 6.045e-03\n",
      "                         :  104 : [6]       : 6.022e-03\n",
      "                         :  105 : [200]     : 6.004e-03\n",
      "                         :  106 : [203]     : 5.996e-03\n",
      "                         :  107 : [217]     : 5.984e-03\n",
      "                         :  108 : [207]     : 5.967e-03\n",
      "                         :  109 : [221]     : 5.965e-03\n",
      "                         :  110 : [182]     : 5.955e-03\n",
      "                         :  111 : [1]       : 5.953e-03\n",
      "                         :  112 : [42]      : 5.952e-03\n",
      "                         :  113 : [73]      : 5.944e-03\n",
      "                         :  114 : [4]       : 5.910e-03\n",
      "                         :  115 : [214]     : 5.903e-03\n",
      "                         :  116 : [146]     : 5.880e-03\n",
      "                         :  117 : [50]      : 5.869e-03\n",
      "                         :  118 : [88]      : 5.866e-03\n",
      "                         :  119 : [251]     : 5.856e-03\n",
      "                         :  120 : [98]      : 5.843e-03\n",
      "                         :  121 : [134]     : 5.806e-03\n",
      "                         :  122 : [79]      : 5.804e-03\n",
      "                         :  123 : [157]     : 5.765e-03\n",
      "                         :  124 : [106]     : 5.754e-03\n",
      "                         :  125 : [147]     : 5.738e-03\n",
      "                         :  126 : [83]      : 5.696e-03\n",
      "                         :  127 : [166]     : 5.684e-03\n",
      "                         :  128 : [132]     : 5.674e-03\n",
      "                         :  129 : [67]      : 5.655e-03\n",
      "                         :  130 : [173]     : 5.593e-03\n",
      "                         :  131 : [17]      : 5.590e-03\n",
      "                         :  132 : [229]     : 5.586e-03\n",
      "                         :  133 : [21]      : 5.576e-03\n",
      "                         :  134 : [191]     : 5.564e-03\n",
      "                         :  135 : [109]     : 5.563e-03\n",
      "                         :  136 : [120]     : 5.548e-03\n",
      "                         :  137 : [235]     : 5.533e-03\n",
      "                         :  138 : [78]      : 5.520e-03\n",
      "                         :  139 : [38]      : 5.458e-03\n",
      "                         :  140 : [82]      : 5.446e-03\n",
      "                         :  141 : [37]      : 5.435e-03\n",
      "                         :  142 : [162]     : 5.431e-03\n",
      "                         :  143 : [183]     : 5.430e-03\n",
      "                         :  144 : [29]      : 5.419e-03\n",
      "                         :  145 : [239]     : 5.412e-03\n",
      "                         :  146 : [196]     : 5.411e-03\n",
      "                         :  147 : [101]     : 5.402e-03\n",
      "                         :  148 : [31]      : 5.390e-03\n",
      "                         :  149 : [137]     : 5.374e-03\n",
      "                         :  150 : [70]      : 5.351e-03\n",
      "                         :  151 : [77]      : 5.349e-03\n",
      "                         :  152 : [136]     : 5.318e-03\n",
      "                         :  153 : [141]     : 5.311e-03\n",
      "                         :  154 : [92]      : 5.308e-03\n",
      "                         :  155 : [236]     : 5.307e-03\n",
      "                         :  156 : [208]     : 5.291e-03\n",
      "                         :  157 : [164]     : 5.291e-03\n",
      "                         :  158 : [19]      : 5.288e-03\n",
      "                         :  159 : [215]     : 5.267e-03\n",
      "                         :  160 : [46]      : 5.249e-03\n",
      "                         :  161 : [159]     : 5.244e-03\n",
      "                         :  162 : [119]     : 5.217e-03\n",
      "                         :  163 : [75]      : 5.196e-03\n",
      "                         :  164 : [87]      : 5.195e-03\n",
      "                         :  165 : [93]      : 5.164e-03\n",
      "                         :  166 : [30]      : 5.158e-03\n",
      "                         :  167 : [210]     : 5.137e-03\n",
      "                         :  168 : [28]      : 5.115e-03\n",
      "                         :  169 : [139]     : 5.115e-03\n",
      "                         :  170 : [209]     : 5.100e-03\n",
      "                         :  171 : [169]     : 5.074e-03\n",
      "                         :  172 : [94]      : 5.067e-03\n",
      "                         :  173 : [213]     : 5.058e-03\n",
      "                         :  174 : [0]       : 5.048e-03\n",
      "                         :  175 : [104]     : 5.047e-03\n",
      "                         :  176 : [59]      : 5.031e-03\n",
      "                         :  177 : [61]      : 5.010e-03\n",
      "                         :  178 : [223]     : 4.994e-03\n",
      "                         :  179 : [167]     : 4.991e-03\n",
      "                         :  180 : [178]     : 4.984e-03\n",
      "                         :  181 : [220]     : 4.969e-03\n",
      "                         :  182 : [172]     : 4.969e-03\n",
      "                         :  183 : [155]     : 4.931e-03\n",
      "                         :  184 : [237]     : 4.919e-03\n",
      "                         :  185 : [206]     : 4.900e-03\n",
      "                         :  186 : [13]      : 4.898e-03\n",
      "                         :  187 : [32]      : 4.865e-03\n",
      "                         :  188 : [181]     : 4.855e-03\n",
      "                         :  189 : [198]     : 4.784e-03\n",
      "                         :  190 : [224]     : 4.780e-03\n",
      "                         :  191 : [16]      : 4.779e-03\n",
      "                         :  192 : [69]      : 4.777e-03\n",
      "                         :  193 : [63]      : 4.774e-03\n",
      "                         :  194 : [49]      : 4.758e-03\n",
      "                         :  195 : [255]     : 4.701e-03\n",
      "                         :  196 : [18]      : 4.678e-03\n",
      "                         :  197 : [135]     : 4.660e-03\n",
      "                         :  198 : [90]      : 4.656e-03\n",
      "                         :  199 : [102]     : 4.633e-03\n",
      "                         :  200 : [72]      : 4.624e-03\n",
      "                         :  201 : [188]     : 4.612e-03\n",
      "                         :  202 : [34]      : 4.610e-03\n",
      "                         :  203 : [212]     : 4.604e-03\n",
      "                         :  204 : [153]     : 4.601e-03\n",
      "                         :  205 : [3]       : 4.576e-03\n",
      "                         :  206 : [225]     : 4.546e-03\n",
      "                         :  207 : [240]     : 4.480e-03\n",
      "                         :  208 : [245]     : 4.475e-03\n",
      "                         :  209 : [254]     : 4.469e-03\n",
      "                         :  210 : [52]      : 4.469e-03\n",
      "                         :  211 : [103]     : 4.464e-03\n",
      "                         :  212 : [165]     : 4.439e-03\n",
      "                         :  213 : [44]      : 4.388e-03\n",
      "                         :  214 : [76]      : 4.345e-03\n",
      "                         :  215 : [156]     : 4.332e-03\n",
      "                         :  216 : [222]     : 4.330e-03\n",
      "                         :  217 : [170]     : 4.292e-03\n",
      "                         :  218 : [175]     : 4.285e-03\n",
      "                         :  219 : [211]     : 4.281e-03\n",
      "                         :  220 : [84]      : 4.259e-03\n",
      "                         :  221 : [35]      : 4.198e-03\n",
      "                         :  222 : [100]     : 4.197e-03\n",
      "                         :  223 : [60]      : 4.191e-03\n",
      "                         :  224 : [190]     : 4.163e-03\n",
      "                         :  225 : [187]     : 4.151e-03\n",
      "                         :  226 : [180]     : 4.143e-03\n",
      "                         :  227 : [189]     : 4.100e-03\n",
      "                         :  228 : [238]     : 4.081e-03\n",
      "                         :  229 : [242]     : 4.042e-03\n",
      "                         :  230 : [68]      : 3.991e-03\n",
      "                         :  231 : [205]     : 3.984e-03\n",
      "                         :  232 : [195]     : 3.959e-03\n",
      "                         :  233 : [33]      : 3.910e-03\n",
      "                         :  234 : [227]     : 3.894e-03\n",
      "                         :  235 : [105]     : 3.860e-03\n",
      "                         :  236 : [179]     : 3.818e-03\n",
      "                         :  237 : [151]     : 3.777e-03\n",
      "                         :  238 : [133]     : 3.763e-03\n",
      "                         :  239 : [152]     : 3.713e-03\n",
      "                         :  240 : [171]     : 3.688e-03\n",
      "                         :  241 : [243]     : 3.684e-03\n",
      "                         :  242 : [118]     : 3.646e-03\n",
      "                         :  243 : [108]     : 3.598e-03\n",
      "                         :  244 : [228]     : 3.535e-03\n",
      "                         :  245 : [174]     : 3.516e-03\n",
      "                         :  246 : [85]      : 3.484e-03\n",
      "                         :  247 : [138]     : 3.460e-03\n",
      "                         :  248 : [197]     : 3.399e-03\n",
      "                         :  249 : [150]     : 3.357e-03\n",
      "                         :  250 : [47]      : 3.317e-03\n",
      "                         :  251 : [252]     : 3.184e-03\n",
      "                         :  252 : [204]     : 3.136e-03\n",
      "                         :  253 : [226]     : 3.105e-03\n",
      "                         :  254 : [15]      : 3.057e-03\n",
      "                         :  255 : [121]     : 2.708e-03\n",
      "                         :  256 : [241]     : 2.564e-03\n",
      "                         : ------------------------------\n",
      "Factory                  : Train method: BDT for Classification\n",
      "                         : \n",
      "BDT                      : #events: (reweighted) sig: 4000 bkg: 4000\n",
      "                         : #events: (unweighted) sig: 4000 bkg: 4000\n",
      "                         : Training 400 Decision Trees ... patience please\n",
      "                         : Elapsed time for training with 8000 events: 38.2 sec         \n",
      "BDT                      : [dataset] : Evaluation of BDT on training sample (8000 events)\n",
      "                         : Elapsed time for evaluation of 8000 events: 0.168 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_BDT.class.C\u001b[0m\n",
      "                         : CNN_ClassificationOutput.root:/dataset/Method_BDT/BDT\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: TMVA_DNN_CPU for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU using MT,  nthreads = 1\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 8  Input = ( 1, 1, 256 )  Batch size = 100  Loss function = C\n",
      "\tLayer 0\t DENSE Layer: \t ( Input =   256 , Width =   100 ) \tOutput = (  1 ,   100 ,   100 ) \t Activation Function = Relu\n",
      "\tLayer 1\t BATCH NORM Layer: \t Input/Output = ( 100 , 100 , 1 ) \t Norm dim =   100\t axis = -1\n",
      "\n",
      "\tLayer 2\t DENSE Layer: \t ( Input =   100 , Width =   100 ) \tOutput = (  1 ,   100 ,   100 ) \t Activation Function = Relu\n",
      "\tLayer 3\t BATCH NORM Layer: \t Input/Output = ( 100 , 100 , 1 ) \t Norm dim =   100\t axis = -1\n",
      "\n",
      "\tLayer 4\t DENSE Layer: \t ( Input =   100 , Width =   100 ) \tOutput = (  1 ,   100 ,   100 ) \t Activation Function = Relu\n",
      "\tLayer 5\t BATCH NORM Layer: \t Input/Output = ( 100 , 100 , 1 ) \t Norm dim =   100\t axis = -1\n",
      "\n",
      "\tLayer 6\t DENSE Layer: \t ( Input =   100 , Width =   100 ) \tOutput = (  1 ,   100 ,   100 ) \t Activation Function = Relu\n",
      "\tLayer 7\t DENSE Layer: \t ( Input =   100 , Width =     1 ) \tOutput = (  1 ,   100 ,     1 ) \t Activation Function = Identity\n",
      "                         : Using 6400 events for training and 1600 for testing\n",
      "                         : Compute initial loss  on the validation data \n",
      "                         : Training phase 1 of 1:  Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = inf\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Val. Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :    Start epoch iteration ...\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |     0.754836    0.821751    0.170229  0.00851326     39575.6           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.606267    0.634486    0.113199  0.00782377     60735.3           0\n",
      "                         :          3 |     0.485724    0.638044    0.109418  0.00730165     62673.5           1\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |     0.421041    0.567613    0.110192  0.00780964       62511           0\n",
      "                         :          5 |     0.374399         inf    0.108656  0.00721197       63089           1\n",
      "                         :          6 |     0.335646         inf    0.112327  0.00714388     60846.1           2\n",
      "                         :          7 |     0.325075         inf    0.109561  0.00744592     62674.5           3\n",
      "                         :          8 |     0.309072         inf    0.108836  0.00723579       62992           4\n",
      "                         :          9 |     0.296541         inf     0.10921  0.00725605     62773.5           5\n",
      "                         :         10 |     0.278261         inf    0.113657  0.00726309       60154           6\n",
      "                         : \n",
      "                         : Elapsed time for training with 8000 events: 1.28 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 100\n",
      "                         : \n",
      "TMVA_DNN_CPU             : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (8000 events)\n",
      "                         : Elapsed time for evaluation of 8000 events: 0.0364 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: TMVA_CNN_CPU for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU using MT,  nthreads = 1\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 7  Input = ( 1, 16, 16 )  Batch size = 100  Loss function = C\n",
      "\tLayer 0\t CONV LAYER: \t( W = 16 ,  H = 16 ,  D = 10 ) \t Filter ( W = 3 ,  H = 3 ) \tOutput = ( 100 , 10 , 10 , 256 ) \t Activation Function = Relu\n",
      "\tLayer 1\t BATCH NORM Layer: \t Input/Output = ( 10 , 256 , 100 ) \t Norm dim =    10\t axis = 1\n",
      "\n",
      "\tLayer 2\t CONV LAYER: \t( W = 16 ,  H = 16 ,  D = 10 ) \t Filter ( W = 3 ,  H = 3 ) \tOutput = ( 100 , 10 , 10 , 256 ) \t Activation Function = Relu\n",
      "\tLayer 3\t POOL Layer: \t( W = 15 ,  H = 15 ,  D = 10 ) \t Filter ( W = 2 ,  H = 2 ) \tOutput = ( 100 , 10 , 10 , 225 ) \n",
      "\tLayer 4\t RESHAPE Layer \t Input = ( 10 , 15 , 15 ) \tOutput = ( 1 , 100 , 2250 ) \n",
      "\tLayer 5\t DENSE Layer: \t ( Input =  2250 , Width =   100 ) \tOutput = (  1 ,   100 ,   100 ) \t Activation Function = Relu\n",
      "\tLayer 6\t DENSE Layer: \t ( Input =   100 , Width =     1 ) \tOutput = (  1 ,   100 ,     1 ) \t Activation Function = Identity\n",
      "                         : Using 6400 events for training and 1600 for testing\n",
      "                         : Compute initial loss  on the validation data \n",
      "                         : Training phase 1 of 1:  Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = inf\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Val. Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :    Start epoch iteration ...\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |      1.33545    0.687319     3.90147    0.331895     1792.93           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |      0.65533    0.653053     3.34285     0.33511     2127.84           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.604258    0.574647     3.44283    0.336498     2060.31           0\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |     0.525914    0.511196     3.48923    0.337264     2030.48           0\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |     0.477175    0.452382     3.27099    0.336633     2181.06           0\n",
      "                         :          6 Minimum Test error found - save the configuration \n",
      "                         :          6 |     0.459496    0.427442     3.27259    0.342783     2184.44           0\n",
      "                         :          7 |     0.401296    0.430585     3.25808    0.330446     2186.06           1\n",
      "                         :          8 Minimum Test error found - save the configuration \n",
      "                         :          8 |     0.387148    0.397241     3.27844    0.336961     2175.78           0\n",
      "                         :          9 |     0.372734    0.419923      3.2674    0.334352     2182.03           1\n",
      "                         :         10 |     0.353854    0.423802      3.3156    0.333942     2146.46           2\n",
      "                         :         11 |     0.374522    0.407339      3.4507    0.336132     2054.86           3\n",
      "                         :         12 |     0.366816    0.402397     4.45235     0.33307     1553.67           4\n",
      "                         :         13 |     0.330173    0.400078     4.92171    0.333133     1394.77           5\n",
      "                         :         14 |      0.35517    0.408318     4.84868    0.339427      1419.3           6\n",
      "                         : \n",
      "                         : Elapsed time for training with 8000 events: 51.9 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 100\n",
      "                         : \n",
      "TMVA_CNN_CPU             : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (8000 events)\n",
      "                         : Elapsed time for evaluation of 8000 events: 1.72 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: PyTorch for Classification\n",
      "                         : \n",
      "                         : \n",
      "                         : \u001b[1m================================================================\u001b[0m\n",
      "                         : \u001b[1mH e l p   f o r   M V A   m e t h o d   [ PyTorch ] :\u001b[0m\n",
      "                         : \n",
      "                         : PyTorch is a scientific computing package supporting\n",
      "                         : automatic differentiation. This method wraps the training\n",
      "                         : and predictions steps of the PyTorch Python package for\n",
      "                         : TMVA, so that dataloading, preprocessing and evaluation\n",
      "                         : can be done within the TMVA system. To use this PyTorch\n",
      "                         : interface, you need to generatea model with PyTorch first.\n",
      "                         : Then, this model can be loaded and trained in TMVA.\n",
      "                         : \n",
      "                         : \n",
      "                         : <Suppress this message by specifying \"!H\" in the booking option>\n",
      "                         : \u001b[1m================================================================\u001b[0m\n",
      "                         : \n",
      "                         : Split TMVA training data in 6400 training events and 1600 validation events\n",
      "                         : Print Training Model Architecture\n",
      "                         : Option SaveBestOnly: Only model weights with smallest validation loss will be stored\n",
      "                         : Elapsed time for training with 8000 events: 32.7 sec         \n",
      "PyTorch                  : [dataset] : Evaluation of PyTorch on training sample (8000 events)\n",
      "                         : Elapsed time for evaluation of 8000 events: 0.433 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "                         : Ranking input variables (method specific)...\n",
      "BDT                      : Ranking result (top variable is best ranked)\n",
      "                         : --------------------------------------\n",
      "                         : Rank : Variable  : Variable Importance\n",
      "                         : --------------------------------------\n",
      "                         :    1 : vars      : 1.079e-02\n",
      "                         :    2 : vars      : 1.077e-02\n",
      "                         :    3 : vars      : 1.077e-02\n",
      "                         :    4 : vars      : 1.031e-02\n",
      "                         :    5 : vars      : 1.010e-02\n",
      "                         :    6 : vars      : 9.967e-03\n",
      "                         :    7 : vars      : 9.801e-03\n",
      "                         :    8 : vars      : 9.606e-03\n",
      "                         :    9 : vars      : 9.556e-03\n",
      "                         :   10 : vars      : 9.483e-03\n",
      "                         :   11 : vars      : 9.400e-03\n",
      "                         :   12 : vars      : 9.259e-03\n",
      "                         :   13 : vars      : 9.120e-03\n",
      "                         :   14 : vars      : 8.942e-03\n",
      "                         :   15 : vars      : 8.721e-03\n",
      "                         :   16 : vars      : 8.582e-03\n",
      "                         :   17 : vars      : 8.564e-03\n",
      "                         :   18 : vars      : 8.561e-03\n",
      "                         :   19 : vars      : 8.274e-03\n",
      "                         :   20 : vars      : 8.127e-03\n",
      "                         :   21 : vars      : 8.117e-03\n",
      "                         :   22 : vars      : 8.018e-03\n",
      "                         :   23 : vars      : 7.925e-03\n",
      "                         :   24 : vars      : 7.826e-03\n",
      "                         :   25 : vars      : 7.824e-03\n",
      "                         :   26 : vars      : 7.820e-03\n",
      "                         :   27 : vars      : 7.802e-03\n",
      "                         :   28 : vars      : 7.574e-03\n",
      "                         :   29 : vars      : 7.556e-03\n",
      "                         :   30 : vars      : 7.445e-03\n",
      "                         :   31 : vars      : 7.441e-03\n",
      "                         :   32 : vars      : 7.410e-03\n",
      "                         :   33 : vars      : 7.405e-03\n",
      "                         :   34 : vars      : 7.317e-03\n",
      "                         :   35 : vars      : 7.241e-03\n",
      "                         :   36 : vars      : 7.152e-03\n",
      "                         :   37 : vars      : 7.134e-03\n",
      "                         :   38 : vars      : 7.086e-03\n",
      "                         :   39 : vars      : 7.069e-03\n",
      "                         :   40 : vars      : 7.066e-03\n",
      "                         :   41 : vars      : 7.065e-03\n",
      "                         :   42 : vars      : 7.044e-03\n",
      "                         :   43 : vars      : 6.965e-03\n",
      "                         :   44 : vars      : 6.954e-03\n",
      "                         :   45 : vars      : 6.879e-03\n",
      "                         :   46 : vars      : 6.874e-03\n",
      "                         :   47 : vars      : 6.861e-03\n",
      "                         :   48 : vars      : 6.773e-03\n",
      "                         :   49 : vars      : 6.693e-03\n",
      "                         :   50 : vars      : 6.628e-03\n",
      "                         :   51 : vars      : 6.549e-03\n",
      "                         :   52 : vars      : 6.483e-03\n",
      "                         :   53 : vars      : 6.441e-03\n",
      "                         :   54 : vars      : 6.426e-03\n",
      "                         :   55 : vars      : 6.390e-03\n",
      "                         :   56 : vars      : 6.386e-03\n",
      "                         :   57 : vars      : 6.384e-03\n",
      "                         :   58 : vars      : 6.300e-03\n",
      "                         :   59 : vars      : 6.289e-03\n",
      "                         :   60 : vars      : 6.260e-03\n",
      "                         :   61 : vars      : 6.257e-03\n",
      "                         :   62 : vars      : 6.202e-03\n",
      "                         :   63 : vars      : 6.169e-03\n",
      "                         :   64 : vars      : 6.147e-03\n",
      "                         :   65 : vars      : 6.131e-03\n",
      "                         :   66 : vars      : 6.004e-03\n",
      "                         :   67 : vars      : 5.977e-03\n",
      "                         :   68 : vars      : 5.937e-03\n",
      "                         :   69 : vars      : 5.893e-03\n",
      "                         :   70 : vars      : 5.879e-03\n",
      "                         :   71 : vars      : 5.876e-03\n",
      "                         :   72 : vars      : 5.869e-03\n",
      "                         :   73 : vars      : 5.846e-03\n",
      "                         :   74 : vars      : 5.831e-03\n",
      "                         :   75 : vars      : 5.768e-03\n",
      "                         :   76 : vars      : 5.688e-03\n",
      "                         :   77 : vars      : 5.674e-03\n",
      "                         :   78 : vars      : 5.650e-03\n",
      "                         :   79 : vars      : 5.637e-03\n",
      "                         :   80 : vars      : 5.623e-03\n",
      "                         :   81 : vars      : 5.520e-03\n",
      "                         :   82 : vars      : 5.405e-03\n",
      "                         :   83 : vars      : 5.387e-03\n",
      "                         :   84 : vars      : 5.349e-03\n",
      "                         :   85 : vars      : 5.211e-03\n",
      "                         :   86 : vars      : 5.149e-03\n",
      "                         :   87 : vars      : 5.041e-03\n",
      "                         :   88 : vars      : 5.008e-03\n",
      "                         :   89 : vars      : 4.950e-03\n",
      "                         :   90 : vars      : 4.939e-03\n",
      "                         :   91 : vars      : 4.923e-03\n",
      "                         :   92 : vars      : 4.825e-03\n",
      "                         :   93 : vars      : 4.812e-03\n",
      "                         :   94 : vars      : 4.753e-03\n",
      "                         :   95 : vars      : 4.737e-03\n",
      "                         :   96 : vars      : 4.712e-03\n",
      "                         :   97 : vars      : 4.642e-03\n",
      "                         :   98 : vars      : 4.637e-03\n",
      "                         :   99 : vars      : 4.583e-03\n",
      "                         :  100 : vars      : 4.551e-03\n",
      "                         :  101 : vars      : 4.458e-03\n",
      "                         :  102 : vars      : 4.433e-03\n",
      "                         :  103 : vars      : 4.397e-03\n",
      "                         :  104 : vars      : 4.329e-03\n",
      "                         :  105 : vars      : 4.308e-03\n",
      "                         :  106 : vars      : 4.295e-03\n",
      "                         :  107 : vars      : 4.277e-03\n",
      "                         :  108 : vars      : 4.270e-03\n",
      "                         :  109 : vars      : 4.199e-03\n",
      "                         :  110 : vars      : 4.192e-03\n",
      "                         :  111 : vars      : 4.167e-03\n",
      "                         :  112 : vars      : 4.148e-03\n",
      "                         :  113 : vars      : 4.140e-03\n",
      "                         :  114 : vars      : 4.131e-03\n",
      "                         :  115 : vars      : 4.130e-03\n",
      "                         :  116 : vars      : 4.102e-03\n",
      "                         :  117 : vars      : 4.101e-03\n",
      "                         :  118 : vars      : 3.989e-03\n",
      "                         :  119 : vars      : 3.973e-03\n",
      "                         :  120 : vars      : 3.939e-03\n",
      "                         :  121 : vars      : 3.855e-03\n",
      "                         :  122 : vars      : 3.816e-03\n",
      "                         :  123 : vars      : 3.803e-03\n",
      "                         :  124 : vars      : 3.778e-03\n",
      "                         :  125 : vars      : 3.744e-03\n",
      "                         :  126 : vars      : 3.687e-03\n",
      "                         :  127 : vars      : 3.677e-03\n",
      "                         :  128 : vars      : 3.674e-03\n",
      "                         :  129 : vars      : 3.654e-03\n",
      "                         :  130 : vars      : 3.583e-03\n",
      "                         :  131 : vars      : 3.573e-03\n",
      "                         :  132 : vars      : 3.560e-03\n",
      "                         :  133 : vars      : 3.553e-03\n",
      "                         :  134 : vars      : 3.544e-03\n",
      "                         :  135 : vars      : 3.465e-03\n",
      "                         :  136 : vars      : 3.465e-03\n",
      "                         :  137 : vars      : 3.436e-03\n",
      "                         :  138 : vars      : 3.432e-03\n",
      "                         :  139 : vars      : 3.364e-03\n",
      "                         :  140 : vars      : 3.331e-03\n",
      "                         :  141 : vars      : 3.321e-03\n",
      "                         :  142 : vars      : 3.277e-03\n",
      "                         :  143 : vars      : 3.250e-03\n",
      "                         :  144 : vars      : 3.248e-03\n",
      "                         :  145 : vars      : 3.205e-03\n",
      "                         :  146 : vars      : 3.172e-03\n",
      "                         :  147 : vars      : 3.153e-03\n",
      "                         :  148 : vars      : 3.135e-03\n",
      "                         :  149 : vars      : 3.095e-03\n",
      "                         :  150 : vars      : 3.092e-03\n",
      "                         :  151 : vars      : 3.086e-03\n",
      "                         :  152 : vars      : 3.044e-03\n",
      "                         :  153 : vars      : 3.020e-03\n",
      "                         :  154 : vars      : 2.984e-03\n",
      "                         :  155 : vars      : 2.967e-03\n",
      "                         :  156 : vars      : 2.928e-03\n",
      "                         :  157 : vars      : 2.881e-03\n",
      "                         :  158 : vars      : 2.876e-03\n",
      "                         :  159 : vars      : 2.819e-03\n",
      "                         :  160 : vars      : 2.811e-03\n",
      "                         :  161 : vars      : 2.782e-03\n",
      "                         :  162 : vars      : 2.757e-03\n",
      "                         :  163 : vars      : 2.696e-03\n",
      "                         :  164 : vars      : 2.690e-03\n",
      "                         :  165 : vars      : 2.606e-03\n",
      "                         :  166 : vars      : 2.598e-03\n",
      "                         :  167 : vars      : 2.554e-03\n",
      "                         :  168 : vars      : 2.548e-03\n",
      "                         :  169 : vars      : 2.543e-03\n",
      "                         :  170 : vars      : 2.500e-03\n",
      "                         :  171 : vars      : 2.491e-03\n",
      "                         :  172 : vars      : 2.484e-03\n",
      "                         :  173 : vars      : 2.456e-03\n",
      "                         :  174 : vars      : 2.424e-03\n",
      "                         :  175 : vars      : 2.383e-03\n",
      "                         :  176 : vars      : 2.349e-03\n",
      "                         :  177 : vars      : 2.266e-03\n",
      "                         :  178 : vars      : 2.229e-03\n",
      "                         :  179 : vars      : 2.203e-03\n",
      "                         :  180 : vars      : 2.161e-03\n",
      "                         :  181 : vars      : 2.131e-03\n",
      "                         :  182 : vars      : 2.120e-03\n",
      "                         :  183 : vars      : 2.095e-03\n",
      "                         :  184 : vars      : 2.074e-03\n",
      "                         :  185 : vars      : 2.032e-03\n",
      "                         :  186 : vars      : 2.030e-03\n",
      "                         :  187 : vars      : 1.943e-03\n",
      "                         :  188 : vars      : 1.882e-03\n",
      "                         :  189 : vars      : 1.860e-03\n",
      "                         :  190 : vars      : 1.856e-03\n",
      "                         :  191 : vars      : 1.778e-03\n",
      "                         :  192 : vars      : 1.702e-03\n",
      "                         :  193 : vars      : 1.638e-03\n",
      "                         :  194 : vars      : 1.601e-03\n",
      "                         :  195 : vars      : 1.596e-03\n",
      "                         :  196 : vars      : 1.527e-03\n",
      "                         :  197 : vars      : 1.481e-03\n",
      "                         :  198 : vars      : 1.389e-03\n",
      "                         :  199 : vars      : 1.350e-03\n",
      "                         :  200 : vars      : 1.324e-03\n",
      "                         :  201 : vars      : 1.274e-03\n",
      "                         :  202 : vars      : 1.264e-03\n",
      "                         :  203 : vars      : 1.190e-03\n",
      "                         :  204 : vars      : 1.178e-03\n",
      "                         :  205 : vars      : 1.084e-03\n",
      "                         :  206 : vars      : 1.035e-03\n",
      "                         :  207 : vars      : 9.057e-04\n",
      "                         :  208 : vars      : 8.352e-04\n",
      "                         :  209 : vars      : 0.000e+00\n",
      "                         :  210 : vars      : 0.000e+00\n",
      "                         :  211 : vars      : 0.000e+00\n",
      "                         :  212 : vars      : 0.000e+00\n",
      "                         :  213 : vars      : 0.000e+00\n",
      "                         :  214 : vars      : 0.000e+00\n",
      "                         :  215 : vars      : 0.000e+00\n",
      "                         :  216 : vars      : 0.000e+00\n",
      "                         :  217 : vars      : 0.000e+00\n",
      "                         :  218 : vars      : 0.000e+00\n",
      "                         :  219 : vars      : 0.000e+00\n",
      "                         :  220 : vars      : 0.000e+00\n",
      "                         :  221 : vars      : 0.000e+00\n",
      "                         :  222 : vars      : 0.000e+00\n",
      "                         :  223 : vars      : 0.000e+00\n",
      "                         :  224 : vars      : 0.000e+00\n",
      "                         :  225 : vars      : 0.000e+00\n",
      "                         :  226 : vars      : 0.000e+00\n",
      "                         :  227 : vars      : 0.000e+00\n",
      "                         :  228 : vars      : 0.000e+00\n",
      "                         :  229 : vars      : 0.000e+00\n",
      "                         :  230 : vars      : 0.000e+00\n",
      "                         :  231 : vars      : 0.000e+00\n",
      "                         :  232 : vars      : 0.000e+00\n",
      "                         :  233 : vars      : 0.000e+00\n",
      "                         :  234 : vars      : 0.000e+00\n",
      "                         :  235 : vars      : 0.000e+00\n",
      "                         :  236 : vars      : 0.000e+00\n",
      "                         :  237 : vars      : 0.000e+00\n",
      "                         :  238 : vars      : 0.000e+00\n",
      "                         :  239 : vars      : 0.000e+00\n",
      "                         :  240 : vars      : 0.000e+00\n",
      "                         :  241 : vars      : 0.000e+00\n",
      "                         :  242 : vars      : 0.000e+00\n",
      "                         :  243 : vars      : 0.000e+00\n",
      "                         :  244 : vars      : 0.000e+00\n",
      "                         :  245 : vars      : 0.000e+00\n",
      "                         :  246 : vars      : 0.000e+00\n",
      "                         :  247 : vars      : 0.000e+00\n",
      "                         :  248 : vars      : 0.000e+00\n",
      "                         :  249 : vars      : 0.000e+00\n",
      "                         :  250 : vars      : 0.000e+00\n",
      "                         :  251 : vars      : 0.000e+00\n",
      "                         :  252 : vars      : 0.000e+00\n",
      "                         :  253 : vars      : 0.000e+00\n",
      "                         :  254 : vars      : 0.000e+00\n",
      "                         :  255 : vars      : 0.000e+00\n",
      "                         :  256 : vars      : 0.000e+00\n",
      "                         : --------------------------------------\n",
      "                         : No variable ranking supplied by classifier: TMVA_DNN_CPU\n",
      "                         : No variable ranking supplied by classifier: TMVA_CNN_CPU\n",
      "                         : No variable ranking supplied by classifier: PyTorch\n",
      "TH1.Print Name  = TrainingHistory_TMVA_DNN_CPU_trainingError, Entries= 0, Total sum= 4.18686\n",
      "TH1.Print Name  = TrainingHistory_TMVA_DNN_CPU_valError, Entries= 0, Total sum= inf\n",
      "TH1.Print Name  = TrainingHistory_TMVA_CNN_CPU_trainingError, Entries= 0, Total sum= 6.99934\n",
      "TH1.Print Name  = TrainingHistory_TMVA_CNN_CPU_valError, Entries= 0, Total sum= 6.59572\n",
      "Factory                  : === Destroy and recreate all methods via weight files for testing ===\n",
      "                         : \n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#  ## Train Methods\n",
    "\n",
    "factory.TrainAllMethods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bce54194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom objects for loading model :  {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7fe8bbedfc10>, 'predict_func': <function predict at 0x7fe8b5474790>}\n",
      "Factory                  : \u001b[1mTest all methods\u001b[0m\n",
      "Factory                  : Test method: BDT for Classification performance\n",
      "                         : \n",
      "BDT                      : [dataset] : Evaluation of BDT on testing sample (2000 events)\n",
      "                         : Elapsed time for evaluation of 2000 events: 0.0428 sec       \n",
      "Factory                  : Test method: TMVA_DNN_CPU for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TMVA_DNN_CPU             : [dataset] : Evaluation of TMVA_DNN_CPU on testing sample (2000 events)\n",
      "                         : Elapsed time for evaluation of 2000 events: 0.0115 sec       \n",
      "Factory                  : Test method: TMVA_CNN_CPU for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TMVA_CNN_CPU             : [dataset] : Evaluation of TMVA_CNN_CPU on testing sample (2000 events)\n",
      "                         : Elapsed time for evaluation of 2000 events: 0.454 sec       \n",
      "Factory                  : Test method: PyTorch for Classification performance\n",
      "                         : \n",
      "                         :  Setup PyTorch Model \n",
      "                         :  Executing user initialization code from  /home/neel/Root/install/tutorials/tmva/PyTorch_Generate_CNN_Model.py\n",
      "                         : Loaded pytorch train function: \n",
      "                         : Loaded pytorch optimizer: \n",
      "                         : Loaded pytorch loss function: \n",
      "                         : Loaded pytorch predict function: \n",
      "                         : Load model from file: PyTorchTrainedModelCNN.pt\n",
      "PyTorch                  : [dataset] : Evaluation of PyTorch on testing sample (2000 events)\n",
      "                         : Elapsed time for evaluation of 2000 events: 0.132 sec       \n"
     ]
    }
   ],
   "source": [
    "## Test and Evaluate Methods\n",
    "\n",
    "factory.TestAllMethods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "991a4e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mEvaluate all methods\u001b[0m\n",
      "Factory                  : Evaluate classifier: BDT\n",
      "                         : \n",
      "BDT                      : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 256 , it is larger than 200\n",
      "Factory                  : Evaluate classifier: TMVA_DNN_CPU\n",
      "                         : \n",
      "TMVA_DNN_CPU             : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 256 , it is larger than 200\n",
      "Factory                  : Evaluate classifier: TMVA_CNN_CPU\n",
      "                         : \n",
      "TMVA_CNN_CPU             : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 256 , it is larger than 200\n",
      "Factory                  : Evaluate classifier: PyTorch\n",
      "                         : \n",
      "PyTorch                  : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Dataset[dataset] :  variable plots are not produces ! The number of variables is 256 , it is larger than 200\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       TMVA_CNN_CPU   : 0.909\n",
      "                         : dataset       PyTorch        : 0.898\n",
      "                         : dataset       BDT            : 0.844\n",
      "                         : dataset       TMVA_DNN_CPU   : 0.841\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              TMVA_CNN_CPU   : 0.305 (0.367)       0.711 (0.730)      0.929 (0.926)\n",
      "                         : dataset              PyTorch        : 0.285 (0.355)       0.681 (0.713)      0.900 (0.896)\n",
      "                         : dataset              BDT            : 0.155 (0.297)       0.544 (0.647)      0.817 (0.861)\n",
      "                         : dataset              TMVA_DNN_CPU   : 0.175 (0.240)       0.555 (0.620)      0.822 (0.837)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "Dataset:dataset          : Created tree 'TestTree' with 2000 events\n",
      "                         : \n",
      "Dataset:dataset          : Created tree 'TrainTree' with 8000 events\n",
      "                         : \n",
      "Factory                  : \u001b[1mThank you for using TMVA!\u001b[0m\n",
      "                         : \u001b[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "factory.EvaluateAllMethods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "544f0900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot ROC Curve\n",
    "\n",
    "c1 = factory.GetROCCurve(loader)\n",
    "c1.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1ed45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close outputfile to save output file\n",
    "outputFile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17bdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
